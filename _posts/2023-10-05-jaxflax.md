---
layout: post
title: What are JAX and Flax? Why those Deep Learning Frameworks can be Very Important?
date: 2023-10-05 09:56:00-0400
description:
tags: JAX, Flax, Deep Learning Frameworks
categories: blogpost
giscus_comments: true
related_posts: false
related_publications:
---

# Understanding JAX and Flax!

Hello, everyone! Today, we will learn about two powerful tools for machine learning: JAX and Flax. JAX helps us with fast math calculations, and Flax makes it easier to build neural networks. We'll use both to make a simple image classifier for handwritten digits.

## Introduction

Before diving into the technical details, let's discuss why we even need frameworks like JAX and Flax when we already have powerful libraries like PyTorch and TensorFlow.

### What's the Issue with Existing Frameworks?

Don't get me wrongâ€”PyTorch and TensorFlow are great. They are versatile, powerful, and have huge communities. However, they can be a bit rigid for some research needs:

- **Not So Easy to Customize**: If you need to modify the behavior of the training loop or gradient calculations, you might find it challenging.
- **Debugging**: Debugging can be hard, especially when computation graphs become complex.

### Why JAX?

JAX is like NumPy which means that JAX's features is its NumPy-compatible API allowing for easy transition from NumPy to JAX for numerical operations, but supercharged:

- **Flexibility**: JAX is functional and allows for more fine-grained control, making it highly customizable.
- **Performance**: With its just-in-time compilation, JAX can optimize your code for high-speed numerical computing.

In many cases, it would make sense to use jax.numpy (often imported as jnp) instead of ordinary NumPy to take advantage of JAX's features like automatic differentiation and GPU acceleration.

### Why Flax?

Flax is like the cherry on top of JAX:

- **Simplicity**: Building neural networks becomes straightforward.
- **Extendable**: Designed with research in mind, you can easily add unconventional elements to your network or training loop.

## What You'll Learn

- What are JAX and Flax?
- How to install them
- Building a simple CNN model for MNIST image classification

### What is JAX?

JAX is a library that helps us do fast numerical operations. It can automatically make our code run faster and allows us to use the GPU easily. It is widely used in research for its flexibility and speed.

### What is Flax?

Flax is built on top of JAX and provides a simple way to build and train neural networks. It is designed to be flexible, making it a good choice for research projects.

## Understanding JAX and Flax through MNIST Image Classification

Let's go into simple practical implementation on MNIST dataset. Before you build a skyscraper, you need to know how to make a small house. The same goes for machine learning. Understanding how to build a simple image classifier can give you the foundation you need to tackle more complex problems later. MNIST Image Classification is a simple but fundamental task in machine learning. It gives us a perfect playground to explore JAX and Flax without getting lost in the complexity of the task itself.

### Installing JAX and Flax

First, let's install JAX and Flax. Open your terminal and run:

```bash
pip install --upgrade jax jaxlib
pip install flax
```

### Import Libraries

Let's import all the libraries we need.

```python
import jax
import flax
import jax.numpy as jnp
from flax import linen as nn
from jax import random
from tensorflow.keras import datasets
```

### Prepare the Data

We'll use the MNIST dataset, which is a set of 28x28 grayscale images of handwritten digits.

```python
(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()

# Normalize and reshape the data using JAX's NumPy
train_images = jnp.expand_dims(train_images / 255.0, axis=-1).astype(jnp.float32)
test_images = jnp.expand_dims(test_images / 255.0, axis=-1).astype(jnp.float32)
```

### Create the Model

Now let's build a simple Convolutional Neural Network (CNN) using Flax.

```python
class CNN(nn.Module):
  @nn.compact
  def __call__(self, x):
    x = nn.Conv(features=32, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = nn.Conv(features=64, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = x.reshape((x.shape[0], -1))
    x = nn.Dense(features=256)(x)
    x = nn.relu(x)
    x = nn.Dense(features=10)(x)
    return nn.log_softmax(x)
```

### Initialize the Model

Before using our model, we need to initialize it.

```python
key = random.PRNGKey(0)
model = CNN()
x = jnp.ones((1, 28, 28, 1), jnp.float32)
params = model.init(key, x)
```

### Training

Now, let's train the model. But, first let's initialize the optimizer. We will use the Adam optimizer provided by Optax.

```python
# Initialize the optimizer
optimizer = optax.adam(0.001)
opt_state = optimizer.init(params)
```

We won't go into detail about training loops here, but you can use JAX's `grad` function to compute gradients and update the model weights.

```python
from jax import grad, jit, value_and_grad
import optax
from jax.scipy.special import logsumexp

def loss_fn(params, images, labels):
    logits = CNN().apply(params, images)
    logprobs = logits - logsumexp(logits, axis=-1, keepdims=True)
    return -jnp.mean(jnp.sum(logprobs * labels, axis=-1))

@jit
def train_step(opt_state, params, images, labels):
    loss, grads = value_and_grad(loss_fn)(params, images, labels)
    updates, new_opt_state = optimizer.update(grads, opt_state)
    new_params = optax.apply_updates(params, updates)
    return new_opt_state, new_params, loss
```

Now we can write the training loop.

```python
# One-hot encode labels
train_labels_onehot = jax.nn.one_hot(train_labels, 10)

# Training loop
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
    for i in range(0, len(train_images), batch_size):
        batch_images = jnp.array(train_images[i:i + batch_size])
        batch_labels = jnp.array(train_labels_onehot[i:i + batch_size])
        # print(jax.tree_map(lambda x: x.shape, params))
        opt_state, params, loss = train_step(opt_state, params, batch_images, batch_labels)

    print(f"Epoch {epoch + 1}, Loss: {loss}")
```

And that's it! You've built a simple CNN for MNIST digit classification using JAX and Flax.

### Conclusion

JAX and Flax are powerful tools for machine learning research and projects. JAX provides fast and flexible numerical operations, while Flax offers a simple and extendable way to build neural networks.

I hope this post helps you understand the basics of JAX and Flax. Happy coding!