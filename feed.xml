<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="ravialdy/ravialdy.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="ravialdy/ravialdy.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-11T11:45:23+00:00</updated><id>ravialdy/ravialdy.github.io/feed.xml</id><title type="html">Ravialdy’s Blog</title><subtitle>Experienced AI Researcher and Data Scientist with over 3 years of experience in computer vision, machine learning, and AI model deployment. Passionate about solving complex problems and advancing the state of technology. </subtitle><entry><title type="html">The Magic of Variational Inference</title><link href="ravialdy/ravialdy.github.io/blog/2023/variational-inference/" rel="alternate" type="text/html" title="The Magic of Variational Inference"/><published>2023-11-20T13:56:00+00:00</published><updated>2023-11-20T13:56:00+00:00</updated><id>ravialdy/ravialdy.github.io/blog/2023/variational-inference</id><content type="html" xml:base="ravialdy/ravialdy.github.io/blog/2023/variational-inference/"><![CDATA[<style>h2{margin-top:1.25em;margin-bottom:.5em}h3{margin-top:1em;margin-bottom:.5em}</style> <h2 id="introduction">Introduction</h2> <p>Hi guys, welcome to my blogpost again :) Today, I want to discuss about the magical and how wonderful Variational Inference (VI) is. This approach is widely used in many applications, such as text-to-image generation, motion planning, Reinforcement Learning (RL), etc.</p> <p>The reason for this is that in many cases the distribution of generated output that we want is very complex, e.g., images, text, video, etc. This is where VI can help us through latent variables. I believe that once we can master this concept well, then we can understand many recent AI techniques more easily and intuitively.</p> <p>I often found the explanation on the internet about this topic is not clear enough in explaining the reasons why this concept needs to exist somehow, why we need to calculate many fancy math terms, etc. Therefore, in this post I also want to focus more on the reasoning part so that all of us can understand the essence of this method beyond the derivations and usefulness of VI.</p> <p>This post is based on my understanding of the topic, so if you find any mistake please let me know :)</p> <div class="row mt-4 justify-content-center"> <div class="col-12 col-md-8 mx-auto mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/VI/DALL%C2%B7E%202023-11-23%2017.07.41-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/VI/DALL%C2%B7E%202023-11-23%2017.07.41-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/VI/DALL%C2%B7E%202023-11-23%2017.07.41-1400.webp"/> <img src="/assets/img/VI/DALL%C2%B7E%202023-11-23%2017.07.41.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 1. Illustration of the output from text-to-image model (Image source : DALLE-3). </div> <h2 id="latent-variable-models">Latent Variable Models</h2> <p>You may ask what Variational Inference (VI) really is? How it can be oftenly used in many recent AI methods? To answer those questions, let me start with latent variable models.</p> <p>Let’s say we want to build a regression model that can fit a simple data distribution like this,</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/VI/regression-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/VI/regression-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/VI/regression-1400.webp"/> <img src="/assets/img/VI/regression.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Regression model that tries to fit simple data distribution (Image source : <a href="https://www.analyticsvidhya.com/blog/2022/01/different-types-of-regression-models/">Analytics Vidhya</a>). </div> <p>What we basically try to do from the image above is to model \(p(\mathbf{y} \mid \mathbf{x})\) where \(y\) is our data given \(x\). It seems very simple right? But now let’s imagine we have quite complex data dsitribution like below,</p> <div class="row mt-4 justify-content-center"> <div class="col-12 col-md-8 mx-auto mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/VI/mixture_gaussian-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/VI/mixture_gaussian-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/VI/mixture_gaussian-1400.webp"/> <img src="/assets/img/VI/mixture_gaussian.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 3. The scenario where data distribution is complex enough (Image source : <a href="https://www.youtube.com/watch?v=iL1c1KmYPM0&amp;t=2900s">Stanford Online</a>). </div> <p>You might be confused initially on how we can build a model that fits that distribution. But don’t worry, I was also used to be like that too :) In reality, the distribution that we face might be much more complex than that.</p> <p>Fortunately, we can approximate that distribution through multiplication of two simple distributions. How we can do that? This is where the concept of latent variable models comes into play.</p> <p>The data distribution itself \(p(\mathbf{x})\) can be expressed mathematically as,</p> \[p(\mathbf{x})=\sum_z p(\mathbf{x} \mid \mathbf{z}) p(\mathbf{z})\] <p>where \(\mathbf{z}\) is the latent variable. Maybe you ask, what is that thing? Basically it is the hidden value that is not the variable \(y\) nor \(x\), but needs to be considered where we want to calculate the probability of the observed data \(\mathbf{x}\) in a more complex distribution. These latent variables represent underlying factors or characteristics that might not be directly observable but significantly influence the observed data.</p> <p>For example, the latent variables for figure 3 is the categorical value that maps each data point into cluster blue, green, or yellow.</p> <p>You may still wonder, how latent variable models is used in this case? First, we need to know that the prior or latent variable distribution \(p(\mathbf{z})\) is assumed to be a simple distribution, typically chosen as a standard gaussian distribution \(\mathcal{N}\left(0, \boldsymbol\Sigma^{2}\right)\), with variance \(\boldsymbol\Sigma^{2}\).</p> <p>So how about the distribution \(p(\mathbf{x} \mid \mathbf{z})\)? This is also assumed to be a normal distribution, but the parameters mean \(\boldsymbol\mu_{nn}\) and the variance \(\boldsymbol\Sigma_{nn}^{2}\) are generated by our neural networks. This means that even though the process of defining that distribution can be quite complex, but it is still considered to be a simple distribution since we can parameterize it.</p> <p>Thus, by doing like that we basically can approximate our data distribution \(p(\mathbf{x})\) as the multiplication of two simple distributions \(p(\mathbf{x})= \sum_z p(\mathbf{x} \mid \mathbf{z}) p(\mathbf{z})\). This is why we use latent variable models.</p> <p>But, there is a problem when we use this approach directly. Remember that earlier we assumed that the prior distribution \(p(\mathbf{z})\) is just a standard gaussian distribution which also means that it is a unimodal distribution.</p> <p>Can you imagine if we leverage that simple distribution directly without any training or learning process to approximate a very complex distribution which is oftenly has many modes? What will happen is that the approximation result will be not good since we do not incorporate any knowledge about our data into the pre-defined prior distribution. This is the where VI plays an important role :)</p> <h2 id="posterior-distribution">Posterior Distribution</h2> <p>From the previous explanation, you may be curious on how to update our prior belief represented by \(p(\mathbf{z})\). Actually, bayes theorem provides a way to do that. Specifically, the answer lies on what we call as posterior distribution \(p(\mathbf{z} \mid \mathbf{x})\). But in many cases, we cannot compute that expression or even if we can, then we need a lot of resources.</p> <p>For understanding why is that, let me briefly recap about the use of bayes theorem in this case.</p> <p>Our posterior can mathematically be expressed as,</p> \[p(\mathbf{z} \mid \mathbf{x}) = \frac{p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z})}{p(\mathbf{x})}\] <p>where \(p(\mathbf{x})\) is the marginal likelihood or evidence of our data distribution.</p> <p>There is also a joint distribution \(p(\mathbf{z}, \mathbf{x})\) that represents the probability of both the latent variables \(\mathbf{z}\) and the observed data \(\mathbf{x}\) occurring together. It can be factored into the product of the likelihood and the prior:</p> \[p(\mathbf{z}, \mathbf{x}) = p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z})\] <p>To calculate the marginal likelihood \(p(\mathbf{x})\), we can view it from bayesian perspective as the probability of observing the data \(\mathbf{x}\) marginalized over all possible values of the latent variables \(\mathbf{z}\). It is obtained by integrating (or summing, in the case of discrete variables) the joint distribution over \(\mathbf{z}\):</p> \[p(\mathbf{x}) = \int p(\mathbf{z}, \mathbf{x}) \, \mathrm{d}\mathbf{z} = \int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z}\] <p>This integral accounts for all possible configurations of the latent variables that could have generated the observed data.</p> <p>So why \(p(\mathbf{z} \mid \mathbf{x})\) is very difficult to calculate? The reason is that if we want to calculate it, then it means that we also need to calculate \(p(\mathbf{x})\) since it is located in the denominator of the posterior math equation. This also means that the complexity will arise rapidly as the dimensionality of \(\mathbf{z}\) grows.</p> <p>This is because the integral used for calculating \(p(\mathbf{x})\) sums over all possible values of \(\mathbf{z}\), and each evaluation of the joint distribution within the integral can also be computationally expensive, leading to an intractable integral.</p> <p>For example, let’s imagine we have a mixture model with \(K = 2\) clusters as our latent variable and \(n = 3\) data points which represents our observed data \(\mathbf{x}\). Thus, we will have 8 combinations as follows: (1,1,1), (1,1,2), (1,2,1), (1,2,2), (2,1,1), (2,1,2), (2,2,1), (2,2,2). Here, each tuple represents the cluster assignments for the three data points.</p> <p>For each of the 8 combinations of cluster assignments, we have to evaluate the likelihood of the entire data set given these assignments and the cluster means. The integral for every combination will be a two-dimensional integral over the two means (\(\mu_1\) and \(\mu_2\)). Then, the integral for marginal distribution can be written as,</p> \[p(\mathbf{x}) = \int \int \prod_{i=1}^{3} p(x_i \mid \mu_{c_i}) \, p(\mu_1) \, p(\mu_2) \, d\mu_1 \, d\mu_2\] <p>Here, \(p(x_i \mid \mu_{c_i})\) represents the likelihood of data point \(x_i\) given the mean of its assigned cluster \(\mu_{c_i}\), where \(c_i\) is the cluster assignment for data point \(i\).</p> <p>Notice that the number of integral follows the total dimensions that our latent variable \(\mathbf{z}\) has. In reality, we can have up to thousands of latent dimensions which means that we need to calculate thousands-dimensional integral!!</p> <p>Even for our very simple example, the cost can be computationally intensive, particularly when the likelihood and prior distributions do not have closed-form solutions!!</p> <p>Note* : closed-form solutions refers to the exact results from using standard math operations.</p> <h2 id="approximate-posterior-distribution">Approximate Posterior Distribution</h2> <p>Now you understand why calculating the exact posterior distribution is often very difficult. Many researchers try to solve this problem by approximating that distribution in various ways. In this post, I just want to focus on the estimation method related to the VI concept. Let’s go into more detail yeeyy :)</p> <p>Remember that the root cause is not the posterior itself, but its requirement to calculate marginal distribution \(p(\mathbf{x})\) to derive \(p(\mathbf{z} \mid \mathbf{x})\) which involves integrations. Thus, the key idea here is to approximate the posterior by replacing the annoying integral operations with the optimization process of expected value \(E_{z \sim q_i(\mathbf{z})}\) with respect to approximate posterior \(q_i(\mathbf{z})\).</p> <p>Specifically, the optimization is used to find the best approximation \(q_i(\mathbf{z} ; \boldsymbol{v*})\) where \(\boldsymbol{v*}\) are the variational parameters from a chosen family of distributions that minimizes the difference (specifically, the Kullback-Leibler divergence) from the true posterior \(p(\mathbf{z} \mid \mathbf{x})\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/VI/VI_optimization-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/VI/VI_optimization-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/VI/VI_optimization-1400.webp"/> <img src="/assets/img/VI/VI_optimization.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Optimization process happening in Variational Inference (VI) (Image source : <a href="https://www.cs.columbia.edu/~blei/talks/2016_NIPS_VI_tutorial.pdf">NIPS 2016 Tutorial</a>). </div> <p>So how we can do that approximation? First recall that the marginal distribution can be expressed as,</p> \[p(\mathbf{x}) = \int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z}\] <p>That above equation can also be written as,</p> \[p(\mathbf{x}) = \int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z} = \int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \frac{q_i(\mathbf{z})}{q_i(\mathbf{z})} \, \mathrm{d}\mathbf{z}\] <p>The introduction of \(\frac{q_i(\mathbf{z})}{q_i(\mathbf{z})}\) is a mathematical trick that allows us to rewrite the marginal likelihood in terms of the variational distribution \(q_i(\mathbf{z})\). By doing this, we can utilize the expected value with respect to \(q_i(\mathbf{z})\) to approximate the integral.</p> <p>The modified expression for the marginal likelihood becomes,</p> \[p(\mathbf{x}) = \int q_i(\mathbf{z}) \, \frac{p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z})}{q_i(\mathbf{z})} \, \mathrm{d}\mathbf{z}\] <p>The above expression can be interpreted as the expected value of the ratio \(\frac{p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z})}{q_i(\mathbf{z})}\) under the variational distribution \(q_i(\mathbf{z})\). Therefore, we can write like this,</p> \[p(\mathbf{x}) = E_{\mathbf{z} \sim q_i(\mathbf{z})}\left[\frac{p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z})}{q_i(\mathbf{z})}\right]\] <p>By using this formulation, we can avoid the direct computation of the integral for the marginal likelihood, which is typically intractable. Instead, we use optimization problem for finding the optimal \(q_i(\mathbf{z})\) that minimizes the difference from the true posterior involving expected value calculation.</p> <p>This approach is the essence of variational inference, transforming a challenging integration problem into a more manageable optimization problem. Specifically, we want to move difficult optimization</p> \[q_i^*(\mathbf{z})=\underset{q_i(\mathbf{z}) \in Q}{\operatorname{argmin}}(D_{\mathrm{KL}}(q_i(\mathbf{z} ; \boldsymbol{v}) \| p(\mathbf{z} \mid \mathbf{x})))\] <p>since we do not have true posterior \(p(\mathbf{z} \mid \mathbf{x})\) into easier one by replacing KL divergence with variational lower bound \(\mathcal{L}_i\left(p, q_i^*\right)\) (Don’t worry, we will discuss this notation in more detail in the next section) like below,</p> \[q_i^*(\mathbf{z})=\underset{q_i(\mathbf{z}) \in Q}{\operatorname{argmax}}(\mathcal{L}_i\left(p, q_i^*\right))\] <h2 id="evidence-lower-bound-objective-elbo">Evidence Lower Bound Objective (ELBO)</h2> <p>It turns out that if we approximate the true posterior by doing what we discussed before, then we can construct a lower bound for the marginal distribution \(p(x_i)\) of each \(x_i\) point. This can be a very powerful idea because by having that lower bound we can also maximize the loglikelihood of \(p(x_i)\).</p> <p>Note that generally increasing the lower bound does not necessarily mean also increase the loglikelihood of \(p(x_i)\), but if some conditions are satisfied, then it does (we will discuss more about this later).</p> <p>Recall that by looking at the previous equation, we can also derive the mathematical equation for each data point \(x_i\) of the distribution \(p(x_i)\) as,</p> \[p(x_i) = E_{\mathbf{z} \sim q_i(z)}\left[\frac{p(x_i \mid z) \, p(z)}{q_i(z)}\right]\] <p>If we apply log operation for both sides of the equation, we can get the expression like below,</p> \[\log p\left(x_i\right) = \log E_{z \sim q_i(z)}\left[\frac{p\left(x_i \mid z\right) p(z)}{q_i(z)}\right]\] <p>Then, we can implement jensen’s inequality \(\log E[y] \geq E[\log y]\) into our case, then we can get,</p> \[\log p\left(x_i\right) \geq E_{z \sim q_i(z)}\left[\log \frac{p\left(x_i \mid z\right) p(z)}{q_i(z)}\right]\] <p>By leveraging log property, the above equation can also be expressed as,</p> \[\log p\left(x_i\right) \geq E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right)+\log p(z)\right] - E_{z \sim q_i(z)}\left[\log q_i(z) \right]\] <p>where \(- E_{z \sim q_i(z)}\left[\log q_i(z) \right]\) is the entropy \(\mathcal{H}\left(q_i\right)\). The above inequality is also called as variational lower bound \(\mathcal{L}_i\left(p, q_i\right)\).</p> <p>So how we can make that lower bound to be tighter? The answer lies on how we can find a good approximation for \(q_i(z)\). So how we can do that? Yes you are right, the answer is by using KL divergence!</p> <p>The mathematical equation for implementing KL divergence between approximate and the true posterior can be written like this,</p> \[D_{\mathrm{KL}}\left(q_i\left(x_i\right) \| p\left(z \mid x_i\right)\right) = E_{z \sim q_i(z)}\left[\log \frac{q_i(z)}{p\left(z \mid x_i\right)}\right]\] <p>For those who are not familiar with KL divergence before, so basically the equation above measures how one probability distribution diverges from a second, expected probability distribution.</p> <p>Recall that the bayes theorem tells us,</p> \[p(z \mid x_i) = \frac{p(x_i \mid z) p(z)}{p(x_i)}\] <p>Therefore, we can use that to rewrite the term inside our KL divergence as:</p> \[\frac{q_i(z)}{p\left(z \mid x_i\right)} = \frac{q_i(z)}{\frac{p(x_i \mid z) p(z)}{p(x_i)}} = \frac{q_i(z) p(x_i)}{p(x_i, z)}\] <p>where \(p(x_i \mid z) p(z) = p(x_i, z)\) is derived from the definition of joint probability. By inserting above expression into inside our KL divergence, we can get,</p> \[D_{\mathrm{KL}}\left(q_i\left(x_i\right) \| p\left(z \mid x_i\right)\right) = E_{z \sim q_i(z)}\left[\log \frac{q_i(z) p\left(x_i\right)}{p\left(x_i, z\right)}\right]\] <p>After that, by using the log property we can also write above equation as,</p> \[\begin{aligned} D_{\mathrm{KL}}\left(q_i\left(x_i\right) \| p\left(z \mid x_i\right)\right) = &amp; -E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right) + \log p(z)\right] \\ &amp; + E_{z \sim q_i(z)}\left[\log q_i(z)\right] + E_{z \sim q_i(z)}\left[\log p\left(x_i\right)\right] \end{aligned}\] <p>Since \(- E_{z \sim q_i(z)}\left[\log q_i(z) \right]\) is the entropy \(\mathcal{H}\left(q_i\right)\), we can also write,</p> \[\begin{aligned} D_{\mathrm{KL}}\left(q_i\left(x_i\right) \| p\left(z \mid x_i\right)\right) = &amp; -E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right) + \log p(z)\right] \\ &amp; - \mathcal{H}\left(q_i\right) + \log p\left(x_i\right) \end{aligned}\] <p>Then, we can also express above equation as,</p> \[D_{\mathrm{KL}}\left(q_i\left(x_i\right) \| p\left(z \mid x_i\right)\right) = -\mathcal{L}_i\left(p, q_i\right)+\log p\left(x_i\right)\] <p>Rearranging above equation give us,</p> \[\log p\left(x_i\right) = D_{\mathrm{KL}}\left(q_i(z) \| p\left(z \mid x_i\right)\right)+\mathcal{L}_i\left(p, q_i\right)\] <p>As you can see, from the equation above we can say that if we successfully minimize the KL divergence part into 0 (which means our approximate posterior is exactly same with the true one), then the loglikelihood of our marginal or data distribution is also exactly same with the variational lower bound \(\mathcal{L}_i\left(p, q_i\right)\).</p> <p>Thus, we already have a way to make that lower bound more tight by minimizing the KL divergence part.</p> <p>But, how we can minimize the KL divergence in order to make the lower bound to be more tighter? Notice that in the last equation for \(\log p\left(x_i\right)\) that we have derived before, \(\log p(x_i)\) is a constant with respect to (w.r.t.) the variational distribution \(q_i(z)\).</p> <p>This means that if we maximize variational lower bound (we can also call it as ELBO at this step) with respect to \(q_i(z)\), then it also means we minimize \(D_{\mathrm{KL}}\left(q_i(z) \| p(z \mid x_i)\right)\) since \(\log p(x_i)\) is constant w.r.t. \(q_i(z)\).</p> <p>Thus, we can say that we can make the lower bound more tight by maximizing ELBO with respect to \(q_i(z)\) since it minimizes KL divergence. Similarly, we can also maximizes our likelihood / model by maximizing the same ELBO w.r.t. \(p\).</p> <h2 id="conclusion">Conclusion</h2> <p>After long discussion, you may still wonder what Variational Inference (VI) essentially tells us about? At its core, VI is a powerful method for making the intractable tractable. By introducing a variational distribution \(q_i(\mathbf{z})\) and optimizing it, VI transforms the challenging problem of computing the true posterior \(p(\mathbf{z} \mid \mathbf{x})\) into an optimization problem that is much more manageable computationally.</p> <p>This transformation rely on the replacement of the computationally intensive KL divergence with the variational lower bound \(\mathcal{L}_i\left(p, q_i^*\right)\), or ELBO. The key idea here is that instead of directly handle the high-dimensional integrals to define the true posterior, we work with a surrogate optimization problem that is much easier to solve but still retains the essential characteristics of the original problem.</p> <p>In essence, VI is about approximation and efficiency. By approximating the intractable posterior distribution with a more tractable form and maximizing the ELBO, we can indirectly estimate the true posterior.</p>]]></content><author><name></name></author><category term="blogpost"/><category term="Variational,"/><category term="Inference,"/><category term="Bayesian"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Interesting Adversarial Defense Methods</title><link href="ravialdy/ravialdy.github.io/blog/2023/interesting-defense-methods/" rel="alternate" type="text/html" title="Interesting Adversarial Defense Methods"/><published>2023-11-16T13:56:00+00:00</published><updated>2023-11-16T13:56:00+00:00</updated><id>ravialdy/ravialdy.github.io/blog/2023/interesting-defense-methods</id><content type="html" xml:base="ravialdy/ravialdy.github.io/blog/2023/interesting-defense-methods/"><![CDATA[<style>h2{margin-top:1.25em;margin-bottom:.5em}h3{margin-top:1em;margin-bottom:.5em}</style> <h2 id="introduction">Introduction</h2> <p>Today I want to discuss about some of the most interesting adversarial defense methods for me. In short, this post is related to several unique techniques that can be used to defend against adversarial attacks.</p> <p>For those who still do not know about adversarial attack really is, it is basically a way to fool the model by giving imperceptible change from the perspective of human eyes, but it can significantly change the prediction of our AI model.</p> <div class="row mt-4"> <div class="col-12 col-lg mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/defense_methods/adversarial_attack-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/defense_methods/adversarial_attack-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/defense_methods/adversarial_attack-1400.webp"/> <img src="/assets/img/defense_methods/adversarial_attack.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 1. Example of adversarial attack by giving a very small change to the image. (Image source : <a href="https://www.youtube.com/watch?v=-p2il-V-0fk&amp;t=511s">Nicholas Carlini</a>). </div> <p>As you can see from the image above, we can find a small change in the adversarial direction that is unseen from human eyes such that the model’s prediction is significantly changed (e.g., from dog to airplane).</p> <p>The reason for this is curse of dimensionality problem. Since we have a lot of dimensions in the input space, there is almost always exist some areas in the boundary decision that is not perfect as shown in the figure 1. In that image, we basically can move the input image a little bit by giving subtle change into another area that is already classified as airplane.</p> <p>How can we defend our model against such kind of attack? Let’s discuss some of the most interesting ones!</p> <h2 id="defense-gan">Defense-GAN</h2> <div class="row mt-4"> <div class="col-12 col-lg mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/defense_methods/defense-gan_overview-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/defense_methods/defense-gan_overview-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/defense_methods/defense-gan_overview-1400.webp"/> <img src="/assets/img/defense_methods/defense-gan_overview.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 2. Overview of Defense-GAN algorithm. (Image source : Samangouei et al [1]). </div> <p>The idea of Defense-GAN is basically to utilize the power of generator representations in GAN to eliminate the adversarial perturbations. This paper assumes that if the input image is changed along the adversarial direction, then the distribution of that image will also be deviated from clean images distributions.</p> <p>For those who are not familiar with GAN concept, it essentially has two main components. The first one is generator \(G\) that maps the learned latent distribution into reconstructed image. And the second one is discriminator \(D\) that minimizes the difference between the clean (unperturbed) image and the reconstructed one.</p> <p>Initially, we need to learn the latent distribution based on the clean images in our dataset by mapping the low-dimensional (latent) vector into the high-dimensional image. The generator \(G\) which is neural networks (or we can imagine it as a function) is responsible for doing this mapping.</p> <p>After that, to ensure that the generated image is indeed similar with the original one, we apply the discriminator \(D\) part which is basically a minimization process of the reconstruction loss.</p> <p>So how Defense-GAN can defend against adversarial attacks? In short, before feeding the input to the model, this algorithm will project the input image onto the range of generators and try to find the best latent vector \(\mathbf{z}^*\) in the learned latent clean distribution such that image generated by \(G(\mathbf{z})\) is as close as possible to the input image \(\mathbf{x}\).</p> <p>This will effectively avoid the wrong prediction since we give the type of image that is reconstructed from the latent vector located in the clean latent distribution. The process of finding that vector is happened during the inference time.</p> <p>But, there is an additional challenge. It turns out that the minimization problem faced by this algorithm is highly non-convex which means that there are many possible local minima where our optimization process can stop there.</p> <div class="row mt-4"> <div class="col-12 col-lg mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/defense_methods/defense-gan_lstepGD-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/defense_methods/defense-gan_lstepGD-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/defense_methods/defense-gan_lstepGD-1400.webp"/> <img src="/assets/img/defense_methods/defense-gan_lstepGD.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 3. Overview of multiple steps GD used in Defense-GAN algorithm. (Image source : Samangouei et al [1]). </div> <p>To address this, Defense-GAN uses \(L\)-steps GD, an iterative optimization algorithm, to navigate the latent space. At each step, this method will update the current estimate of \(\mathbf{z}\) to reduce the difference \(\|G(\mathbf{z})-\mathbf{x}\|_{2}^{2}\). The \(L\) steps indicate the number of iterations GD will run to refine the solution.</p> <p>In addition to that, Defense-GAN also employs multiple random restarts, meaning it initializes \(\mathbf{z}\) at different points in the latent space several times (each one denoted as \(Z_0^{(1)}, Z_0^{(2)}, ..., Z_0^{(R)}\)). For each restart, GD runs for \(L\) steps. This strategy increases the likelihood of finding the global minimum, or at least a very good approximation, rather than getting stuck in a local minimum.</p> <p>Finally, after \(L\) GD steps, the algorithm selects the best \(\mathbf{z}^*\) that resulted in the closest generated image to \(\mathbf{x}\). The output image \(G(\mathbf{z}^*)\) is considered as cleansed of version of the input image and is passed to the classifier for label prediction.</p> <h2 id="input-transformation">Input Transformation</h2> <p>Now, let’s discuss another fascinating defense mechanism, Input Transformation by Guo et al [2].</p> <p>The motivation behind this approach is based on a simple yet profound observation that adversarial attacks can a little bit alter certain statistics of the input image, thus changing the model’s prediction. These perturbations are not random but optimized in a way that they significantly impact the model’s output.</p> <p>But this is also means that we can manipulate these adversarial alterations in a way that not only negates their effects but also preserves the essence of the original image for accurate classification.</p> <p>The key intuition of this proposed method is quite straightforward. It is based on the hypothesis that by applying specific transformations to an image, we can disrupt the adversarial perturbations while retaining the critical information necessary for the model to make accurate predictions.</p> <div class="row mt-4"> <div class="col-12 col-lg mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/defense_methods/Input-transformation-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/defense_methods/Input-transformation-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/defense_methods/Input-transformation-1400.webp"/> <img src="/assets/img/defense_methods/Input-transformation.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 4. Illustration of how proposed input transformation method can give some insights. (Image source : Guo et al [2]). </div> <p>As you can see from the figure above, if we give careful image transformations, such as TV minimization and image quilting, then it can give meaningful differences with the first one seems like the core object itself while another one emphasize more to the background.</p> <p>The paper [2] proposes a series of image transformations, each with its unique way of combating adversarial effects:</p> <ol> <li> <p><strong>Image Cropping and Rescaling</strong>: This technique alters the spatial positioning of adversarial perturbations. By cropping and rescaling images during training and averaging predictions over random image crops at test time, it can hinder the effectiveness of adversarial attacks.</p> </li> <li> <p><strong>Bit-Depth Reduction</strong>: The idea is to perform a form of quantization, reducing the image to fewer bits, specifically to 3 bits in the experiments. This process helps in removing small adversarial variations in pixel values.</p> </li> <li> <p><strong>JPEG Compression</strong>: Similar to bit-depth reduction, JPEG compression removes small perturbations, but now it performs compression at a quality level of 75 out of 100, for maintaining image quality and eliminating adversarial effects.</p> </li> <li> <p><strong>Total Variance Minimization</strong>: This approach combines pixel dropout with total variation minimization. It reconstructs the simplest image consistent with a selected set of pixels, essentially filtering out the adversarial noise.</p> </li> <li> <p><strong>Image Quilting</strong>: Assumes that any part of the image could be affected by adversarial noise. The algorithm uses a database of clean patches—these are small parts of images that are known to be free from adversarial modifications to match sections of the input image with similar-looking patches from the database.</p> </li> </ol> <p>Each of these transformations contributes to destabilizing the adversarial perturbations, improving the model’s ability to classify images accurately. The beauty of this method lies in its simplicity and the non-invasive nature of the transformations, making it an elegant solution to such a complex problem.</p> <p>For the other defense methods, stay tune :) I will update more interesting approaches in the future :)</p>]]></content><author><name></name></author><category term="blogpost"/><category term="Adversarial"/><category term="Defense,"/><category term="Adversarial,"/><category term="Machine"/><category term="Learning"/><category term="Security"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Autonomous Driving Paper Review 1, HLS for Future Trajectory!</title><link href="ravialdy/ravialdy.github.io/blog/2023/paper-review-HLS/" rel="alternate" type="text/html" title="Autonomous Driving Paper Review 1, HLS for Future Trajectory!"/><published>2023-10-31T13:56:00+00:00</published><updated>2023-10-31T13:56:00+00:00</updated><id>ravialdy/ravialdy.github.io/blog/2023/paper-review-HLS</id><content type="html" xml:base="ravialdy/ravialdy.github.io/blog/2023/paper-review-HLS/"><![CDATA[<style>h2{margin-top:1.25em;margin-bottom:.5em}h3{margin-top:1em;margin-bottom:.5em}</style> <h2 id="introduction">Introduction</h2> <p><strong>Disclaimer</strong> : This review is based on my understanding of the reference paper [1]. While I have made much effort to ensure the accuracy of this article, there may things that I have not fully captured. If you notice any misinterpretation or error, please feel free to point them out in the comments section.</p> <p>I’m very excited to present a review of the paper titled “Hierarchical Latent Structure for Multi-Modal Vehicle Trajectory Forecasting” [1] authored by Dooseop Choi and KyoungWook Min. This paper is a very good work proved by its acceptance at the European Conference on Computer Vision (ECCV) 2022.</p> <p>For you who are not familiar with academia world in the AI field yet, ECCV is one of the most prestigious conferences in the domain. I truly believe this research paper is crucial for the autonomous driving topic, particularly in trajectory forecasting.</p> <div class="row mt-4"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/HLS.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/HLS.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/HLS.gif-1400.webp"/> <img src="/assets/img/HLS_Paper/HLS.gif" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Illustration of how the proposed Hierarchical Latent Structure (HLS) is used in the trajectory forecasting (Image source : D. Choi &amp; K. Min [1]). </div> <h2 id="notations-and-definitions">Notations and Definitions</h2> <table> <thead> <tr> <th>Notation</th> <th>Definition</th> </tr> </thead> <tbody> <tr> <td>\(N\)</td> <td>Number of vehicles in the traffic scene</td> </tr> <tr> <td>\(T\)</td> <td>Total number of timesteps for which trajectories are forecasted</td> </tr> <tr> <td>\(H\)</td> <td>Number of previous timesteps considered for positional history</td> </tr> <tr> <td>\(V_{i}\)</td> <td>The \(i^{th}\) vehicle in the traffic scene</td> </tr> <tr> <td>\(\mathbf{Y}_{i}\)</td> <td>Future positions of \(V_{i}\) for the next \(T\) timesteps</td> </tr> <tr> <td>\(\mathbf{X}_{i}\)</td> <td>Positional history of \(V_{i}\) for the previous \(H\) timesteps at time \(t\)</td> </tr> <tr> <td>\(\mathcal{C}_{i}\)</td> <td>Additional scene information available to \(V_{i}\)</td> </tr> <tr> <td>\(\mathbf{L}^{(1: M)}\)</td> <td>Lane candidates available for \(V_{i}\) at time \(t\)</td> </tr> <tr> <td>\(\mathbf{z}_{l}\)</td> <td>Low-level latent variable used to model the modes</td> </tr> <tr> <td>\(\mathbf{z}_{h}\)</td> <td>High-level latent variable used to model the weights for the modes</td> </tr> <tr> <td>\(p_{\theta}\)</td> <td>Decoder network</td> </tr> <tr> <td>\(p_{\gamma}\)</td> <td>Prior network</td> </tr> <tr> <td>\(\mathcal{L}_{E L B O}\)</td> <td>Modified ELBO objective</td> </tr> <tr> <td>\(q_{\phi}\)</td> <td>Approximated posterior network</td> </tr> <tr> <td>\(f_{\varphi}\)</td> <td>Proposed mode selection network</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">VLI</code></td> <td>Vehicle-Lane Interaction</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">V2I</code></td> <td>Vehicle-to-Vehicle Interaction</td> </tr> </tbody> </table> <h2 id="the-main-problem--mode-blur">The Main Problem : “Mode Blur”</h2> <p>The paper aims to overcome a specific limitation in vehicle trajectory forecasting models that leverage Variational Autoencoders (VAEs) concept called as the “mode blur” problem. For clearer illustration, please take a look at the figure below (this corresponds to the figure 1 in the reference paper [1]) :</p> <div class="row mt-4 justify-content-center"> <div class="col-12 col-md-8 mx-auto mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/figure1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/figure1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/figure1-1400.webp"/> <img src="/assets/img/HLS_Paper/figure1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 2. Illustration of the "mode blur" problem in VAE-based generated trajectory forecasts (Image source : D. Choi &amp; K. Min [1]). </div> <h3 id="what-is-mode-blur-problem">What is “Mode Blur” Problem?</h3> <p>As you can see from the figure above, the red vehicle is attempting to forecast its future trajectory represented by the branching gray paths. The challenge faced here lies in the generated forecast trajectories’ that are sometimes between defined lane paths.</p> <p>This phenomenon is what the author mean by the “mode blur” problem. Specifically, the VAE-based model is not committing to a specific path, but rather giving a “blurred” average of possible outcomes.</p> <div class="row mt-4"> <div class="col-12 col-lg mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/modeblur-previousSOTA-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/modeblur-previousSOTA-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/modeblur-previousSOTA-1400.webp"/> <img src="/assets/img/HLS_Paper/modeblur-previousSOTA.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 3. Example of "mode blur" problem that exists in the previous SOTA model (Image source: Cui et al, 2021 [2]). </div> <p>If you still wonder why the “mode blur” problem can be very important, consider the above figure example taken from the previous SOTA model as observed by D. Choi &amp; K. Min [1]. Before analyzing that figure in more detail, assume that the green bounding box represents the Autonomous Vehicle (AV), the light blue bounding boxes represent surrounding vehicles, and the trajectories (path predictions) of the surrounding vehicles are shown using the solid lines with light blue dots.</p> <div class="row mt-4 justify-content-center"> <div class="col-12 col-md-8 mx-auto mt-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/scenario2_ModeBlur-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/scenario2_ModeBlur-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/scenario2_ModeBlur-1400.webp"/> <img src="/assets/img/HLS_Paper/scenario2_ModeBlur.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 4. Scenario 2 of the "mode blur" problem that exist in the previous SOTA model (Image source : Cui et al, 2021 [2]). </div> <p>In scenario 2, a clear observation here is the overlapping and intersecting trajectories, especially around the intersection. These trajectories seem to be “blurred” between the lanes rather than being clearly defined in one lane or another. While in the scenario 3, despite the clearer trajectory forecasts than the previous one, we can still observe “mode blur” problems. Some predicted trajectories seem to be dispersed across the lane without a distinct path.</p> <p>This issue can lead to the Autonomous Vehicle (AV) having to make frequent adjustments to its path. This is indeed problematic as the AV might need to execute sudden brakes and make abrupt steering changes. This not only results in an uncomfortable ride for the passengers but also raises safety concerns.</p> <div class="row mt-4 justify-content-center"> <div class="col-12 col-md-8 mx-auto mt-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/scenario3_ModeBlur-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/scenario3_ModeBlur-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/scenario3_ModeBlur-1400.webp"/> <img src="/assets/img/HLS_Paper/scenario3_ModeBlur.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 5. Scenario 3 of the "mode blur" problem that exist in the previous SOTA model (Image source : Cui et al, 2021 [2]). </div> <h3 id="reason-why-mode-blur-happens">Reason Why “Mode Blur” Happens</h3> <p>The reason for this problem is the use of Variational Autoencoders (VAEs) in the trajectory forecasting models since they have a well-known limitation: the outputs that they generate can often be “blurry”. The authors of paper [1] observed that similar problem also found in the trajectory planning case, not only in the tasks involving image reconstruction and synthesis.</p> <p>VAEs aim to learn a probabilistic latent space representation of the data. So instead of generating fixed value of latent variable, what VAE does is to learn the latent space distribution and sample from it in order to get the latent vector that can be used to reconstruct the input.</p> <div class="row mt-4"> <div class="col-12 col-lg mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/AEvsVAE_Latent-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/AEvsVAE_Latent-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/AEvsVAE_Latent-1400.webp"/> <img src="/assets/img/HLS_Paper/AEvsVAE_Latent.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 6. Generated latent variable from common Autoencoder (fixed value) vs. VAE (probability distribution) (Image source : <a href="https://www.jeremyjordan.me/variational-autoencoders/">Jeremy Jordan</a>). </div> <p>The main objective of the VAEs is to optimize the Evidence Lower Bound Objective (ELBO) on the marginal likelihood of data \(p_\theta(\mathbf{x})\). This lower bound is formulated as:</p> \[\text{ELBO} = \mathbb{E}_{q_\phi(\mathbf{z} \mid \mathbf{x})}[\log p_\theta(\mathbf{x} \mid \mathbf{z})] - D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) \| p_\theta(\mathbf{z}))\] <p>Two components in the ELBO:</p> <ul> <li>The first term \(\mathbb{E}_{q_\phi(\mathbf{z} \mid \mathbf{x})}[\log p_\theta(\mathbf{x} \mid \mathbf{z})]\) is the reconstruction loss which measures how well the VAE reconstructs the original data when sampled from the approximate posterior \(q_\phi\).</li> <li>The second term \(D_{KL}(q_\phi(\mathbf{z} \mid \mathbf{x}) \| p_\theta(\mathbf{z}))\) is the Kullback-Leibler divergence between the approximate posterior \(q_\phi\) and the prior \(p_\theta\). This term acts as a regularizer, pushing the approximate posterior towards the prior.</li> </ul> <div class="row mt-4"> <div class="col-12 col-lg mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/VAE_Image-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/VAE_Image-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/VAE_Image-1400.webp"/> <img src="/assets/img/HLS_Paper/VAE_Image.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 7. Variational Autoencoder (VAE) which uses variational bayesian principle (Image source : <a href="https://sebastianraschka.com/teaching/stat453-ss2021/">Sebastian Raschka slide</a>). </div> <p>For more detailed understanding, you can take a look at this very good blogpost <a href="https://lilianweng.github.io/posts/2018-08-12-vae/">Lil’Log</a> or excellent explanation by <a href="https://www.youtube.com/watch?v=YHldNC1SZVk">Ahlad Kumar</a>.</p> <p>As far as i know, several previous works assume the prior distribution for the latent variables, \(Z\), to be a standard Gaussian distribution, \(\mathcal{N}(0, I)\), which is fixed and does not depend on the input context. The reason for using this assumption is to simplify the learning process.</p> <p>This can be problematic because a standard Gaussian prior assumes that the latent space is unimodal and therefore does not fully capture the multi-modal nature of the future trajectories where multiple distinct future paths (modes) are possible. This will make the alignment via KL divergence much more difficult for the model.</p> <p>Furthermore, when the VAE learns to represent data in the latent space, it must balance the reconstruction and KL divergence terms. It wants to spread out the representations to minimize the reconstruction loss (since the trajectory distribution is multi-modal) but it is also constrained by the KL divergence to keep these representations from getting too dispersed (since the prior is assumed to be unimodal).</p> <p>As a consequence, during the generation phase, when the model samples from these latent representations, it also may end up sampling from “in-between” spaces if the distinct modes are not well-separated.</p> <p>So in this case, the “mode blur” problem is most likely happened due to the balancing act between reconstruction loss and the KL divergence done by the ELBO objective function. When generating data, the VAE may generate a predicted trajectory that doesn’t clearly commit to any of the possible paths. Instead, it generates a trajectory that lies somewhere in between.</p> <h2 id="key-contributions">Key Contributions</h2> <p>Based on my understanding so far, there are 4 major contributions of this paper [1]:</p> <ol> <li> <p><strong>Mitigating Mode Blur</strong>: Propose a hierarchical latent structure within a VAE-based forecasting model to avoid “mode blur” problem, enabling clearer and more precise trajectory predictions.</p> </li> <li> <p><strong>Context Vectors</strong>: Two lane-level context vectors <code class="language-plaintext highlighter-rouge">VLI</code> and <code class="language-plaintext highlighter-rouge">V2I</code> are conditioned on the low-level latent variables for more accurate trajectory predictions.</p> </li> <li> <p><strong>Additional Methods</strong>: Introduce positional data preprocessing and GAN-based regularization to further enhance the performance.</p> </li> <li> <p><strong>Benchmark Performance</strong>: The state-of-the-art performance on two large-scale real-world datasets.</p> </li> </ol> <div class="row mt-4"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/VLI-visualization-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/VLI-visualization-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/VLI-visualization-1400.webp"/> <img src="/assets/img/HLS_Paper/VLI-visualization.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 8. Visualization of VLI (Image source : D. Choi &amp; K. Min [1]). </div> <h2 id="hierarchical-latent-structure-hls">Hierarchical Latent Structure (HLS)</h2> <div class="row mt-4"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/HLS-Avoid-ModeBlur_Example-fotor-20231104133653-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/HLS-Avoid-ModeBlur_Example-fotor-20231104133653-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/HLS-Avoid-ModeBlur_Example-fotor-20231104133653-1400.webp"/> <img src="/assets/img/HLS_Paper/HLS-Avoid-ModeBlur_Example-fotor-20231104133653.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 9. Example of how HLS avoids "mode blur" problem (Image source : D. Choi &amp; K. Min [1]). </div> <h3 id="introduction-to-hls">Introduction to HLS</h3> <p>You may wonder how that kind of approach can avoid the “mode blur” problem that happens in the previous work. Basically they implement Conditional-VAE with the input \(\mathbf{X}_{i}\) and \(\mathcal{C}_{i}\) as conditional variable. The goal of the proposed method is to generate a trajectory distribution \(p\left(\mathbf{Y}_{i} \mid \mathbf{X}_{i}, \mathcal{C}_{i}\right)\) for vehicles.</p> <p>The generated trajectory distribution is represented as a sum of modes, weighted by their probability or importance. Mathematically, it can be defined like below :</p> \[p\left(\mathbf{Y}_{i} \mid \mathbf{X}_{i}, \mathcal{C}_{i}\right)=\sum_{m=1}^{M} \underbrace{p\left(\mathbf{Y}_{i} \mid E_{m}, \mathbf{X}_{i}, \mathcal{C}_{i}\right)}_{\text {mode }} \underbrace{p\left(E_{m} \mid \mathbf{X}_{i}, \mathcal{C}_{i}\right)}_{\text {weight }}\] <p>Note that the term “mode” represents a plausible path, and the term “weight” represents the probability of each mode occurring.</p> <h3 id="hls-to-avoid-mode-blur">HLS to Avoid “Mode Blur”</h3> <p>The paper assumes that the trajectory distribution can be approximated as a mixture of simpler distributions. Each of these simpler distributions, or “modes”, represents a distinct pattern or type of trajectory that a vehicle could follow.</p> <p>The key intuition here is that instead of learning the overall distributions of the future trajectories, the proposed method tries to consider each possible trajectory (mode) separately and then approximate the trajectory distributions by combining all possible modes with its own probability.</p> <div class="row mt-4 justify-content-center"> <div class="col-12 col-md-8 mx-auto mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/figure1b_mode-separately-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/figure1b_mode-separately-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/figure1b_mode-separately-1400.webp"/> <img src="/assets/img/HLS_Paper/figure1b_mode-separately.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 10. Illustration of the trajectory forecasting distribution generated by HLS model (Image source : D. Choi &amp; K. Min [1]). </div> <p>To capture this mixture of distributions, the HLS model employs two levels of latent variables:</p> <ol> <li><strong>Low-level latent variable \(\mathbf{z}_{l}\)</strong>: Used to model individual modes of the trajectory distributions.</li> <li><strong>High-level latent variable \(\mathbf{z}_{h}\)</strong>: Employed to determine the weights for different modes.</li> </ol> <p>The mathematical equation of the new objective function can be expressed as below :</p> \[\begin{aligned} \mathcal{L}_{ELBO} = &amp; -\mathbb{E}_{\mathbf{z}_{l} \sim q_{\phi}}\left[\log p_{\theta}\left(\mathbf{Y}_{i} \mid \mathbf{z}_{l}, \mathbf{X}_{i}, \mathcal{C}_{i}^{m}\right)\right] \\ &amp; + \beta KL\Big(q_{\phi}\left(\mathbf{z}_{l} \mid \mathbf{Y}_{i}, \mathbf{X}_{i}, \mathcal{C}_{i}^{m}\right)\| p_{\gamma}\left(\mathbf{z}_{l} \mid \mathbf{X}_{i}, \mathcal{C}_{i}^{m}\right)\Big), \end{aligned}\] <p>The key aspect here is that the prior \(p_{\gamma}(\mathbf{z}_l \mid \mathbf{X}_{i}, \mathcal{C}_{i}^{m})\) is conditional on the input and the context. This implies that the model isn’t learning a single static prior for all data but rather a dynamic prior that adapts based on the specific input \(\mathbf{X}_i\) and context \(\mathcal{C}_{i}^{m}\).</p> <p>This conditionality allows the model to learn different representations for different subsets of data, guided by the vehicle’s past trajectory and additional scene information relevant to the vehicle. By doing so, the model can capture the nuances and variations in trajectory distributions that are specific to different traffic situations and lane configurations.</p> <p>That means we can have much more richer variations in the prior distribution which hopefully can reduce the gap that can happen between it and the posterior distribution (since it is conditioned on the trajectory distributions that are known to be multi-modal).</p> <p>By structuring the model in this way, the HLS method can generate trajectory predictions that are a combination of distinct, plausible paths that a vehicle might realistically take, each with its own probability.</p> <h3 id="generating-vli-and-v2i-context-vectors">Generating <code class="language-plaintext highlighter-rouge">VLI</code> and <code class="language-plaintext highlighter-rouge">V2I</code> Context Vectors</h3> <p>Now, we will discuss how the <code class="language-plaintext highlighter-rouge">VLI</code> and <code class="language-plaintext highlighter-rouge">V2I</code> context vectors are generated in the HLS paper. This process is done in the Scene Context Extraction module of the proposed trajectory forecasting model:</p> <div class="row mt-4"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/scene_extraction_module-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/scene_extraction_module-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/scene_extraction_module-1400.webp"/> <img src="/assets/img/HLS_Paper/scene_extraction_module.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 11. Diagram of Scene Context Extraction module of HLS method (Image source : D. Choi &amp; K. Min [1]). </div> <h4 id="vli-context-vector"><code class="language-plaintext highlighter-rouge">VLI</code> Context Vector:</h4> <p>The <code class="language-plaintext highlighter-rouge">VLI</code> context vector, denoted as \(\mathbf{a}_{i}^{m}\), is built around the idea that a vehicle’s trajectory is influenced by its reference lane (\(\mathbf{L}^{m}\)) and the surrounding lanes. The model first identifies the reference lane for the vehicle \(V_{i}\) and then calculates weights (\(\alpha_{l}\)) for the surrounding lanes. These weights signify the relative importance of each surrounding lane compared to the reference lane.</p> <p>The weights (\(\alpha_{l}\)) are determined through an attention mechanism between \(\tilde{\mathbf{X}}_{i}\) and \(\tilde{\mathbf{L}}^{(1: M)}\). The attention operation assesses how relevant each surrounding lane is in the context of the vehicle’s past motion and the current trajectory.</p> <p>Then, the final <code class="language-plaintext highlighter-rouge">VLI</code> context vector is a concatenation of the encoded reference lane \(\tilde{\mathbf{L}}^{m}\) and a weighted sum of the encoded surrounding lanes, where the weights are the attention scores \(\alpha_{l}\).</p> <p>The final mathematical expression for generating <code class="language-plaintext highlighter-rouge">VLI</code> context vector can be written :</p> \[\mathbf{a}_{i}^{m}=\left[\tilde{\mathbf{L}}^{m} ; \sum_{l=1, l \neq m}^{M} \alpha_{l} \tilde{\mathbf{L}}^{l}\right]\] <h4 id="v2i-context-vector"><code class="language-plaintext highlighter-rouge">V2I</code> Context Vector:</h4> <p>The <code class="language-plaintext highlighter-rouge">V2I</code> context vector, denoted as \(\mathbf{b}_{i}^{m}\), captures the interactions between the vehicle \(V_{i}\) and its neighboring vehicles within a certain distance (defined by a threshold \(\tau\)). This subset of neighboring vehicles is represented by \(\mathcal{N}_{i}^{m}\).</p> <div class="row mt-4 justify-content-center"> <div class="col-12 col-md-8 mx-auto mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/neighbor_vehicles-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/neighbor_vehicles-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/neighbor_vehicles-1400.webp"/> <img src="/assets/img/HLS_Paper/neighbor_vehicles.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 12. Illustration of surrounding vehicles chosen for generating `V2I` context vector (Image source : D. Choi &amp; K. Min [1]). </div> <p>The interactions itself are modeled using a Graph Neural Network (GNN). Message passing occurs from each neighboring vehicle \(V_{j}\) to the vehicle \(V_{i}\), defined by \(\mathbf{m}_{j \rightarrow i}\). These messages are a function of the relative positions and hidden states of \(V_{i}\) and \(V_{j}\) at each time step, processed through a Multi-Layer Perceptron (MLP).</p> <p>Then, the messages are aggregated, and the aggregated output is fed into a Gated Recurrent Unit (GRU) to update the hidden state of \(V_{i}\). The process repeats for \(K\) rounds, capturing the evolving interaction over time.</p> <p>Finally, the <code class="language-plaintext highlighter-rouge">V2I</code> context vector is the sum of the final hidden states of all neighboring vehicles, representing the cumulative effect of the vehicle-vehicle interactions on \(V_{i}\)’s future trajectory.</p> <h3 id="hls-overall-architecture">HLS Overall Architecture</h3> <div class="row mt-4"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/HLS_Architecture-fotor-20231104133313-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/HLS_Architecture-fotor-20231104133313-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/HLS_Architecture-fotor-20231104133313-1400.webp"/> <img src="/assets/img/HLS_Paper/HLS_Architecture-fotor-20231104133313.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 13. Diagram of HLS architecture (Image source : D. Choi &amp; K. Min [1]). </div> <h4 id="encoder-prior-and-decoder">Encoder, Prior, and Decoder:</h4> <ul> <li> <p><strong>Encoder</strong>: Approximate the posterior distributions with Multi-Layer Perceptrons (MLPs) where the encoding \(\tilde{\mathbf{Y}}_{i}\) and \(\mathbf{c}_{i}^{m}\) become inputs. Thus, it outputs two vectors, mean \(\mu_{e}\) and standard deviation \(\sigma_{e}\). Notably, the encoder is used only during the training phase because \(\mathbf{Y}_{i}\) is not available during inference.</p> </li> <li> <p><strong>Prior</strong>: Represents the prior distribution over the latent variable and is also implemented as MLPs. It takes \(\mathbf{c}_{i}^{m}\) as its input and outputs mean \(\mu_{p}\) and standard deviation \(\sigma_{p}\) vectors.</p> </li> <li> <p><strong>Decoder</strong>: Generates \(\hat{\mathbf{Y}}_{i}\) via an LSTM network. The input consists of an embedding of the predicted position \(\mathbf{e}_{i}^{t}\) along with \(\mathbf{c}_{i}^{m}\) and \(\mathbf{z}_{l}\). The LSTM updates its \(\mathbf{h}_{i}^{t+1}\) based on these inputs, and the new predicted position \(\hat{\mathbf{p}}_{i}^{t+1}\) is generated from this hidden state.</p> </li> </ul> <p>Note that the lane-level scene context vector is just the concatenation of</p> \[\mathbf{c}_{i}^{m}=\left[\tilde{\mathbf{X}}_{i} ; \mathbf{a}_{i}^{m} ; \mathbf{b}_{i}^{m}\right]\] <p>The design of this method aims to provide a holistic understanding of the vehicle’s motion by considering both lane and vehicle interactions. Lanes guide the general direction of movement, while nearby vehicles influence more immediate decisions like lane changes or speed adjustments.</p> <h2 id="conclusion">Conclusion</h2> <div class="row mt-4"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/Example_HLS_nuScene-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/Example_HLS_nuScene-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/Example_HLS_nuScene-1400.webp"/> <img src="/assets/img/HLS_Paper/Example_HLS_nuScene.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 14. Example of trajectory forecasting generated by HLS on nuScenes dataset (Image source : D. Choi &amp; K. Min [1]). </div> <p>This paper proposes a novel and unique way to tackle the problem of “mode blur” predictions in trajectory forecasting. Instead of assuming the prior as simple unimodal distribution, it uses conditional-VAE to capture richness and diversity of the future trajectories which is a multi-modal distribution. The final future trajectories are combinations of all possible modes each with its weight. The use of lane-level context vectors can add more precision, especially in understanding vehicle-lane and vehicle-vehicle interactions. This work not only sharpens the predictions but also can outperform the previous SOTA models in terms of accuracy.</p>]]></content><author><name></name></author><category term="blogpost"/><category term="Autonomous"/><category term="Driving,"/><category term="Trajectory"/><category term="Planning"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Grad-CAM Demystified, Understanding the Magic Behind Visual Explanations in Neural Networks</title><link href="ravialdy/ravialdy.github.io/blog/2023/gradcam-howitworks/" rel="alternate" type="text/html" title="Grad-CAM Demystified, Understanding the Magic Behind Visual Explanations in Neural Networks"/><published>2023-10-20T13:56:00+00:00</published><updated>2023-10-20T13:56:00+00:00</updated><id>ravialdy/ravialdy.github.io/blog/2023/gradcam-howitworks</id><content type="html" xml:base="ravialdy/ravialdy.github.io/blog/2023/gradcam-howitworks/"><![CDATA[<style>h2{margin-top:1.25em;margin-bottom:.5em}h3{margin-top:1em;margin-bottom:.5em}</style> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradcam/gradcam_our_result-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradcam/gradcam_our_result-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradcam/gradcam_our_result-1400.webp"/> <img src="/assets/img/gradcam/gradcam_our_result.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Grad-CAM visualization that we will implement in this blogpost. </div> <p>Convolutional Neural Networks (CNNs) are amazing. They can recognize cats in pictures, help self-driving cars see, and even beat humans at games. But what most people see about neural networks is this, they’re like magic boxes: data goes in, and the answer comes out, without knowing what happens in between. So, how do we know what part of an image the network finds important for its decision? Introducing Grad-CAM method, a technique that helps us “see” what the network is looking at.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradcam/GradCAM-Example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradcam/GradCAM-Example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradcam/GradCAM-Example-1400.webp"/> <img src="/assets/img/gradcam/GradCAM-Example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Example of how Grad-CAM visualization shows the important part for the model's decision on different classes (Image source : <a href="https://github.com/kazuto1011/grad-cam-pytorch">Kazuto</a>). </div> <h2 id="what-is-grad-cam">What is Grad-CAM?</h2> <p>Grad-CAM stands for Gradient-weighted Class Activation Mapping. Why the name is like that? In short, we use gradient to help us understand how neural networks behave in certain circumstances, while activation here is analogous with the level of excitement or interest the neural network has when it comes to certain features used in recognizing the important part in the image (we will discuss in detail about it later). How it does that? Basically, Grad-CAM will create what we call a “heatmap.” Imagine you have your cat picture. Now, think of putting a see-through red paper over it. This red paper will have some areas darker and some areas lighter. The darker areas show where the neural network looked the most. Maybe the network looked a lot at the cat’s eyes and a little at the tail. This heatmap will help you “see” what parts of the picture made the neural network decide it’s looking at a cat. It’s like the network is saying, “Look, I think this is a cat because of these parts of the picture.”</p> <h2 id="the-core-idea">The Core Idea</h2> <p>Grad-CAM will use something called “gradients” which can tell us how much each neuron’s activity would need to change in order to affect the final decision (class scores or logits that are output by the neural network) of the model. The key intuition here is that if the gradient is large in magnitude, a small change in the neuron’s activity will have a significant impact on the final decision. Conversely, if the gradient is small, the neuron’s contribution to the final decision is relatively minor. Grad-CAM also often uses deeper layers in order to visualize important part of the image. In a CNN, the early layers usually can only understand simple things like edges or colors. The deeper you go, the more complex the things they understand, like ears or whiskers. Grad-CAM focuses on the last set of these layers because they understand both the important details (like whiskers) and the bigger picture (like the shape of a cat). Remember that in the context of CNN, feature maps in the early layers of can only capture basic features like edges and textures. But, as you move deeper into the network, the feature maps begin to assemble these into more complex structures, capturing higher-level features like shapes, patterns, and even entire objects in some cases.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradcam/deeplearning_featuremaps-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradcam/deeplearning_featuremaps-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradcam/deeplearning_featuremaps-1400.webp"/> <img src="/assets/img/gradcam/deeplearning_featuremaps.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Illustration of how CNN and common NN architecture can learn more complex features as the layer goes deeper (Image source : <a href="https://julien-vitay.net/lecturenotes-neurocomputing/3-deeplearning/3-CNN.html">Julien Vitay</a>). </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradcam/gradcam_different_layers-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradcam/gradcam_different_layers-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradcam/gradcam_different_layers-1400.webp"/> <img src="/assets/img/gradcam/gradcam_different_layers.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Illustration of the effect of deeper layers towards Grad-CAM visualization (Image source : Selvaraju et al, 2017 [1]). </div> <h2 id="how-does-it-work-in-quite-detail">How Does it Work in Quite Detail?</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradcam/gradcam_detail_works-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradcam/gradcam_detail_works-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradcam/gradcam_detail_works-1400.webp"/> <img src="/assets/img/gradcam/gradcam_detail_works.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5. Overview Grad-CAM architecture (Image source : Selvaraju et al, 2017 [1]). </div> <h3 id="step-1-backward-pass">Step 1: Backward Pass</h3> <p>First, we need to find out how much each part of our image contributed to the final decision. So, we go backward through the network, from the output (“this is a cat”) toward the input image. As we go back, we calculate something called gradients. Remember that the “gradient” of a neuron with respect to the final decision can give us a measure of sensitivity. Specifically, it tells us how much the final output (e.g., the probability score for the class “cat”) would change if the activity of that particular neuron were to change by a small amount. In mathematical terms, if \(y\) is the final output and \(A_{ij}^k\) is the activation of neuron \(k\) at position \((i, j)\) in some layer, then \(\frac{\partial y}{\partial A_{ij}^k}\) is the gradient that tells us the rate of change of \(y\) with respect to \(A_{ij}^k\).</p> <h3 id="step-2-average-pooling">Step 2: Average Pooling</h3> <p>We then average these gradients across the spatial dimensions (width and height) of each feature map. This gives us a single number for each feature map, which we call the “importance weight.”</p> <p>The math looks like this:</p> \[\alpha_{k}^{c} = \frac{1}{Z} \sum_{i} \sum_{j} \frac{\partial y^{c}}{\partial A_{i j}^{k}}\] <p>Here, \(\alpha_{k}^{c}\) is the importance weight for feature map \(k\) when identifying class \(c\).</p> <h3 id="step-3-weighted-sum">Step 3: Weighted Sum</h3> <p>Next, we take a weighted sum of our original feature maps, using these importance weights. This gives us a rough heatmap. We will explain that in more detail about how it is used in the step 5.</p> \[L_{\text{Grad-CAM}}^{c} = \text{ReLU}\left(\sum_{k} \alpha_{k}^{c} A^{k}\right)\] <h3 id="step-4-relu-activation">Step 4: ReLU Activation</h3> <p>Finally, we apply a ReLU (Rectified Linear Unit) function to this heatmap. Why? Because we’re only interested in the parts of the image that positively influence the final decision.</p> <h3 id="step-5-understanding-the-heatmap">Step 5: Understanding the Heatmap</h3> <p>At this point, you might wonder, “How exactly does the weighted sum of feature maps and ReLU activation contribute to generating a heatmap?”</p> <p>The heatmap \(L_{\text{Grad-CAM}}^{c}\) is essentially a 2D spatial map of the image that highlights the important regions, which have been “weighted” based on their contribution to the class score. Recall that this weighted sum can be formally represented as:</p> \[L_{\text{Grad-CAM}}^{c} = \text{ReLU}\left(\sum_{k} \alpha_{k}^{c} A^{k}\right)\] <p>Here, \(\alpha_{k}^{c}\) serves as a weight indicating the importance of feature map \(A^{k}\) for the particular class \(c\). So, when we multiply \(\alpha_{k}^{c}\) with the feature map \(A^{k}\), we’re essentially weighing the feature map based on its importance for class \(c\).</p> <p>After the weighted sum, we apply the ReLU non-linearity function. Why ReLU? This is to ensure that only the features that have a positive influence on the class of interest are kept. ReLU zeroes out negative values, leaving only the positive regions that are important for identifying the specific class. The ReLU function can be represented mathematically as:</p> \[\text{ReLU}(x) = \max(0, x)\] <p>Thus, the heatmap generated is a filtered version of the weighted sum of feature maps, where only the ‘positively contributing’ features are illuminated. This enables you to see which regions in the image were pivotal in making the final class decision.</p> <h2 id="a-simple-pytorch-grad-cam-implementation">A Simple PyTorch Grad-CAM Implementation</h2> <p>To see Grad-CAM in action, let’s walk through a straightforward example using PyTorch. We’ll use a pretrained VGG16 model for this demonstration.</p> <p>First, make sure to install PyTorch if you haven’t already.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>torch torchvision
</code></pre></div></div> <h3 id="import-libraries">Import Libraries</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torchvision.models</span> <span class="k">as</span> <span class="n">models</span>
<span class="kn">import</span> <span class="n">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</code></pre></div></div> <h3 id="load-pretrained-model">Load Pretrained Model</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load a pretrained VGG16 model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">vgg16</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
</code></pre></div></div> <h3 id="utility-function-to-get-model-features-and-gradients">Utility Function to Get Model Features and Gradients</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>

<span class="k">def</span> <span class="nf">get_features_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Forward pass to get the features and register hook to get gradients.
    
    Parameters:
    - model (nn.Module): Neural network model
    - x (torch.Tensor): Input image tensor
    
    Returns:
    - features (torch.Tensor): Extracted features from the last convolutional layer
    - gradients (torch.Tensor): Gradients w.r.t the features
    </span><span class="sh">"""</span>
    <span class="n">features</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="k">def</span> <span class="nf">hook_feature</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="k">nonlocal</span> <span class="n">features</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">hook_gradient</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="k">nonlocal</span> <span class="n">gradients</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="n">grad_output</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">detach</span><span class="p">()</span>
        
    <span class="c1"># Register hooks
</span>    <span class="n">handle_forward</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">features</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">register_forward_hook</span><span class="p">(</span><span class="n">hook_feature</span><span class="p">)</span>
    <span class="n">handle_backward</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">features</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">register_backward_hook</span><span class="p">(</span><span class="n">hook_gradient</span><span class="p">)</span>
    
    <span class="c1"># Forward and backward pass
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># Class-specific backprop
</span>    <span class="n">output</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">([[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">243</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]]))</span>
    
    <span class="c1"># Remove hooks
</span>    <span class="n">handle_forward</span><span class="p">.</span><span class="nf">remove</span><span class="p">()</span>
    <span class="n">handle_backward</span><span class="p">.</span><span class="nf">remove</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">features</span><span class="p">,</span> <span class="n">gradients</span>
</code></pre></div></div> <h3 id="generate-grad-cam-heatmap">Generate Grad-CAM Heatmap</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>

<span class="k">def</span> <span class="nf">generate_grad_cam</span><span class="p">(</span><span class="n">features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">gradients</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">image_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Generate Grad-CAM heatmap.

    Parameters:
    - features (torch.Tensor): Extracted features from the last convolutional layer
    - gradients (torch.Tensor): Gradients w.r.t the features
    - image_shape (Tuple[int, int]): Original shape of the input image (height, width)

    Returns:
    - torch.Tensor: Grad-CAM heatmap
    </span><span class="sh">"""</span>
    <span class="c1"># Global average pooling on gradients to get neuron importance
</span>    <span class="n">alpha</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Weighted sum of feature maps based on neuron importance
</span>    <span class="n">weighted_features</span> <span class="o">=</span> <span class="n">features</span> <span class="o">*</span> <span class="n">alpha</span>

    <span class="c1"># ReLU applied on weighted combination of feature maps
</span>    <span class="n">heatmap</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">weighted_features</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    
    <span class="c1"># Resizing the heatmap to original image size
</span>    <span class="n">heatmap</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">interpolate</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">image_shape</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">bilinear</span><span class="sh">'</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">heatmap</span>
</code></pre></div></div> <h3 id="function-to-overlay-heatmap-on-original-image">Function to Overlay Heatmap on Original Image</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Union</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">overlay_heatmap_on_image</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Image</span><span class="p">.</span><span class="n">Image</span><span class="p">],</span> 
                             <span class="n">heatmap</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">],</span> 
                             <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Overlay the Grad-CAM heatmap on the original image.
    
    Parameters:
    - image (np.ndarray or PIL.Image): Original input image
    - heatmap (Union[np.ndarray, torch.Tensor]): Grad-CAM heatmap
    - alpha (float): Weight of the heatmap when overlaying
    
    Returns:
    - np.ndarray: Image with heatmap overlaid
    </span><span class="sh">"""</span>
    <span class="c1"># Convert PIL image to numpy array if necessary
</span>    <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">Image</span><span class="p">.</span><span class="n">Image</span><span class="p">):</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    
    <span class="c1"># Convert torch.Tensor to numpy array if necessary
</span>    <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">heatmap</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>
    
    <span class="c1"># Normalize the heatmap and convert to RGB format
</span>    <span class="n">heatmap_normalized</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="n">cv2</span><span class="p">.</span><span class="n">NORM_MINMAX</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cv2</span><span class="p">.</span><span class="n">CV_8U</span><span class="p">)</span>
    <span class="n">heatmap_colored</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">applyColorMap</span><span class="p">(</span><span class="n">heatmap_normalized</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLORMAP_JET</span><span class="p">)</span>
    
    <span class="c1"># Resize heatmap to match the image size
</span>    <span class="n">heatmap_resized</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span><span class="n">heatmap_colored</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    
    <span class="c1"># Overlay heatmap on image
</span>    <span class="n">overlayed</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">addWeighted</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">heatmap_resized</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">overlayed</span>
</code></pre></div></div> <h3 id="function-to-visualize-heatmap">Function to Visualize Heatmap</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Union</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">visualize_heatmap</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Image</span><span class="p">.</span><span class="n">Image</span><span class="p">],</span> 
                      <span class="n">heatmap</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                      <span class="n">figsize</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Visualize the original image, the Grad-CAM heatmap, and the overlayed image.
    
    Parameters:
    - image (Union[np.ndarray, Image.Image]): The original input image.
    - heatmap (torch.Tensor): The Grad-CAM heatmap.
    - figsize (Tuple[int, int]): The size of the figure for plotting.
    
    Returns:
    - None
    </span><span class="sh">"""</span>
    <span class="c1"># Normalize the heatmap for visualization
</span>    <span class="n">heatmap_normalized</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
    <span class="n">heatmap_normalized</span> <span class="o">=</span> <span class="p">(</span><span class="n">heatmap_normalized</span> <span class="o">-</span> <span class="n">heatmap_normalized</span><span class="p">.</span><span class="nf">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">heatmap_normalized</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">heatmap_normalized</span><span class="p">.</span><span class="nf">min</span><span class="p">())</span>
    
    <span class="c1"># Overlay the heatmap on the original image
</span>    <span class="n">overlayed_image</span> <span class="o">=</span> <span class="nf">overlay_heatmap_on_image</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">heatmap_normalized</span><span class="p">)</span>
    
    <span class="c1"># Create the plot
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Original Image</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">image</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Grad-CAM Heatmap</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">heatmap_normalized</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">jet</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Overlayed Image</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">overlayed_image</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h3 id="putting-it-all-together">Putting it All Together</h3> <p>Now, let’s apply Grad-CAM on an example image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load and preprocess an example image (here, 'bull_mastiff.jpg' is an example image file)
</span><span class="n">input_image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">/content/bull_mastiff.jpg</span><span class="sh">"</span><span class="p">).</span><span class="nf">resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
<span class="n">preprocess</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]),</span>
<span class="p">])</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">input_image</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Get features and gradients
</span><span class="n">features</span><span class="p">,</span> <span class="n">gradients</span> <span class="o">=</span> <span class="nf">get_features_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span>

<span class="c1"># Generate Grad-CAM heatmap
</span><span class="n">image_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_image</span><span class="p">.</span><span class="n">height</span><span class="p">,</span> <span class="n">input_image</span><span class="p">.</span><span class="n">width</span><span class="p">)</span>
<span class="n">heatmap</span> <span class="o">=</span> <span class="nf">generate_grad_cam</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">image_shape</span><span class="p">)</span>

<span class="c1"># Visualize the heatmap
</span><span class="nf">visualize_heatmap</span><span class="p">(</span><span class="n">input_image</span><span class="p">,</span> <span class="n">heatmap</span><span class="p">)</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradcam/gradcam_our_result-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradcam/gradcam_our_result-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradcam/gradcam_our_result-1400.webp"/> <img src="/assets/img/gradcam/gradcam_our_result.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 6. Grad-CAM visualization result. </div> <p>In this example, we focused on the ‘bull mastiff’ class, which corresponds to index 243 in the ImageNet dataset. You can replace this with the index for any other class you’re interested in.</p> <h2 id="conclusion">Conclusion</h2> <p>Grad-CAM is like understanding how exactly neural networks make a decision. It allows the network to tell us, “Hey, I think this is a cat because of these whiskers and this tail.” And it does this all without requiring any change to the existing model architecture and retraining the model, making it a powerful tool for understanding these complex networks.</p> <p>I hope this blog post has demystified Grad-CAM for you. It’s a very good visualization method that can explain the decision of complex neural networks, letting us see what’s happening under the hood.</p>]]></content><author><name></name></author><category term="blogpost"/><category term="Computer"/><category term="Vision"/><category term="(CV),"/><category term="GradCAM,"/><category term="Visualization"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Understanding Phenomenal REINFORCE Policy Gradient Method</title><link href="ravialdy/ravialdy.github.io/blog/2023/reinforce-blog/" rel="alternate" type="text/html" title="Understanding Phenomenal REINFORCE Policy Gradient Method"/><published>2023-10-15T13:56:00+00:00</published><updated>2023-10-15T13:56:00+00:00</updated><id>ravialdy/ravialdy.github.io/blog/2023/reinforce-blog</id><content type="html" xml:base="ravialdy/ravialdy.github.io/blog/2023/reinforce-blog/"><![CDATA[<style>h2{margin-top:1.25em;margin-bottom:.5em}h3{margin-top:1em;margin-bottom:.5em}</style> <h2 id="introduction">Introduction</h2> <p>Welcome to my blog post! Today we are going to discuss about a very fascinating and important topic in the world of Reinforcement Learning (RL) — the Policy Gradient REINFORCE Method. This method is quite famous for solving some complex problems in RL.</p> <p>Don’t worry if you’re new to this field, I will try to keep things simple and easy to understand. First of all, I will be focusing on the background of the Policy Gradient Theorem and why it was proposed in the first place.</p> <h3 id="brief-recap-about-reinforcement-learning">Brief Recap about Reinforcement Learning</h3> <p>Before diving into the core method, it’s important to get some basics right. Imagine we have a small robot placed at the entrance of a maze. The maze is simple: it has walls, open passages, and a cheese located at the exit. The robot’s ultimate goal is to find the most efficient path to reach the cheese.</p> <p>In RL, an agent (a robot in this case) interacts with an environment (like a maze). At each time \(t\), the agent is in a state \(s_t\), takes an action \(a_t\), and receives a reward \(r_t\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/reinforce/DALL%C2%B7E%202023-10-24%2015.38.59%20-%20Vector%20design%20of%20a%20playful%20scene%20where%20a%20cartoon%20robot%20is%20gearing%20up%20to%20enter%20a%20maze.%20The%20maze's%20pathways%20are%20clear,%20with%20walls%20separating%20the%20routes-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/reinforce/DALL%C2%B7E%202023-10-24%2015.38.59%20-%20Vector%20design%20of%20a%20playful%20scene%20where%20a%20cartoon%20robot%20is%20gearing%20up%20to%20enter%20a%20maze.%20The%20maze's%20pathways%20are%20clear,%20with%20walls%20separating%20the%20routes-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/reinforce/DALL%C2%B7E%202023-10-24%2015.38.59%20-%20Vector%20design%20of%20a%20playful%20scene%20where%20a%20cartoon%20robot%20is%20gearing%20up%20to%20enter%20a%20maze.%20The%20maze's%20pathways%20are%20clear,%20with%20walls%20separating%20the%20routes-1400.webp"/> <img src="/assets/img/reinforce/DALL%C2%B7E%202023-10-24%2015.38.59%20-%20Vector%20design%20of%20a%20playful%20scene%20where%20a%20cartoon%20robot%20is%20gearing%20up%20to%20enter%20a%20maze.%20The%20maze's%20pathways%20are%20clear,%20with%20walls%20separating%20the%20routes.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Illustration of an agent (robot) tries to reach the cheese as soon as possible (Image source : DALLE-3). </div> <p>The agent follows a “policy” \(\pi(a \mid s)\), which tells it what action \(a\) to take when in state \(s\). The policy itself is controlled by some parameters \(\theta\), which we adjust to make it better. Here are the more formal definitions of important terms in RL:</p> <ul> <li> <p><strong>Environment</strong>: The space or setting in which the agent operates.</p> </li> <li> <p><strong>State</strong>: The condition of the environment at a given time point, often denoted as \(s\) or \(s_t\) to indicate its time-dependence.</p> </li> <li> <p><strong>Agent</strong>: An entity that observes the state of the environment and takes actions to achieve a specific objective.</p> </li> <li> <p><strong>Action</strong>: A specific operation that an agent can execute, typically denoted by \(a\) or \(a_t\).</p> </li> <li> <p><strong>Policy</strong>: A policy, denoted by \(\pi(a \mid s)\) or \(\pi(s, a)\), is a mapping from states to actions, or to probabilities of selecting each action.</p> </li> <li> <p><strong>Reward</strong>: A scalar value, often denoted by \(r\) or \(r_t\), that the environment returns in response to the agent’s action.</p> </li> </ul> <h2 id="the-policy-gradient-theorem">The Policy Gradient Theorem</h2> <p>Before we delve into the details of REINFORCE algorithm, we need to understand what policy gradient really is and why it can be a game-changer in the world of RL. The reason for this is that REINFORCE itself belongs to this approach.</p> <p>Unlike traditional value-based methods which assess the “goodness” of states or state-action pairs, policy gradients aim to directly optimize the policy. Let’s take a look at what kind of potential problems value-based approach has and how policy gradient methods can avoid those issues:</p> <ul> <li> <p><strong>Curse of Dimensionality</strong>: Value-based methods require an estimated value for every possible state or state-action pair. As the number of states and actions increases, the size of the value function grows exponentially. By focusing directly on optimizing the policy, policy gradient methods can avoid this issue since it works with a much smaller set of parameters.</p> </li> <li> <p><strong>Non-Markovian Environments</strong>: In some cases, the environment is not following the Markov Property, where the future state depends only on the current state and action. Policy gradient methods do not rely on the Markov property because they do not predict future values; they only need to evaluate the outcomes of current actions.</p> </li> <li> <p><strong>Exploration vs. Exploitation</strong>: Value-based methods often cause the agent to stick to known high-value states and actions, missing out on potentially better options. By adjusting the policy parameters directly, policy gradient methods can encourage the agent to explore different actions with probabilities, rather than committing to the action with the highest estimated value.</p> </li> </ul> <p>In other words, the key difference is that the size of the parameter set in policy gradient methods is determined by the complexity of the policy representation (e.g., the architecture of the neural network), not by the size of the state or action space.</p> <p>For example, suppose you have a neural network with 1000 parameters. It can still process thousands or even millions of different states and output actions for each of them because the same parameters are used to evaluate every state through the network’s forward pass.</p> <p>This means that even for complex environments, the number of parameters doesn’t necessarily increase with the complexity of the state space, which is often the case with value-based methods.</p> <p>Thus, policy gradient methods are particularly well-suited for high-dimensional or continuous action spaces, can accommodate stochastic policies, and are less sensitive to the challenges associated with value function approximation.</p> <h3 id="the-formal-objective">The Formal Objective</h3> <p>The objective is to maximize the expected return \(J(\theta)\), defined as the average sum of rewards an agent can expect to receive while following a specific policy \(\pi\).</p> \[\max_{\theta} \mathbb{E}_{\tau \sim \pi_{\theta}}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t}\right]\] <p>In this equation, \(\gamma\) is the discount factor, \(\theta\) are the parameters governing the policy \(\pi\), and \(T\) is the time horizon.</p> <h3 id="the-policy-gradient-theorem-in-detail">The Policy Gradient Theorem in Detail</h3> <p>To find the maximum of this objective function, we need its gradient w.r.t \(\theta\). But first we need to understand that the probability of observing a trajectory \(\tau = (s_0, a_0, ..., s_{T+1})\) under policy \(\pi_{\theta}\) can be expressed as :</p> \[P(\tau \mid \theta) = \rho_0 (s_0) \prod_{t=0}^{T} P(s_{t+1}\mid s_t, a_t) \pi_{\theta}(a_t \mid s_t)\] <p>where \(\rho_0(s_0)\) represents the probability of starting in the initial state, \(\pi_{\theta}(a_t \mid s_t)\) is the probability of taking action \(a_t\) in state \(s_t\), as dictated by the policy, and \(P(s_{t+1} \mid s_t, a_t)\) represents the probability of transitioning to state \(s_{t+1}\) after taking action \(a_t\) in state \(s_t\).</p> <p>The reason for getting the expression \(P(\tau \mid \theta)\) is due to Markov Decision Process (MDP). Since each state-action pair and subsequent state transition is considered an independent event, the probability of the entire trajectory \(\tau\) occurring is the product of the probabilities of each of these events.</p> <p>The process includes the probability of the initial state, the probability of each action taken according to the policy, and the probability of each state transition as dictated by the environment’s dynamics.</p> <p>If we apply the log function to both sides of equation, we will get :</p> \[\log P(\tau \mid \theta) = \log \rho_0 (s_0) + \sum_{t=0}^{T} \left( \log P(s_{t+1}\mid s_t, a_t) + \log \pi_{\theta}(a_t \mid s_t)\right)\] <p>Since log operation can change multiplications into summations. Next, we will use the Log-Derivative trick like below,</p> \[\nabla_{\theta} P(\tau \mid \theta) = P(\tau \mid \theta) \nabla_{\theta} \log P(\tau \mid \theta)\] <p>Notice that if we take the derivative w.r.t \(\theta\), then the gradient of environment-specific functions like \(\rho_0(s_0)\), \(P(s_{t+1} \mid s_t, a_t)\), which do not depend on \(\theta\), are zero.</p> <p>Thus, applying that and implementing the Log-Derivative trick into the equation above will generate the equation below :</p> \[\nabla_{\theta} \log P(\tau \mid \theta) = \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\] <p>So now we are ready to calculate the gradient of the expected return w.r.t model parameters \(\theta\) which is the essence of the policy gradient theorem that we have discussed so far.</p> <p>Recall that by definition, \(J(\theta)\) is the expectation of the return \(R(\tau)\) over all possible trajectories \(\tau\) under the policy \(\pi_{\theta}\). Mathematically, this expectation can be represented as an integral over all possible trajectories, weighted by the probability of each trajectory under our policy,</p> \[\mathbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)] = \int_{\tau} P(\tau \mid \theta) R(\tau)\] <p>After that, we want to bring the gradient inside the integral. This is possible due to the linearity property of gradients, allowing us to interchange the order of differentiation and integration. Thus, we can rewrite the above expression as,</p> \[\int_{\tau} \nabla_{\theta} P(\tau \mid \theta) R(\tau)\] <p>Next, we apply the log-derivative trick in order to change the focus from the gradient of the probability to the gradient of its logarithm,</p> \[\int_{\tau} P(\tau \mid \theta) \nabla_{\theta} \log P(\tau \mid \theta) R(\tau)\] <p>Notice that the equation above essentially can be seen as the expected value of the gradient of the log-probability of the trajectory times the return, under our policy. Thus, we can say :</p> \[\mathbb{E}_{\tau \sim \pi_{\theta}}[\nabla_{\theta} \log P(\tau \mid \theta) R(\tau)]\] <p>Then, we can apply the expression that we have derived before \(\nabla_{\theta} \log P(\tau \mid \theta) = \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\) into the above equation,</p> \[\therefore \nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}\left[\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) R(\tau)\right]\] <p>As you can see above, that expression is very special since it provides a gradient of the expected return with respect to the policy parameters. This allows the policy gradient theorem to directly optimize the policy parameters \(\theta\) ignoring the parts that are independent of the policy.</p> <h2 id="introducing-reinforce-algorithm">Introducing REINFORCE Algorithm</h2> <p>After understanding the power and flexibility of Policy Gradient methods, it’s time to delve into one of its most famous implementations: the REINFORCE algorithm which stands for REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility.</p> <p>This algorithm is often considered as one of the most important and fundamental building block in the world of Reinforcement Learning.</p> <h3 id="main-idea-of-reinforce">Main Idea of REINFORCE</h3> <p>The core idea of REINFORCE that differentiate it with other methods is in its utilization of Monte Carlo methods to estimate the gradients needed for policy optimization.</p> <p>By taking sample paths through the state and action space, REINFORCE avoids the need for a model of the environment and sidesteps the computational bottleneck of calculating the true gradients. This is particularly useful when the state and/or action spaces are large or continuous, making other methods infeasible.</p> <p>For those who are not familiar with Monte Carlo approach, it is basically the process of sampling and averaging for estimating expected values in situations with large or infinite state spaces. By doing this, we can get the estimates that are unbiased without incorporating all available data.</p> <p>To understand more about the role of Monte Carlo in the REINFORCE, see the explanation below.</p> <h3 id="reinforce--policy-gradient-theorem">REINFORCE &amp; Policy Gradient Theorem</h3> <p>REINFORCE directly employs Policy Gradient theorem but takes it a step further by providing a practical way to estimate this gradient through sampling. Recall that mathematical equation for obtaining expected return \(J(\theta)\) using this theorem can be written as:</p> \[\nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) Q^{\pi}(s_t, a_t) \right]\] <p>REINFORCE simplifies this expression by utilizing the Monte Carlo estimate for \(Q^{\pi}(s_t, a_t)\), which is the sampled return \(G_t\):</p> \[\nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) G_t \right]\] <p>Here, \(G_t\) is the return obtained using a Monte Carlo estimate. Note that while \(R(\tau)\) is used in the theoretical derivation to denote the total return of a trajectory, \(G_t\) in REINFORCE serves as a practical, sampled estimate of the return from time \(t\) onwards.</p> <p>In essence, REINFORCE is a concrete implementation of the Policy Gradient method that uses Monte Carlo sampling to estimate the otherwise intractable or unknown quantities in the Policy Gradient Theorem. By doing so, it provides a computationally efficient, model-free method to optimize policies in complex environments.</p> <h3 id="mathematical-details-of-reinforce">Mathematical Details of REINFORCE</h3> <p>The REINFORCE algorithm can be understood through a sequence of mathematical steps, which are as follows:</p> <ol> <li> <p><strong>Initialize Policy Parameters</strong>: Randomly initialize the policy parameters \(\theta\).</p> </li> <li> <p><strong>Generate Episode</strong>: Using the current policy \(\pi_\theta\), generate an episode \(S_1, A_1, R_2, \ldots, S_T\).</p> </li> <li><strong>Compute Gradients</strong>: For each step \(t\) in the episode, <ul> <li>Compute the return \(G_t\).</li> <li>Compute the policy gradient \(\Delta \theta_t = \alpha \gamma^t G_t \nabla_\theta \log \pi_{\theta}(a_t \mid s_t)\).</li> </ul> </li> <li><strong>Update Policy</strong>: Update the policy parameters \(\theta\) using \(\Delta \theta\).</li> </ol> <p>The key equation that governs this update is:</p> \[\nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_{\theta}(a_t \mid s_t) G_t \right]\] <h3 id="conclusion-and-limitations">Conclusion and Limitations</h3> <p>While REINFORCE is oftenly used for its simplicity and directness, it’s also essential to recognize its limitations. The method tends to have high variance in its gradient estimates, which could lead to unstable training. However, various techniques, like using a baseline or employing advanced variance reduction methods, can alleviate these issues to some extent.</p> <p>REINFORCE is often the easy choice when you need a simple yet effective method for policy optimization, especially in high-dimensional or continuous action spaces.</p>]]></content><author><name></name></author><category term="blogpost"/><category term="Reinforcement"/><category term="Learning"/><category term="(RL),"/><category term="REINFORCE,"/><category term="Policy"/><category term="Gradient"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">What are JAX and Flax? Why those Deep Learning Frameworks can be Very Important?</title><link href="ravialdy/ravialdy.github.io/blog/2023/jaxflax/" rel="alternate" type="text/html" title="What are JAX and Flax? Why those Deep Learning Frameworks can be Very Important?"/><published>2023-10-05T13:56:00+00:00</published><updated>2023-10-05T13:56:00+00:00</updated><id>ravialdy/ravialdy.github.io/blog/2023/jaxflax</id><content type="html" xml:base="ravialdy/ravialdy.github.io/blog/2023/jaxflax/"><![CDATA[<h1 id="understanding-jax-and-flax">Understanding JAX and Flax!</h1> <p>Hello, everyone! Today, we will learn about two powerful tools for machine learning: JAX and Flax. These frameworks can be much faster than the common deep learning frameworks, such as Pytorch and Tensorflow. JAX can help us with fast math calculations, and Flax can make it easier to build neural networks. We’ll use both to make a simple image classifier for handwritten digits.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/jax-flax1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/jax-flax1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/jax-flax1-1400.webp"/> <img src="/assets/img/jax-flax1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. JAX vs. Tensorflow Speed Performance on Simple MNIST Image Classification Dataset </div> <h2 id="table-of-contents">Table of Contents</h2> <ol> <li><a href="#introduction">Introduction</a> <ul> <li><a href="#whats-the-issue-with-existing-frameworks">Issues with Existing Frameworks</a></li> </ul> </li> <li><a href="#so,-why-using-jax-and-flax">So, Why Using JAX and Flax?</a> <ul> <li><a href="#so-what-is-just-in-time-compilation">What is Just-In-Time Compilation?</a></li> <li><a href="#why-flax">Why Flax?</a></li> </ul> </li> <li><a href="#what-youll-learn">What You’ll Learn</a> <ul> <li><a href="#what-is-jax">JAX Explained</a></li> <li><a href="#what-is-flax">Flax Explained</a></li> </ul> </li> <li><a href="#jax-and-flax-w-mnist-image-classification">JAX and Flax Implementation w/ MNIST Image Classification</a></li> </ol> <h2 id="introduction">Introduction</h2> <p>Before diving into the technical details, let’s discuss why we even need frameworks like JAX and Flax when we already have powerful libraries like PyTorch and TensorFlow.</p> <h3 id="whats-the-issue-with-existing-frameworks">What’s the Issue with Existing Frameworks?</h3> <p>Don’t get me wrong—PyTorch and TensorFlow are great. They are powerful, easy to use, and have huge communities. However, they can be a bit rigid for some research needs:</p> <ul> <li><strong>Not So Easy to Customize</strong>: If you need to modify the behavior of the training loop or gradient calculations, you might find it challenging.</li> <li><strong>Debugging</strong>: Debugging can be hard, especially when computation graphs become complex.</li> </ul> <h2 id="so-why-using-jax-and-flax">So, Why Using JAX and Flax?</h2> <p>JAX is like NumPy which means that JAX’s features is its NumPy-compatible API allowing for easy transition from NumPy to JAX for numerical operations, but supercharged:</p> <ul> <li><strong>Flexibility</strong>: JAX is functional and allows for more fine-grained control, making it highly customizable.</li> <li><strong>Performance</strong>: With its just-in-time compilation, JAX can optimize your code for high-speed numerical computing.</li> </ul> <p>In many cases, it would make sense to use jax.numpy (often imported as jnp) instead of ordinary NumPy to take advantage of JAX’s features like automatic differentiation and GPU acceleration.</p> <h3 id="why-flax">Why Flax?</h3> <p>Flax is like the cherry on top of JAX:</p> <ul> <li><strong>Simplicity</strong>: Building neural networks becomes straightforward.</li> <li><strong>Extendable</strong>: Designed with research in mind, you can easily add unconventional elements to your network or training loop.</li> </ul> <h2 id="what-youll-learn">What You’ll Learn</h2> <ul> <li>What are JAX and Flax?</li> <li>How to install them</li> <li>Building a simple CNN model for MNIST image classification</li> </ul> <h3 id="what-is-jax">What is JAX?</h3> <p>JAX is a library that helps us do fast numerical operations. It can automatically make our code run faster and allows us to use the GPU easily by utilizing Just-In-Time (JIT) Compilation. It is widely used in research for its flexibility and speed.</p> <h3 id="so-what-is-just-in-time-compilation">So, what is Just-In-Time Compilation?</h3> <p>Imagine you’re a chef, and you have a recipe (your code). Traditional Python executes this recipe step-by-step, which is time-consuming. JIT compilation is like having an assistant chef who learns from watching you and then can perform the entire recipe in a much more optimized manner.</p> <p>In my experience, after applying JIT compilation properly, JAX can outperform TensorFlow and Pytorch in training speed, making it highly efficient for machine learning tasks.</p> <p>While JAX is powerful, it also requires careful coding practices. For example, to take full benefits of using JIT compilation, it is crucial to avoid changing the code inside the training loop to prevent re-compilation, which can slow down the training process. Once you grasp these nuances, harnessing JAX’s full power becomes straightforward.</p> <h3 id="what-is-flax">What is Flax?</h3> <p>Flax is built on top of JAX and provides a simple way to build and train neural networks. It is designed to be flexible, making it a good choice for research projects.</p> <h2 id="jax-and-flax-w-mnist-image-classification">JAX and Flax w/ MNIST Image Classification</h2> <p>Let’s go into simple practical implementation on MNIST dataset. MNIST Image Classification is a simple but fundamental task in machine learning. It gives us a perfect playground to explore JAX and Flax without getting lost in the complexity of the task itself.</p> <h3 id="installing-jax-and-flax">Installing JAX and Flax</h3> <p>First, let’s install JAX and Flax. Open your terminal and run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">--upgrade</span> jax jaxlib
pip <span class="nb">install </span>flax
</code></pre></div></div> <h3 id="import-libraries">Import Libraries</h3> <p>Let’s import all the libraries we need.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">flax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">from</span> <span class="n">flax</span> <span class="kn">import</span> <span class="n">linen</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">tensorflow.keras</span> <span class="kn">import</span> <span class="n">datasets</span>
</code></pre></div></div> <h3 id="prepare-the-data">Prepare the Data</h3> <p>We’ll use the MNIST dataset, which is a set of 28x28 grayscale images of handwritten digits. We normalize the images by dividing by 255, as this scales the pixel values between 0 and 1, which generally helps the model to learn more efficiently.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>

<span class="c1"># Normalize and reshape the data using JAX's NumPy
</span><span class="n">train_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">train_images</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">test_images</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
</code></pre></div></div> <h3 id="create-the-model">Create the Model</h3> <p>Now let’s build a simple Convolutional Neural Network (CNN) using Flax.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define the CNN model using Flax
</span><span class="k">class</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    A simple CNN model for MNIST classification.
    </span><span class="sh">"""</span>
    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">window_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">window_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">256</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <h3 id="initialize-the-model">Initialize the Model</h3> <p>Before using our model, we need to initialize it. Initialization is crucial because it sets the initial random weights of the model, which will be updated during training.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">CNN</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <h3 id="training">Training</h3> <p>Now, let’s train the model. But first, let’s initialize the optimizer. We will use the Adam optimizer provided by Optax. Optax is a flexible and extensible optimization library that provides a wide range of optimization algorithms.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize the optimizer
</span><span class="kn">import</span> <span class="n">optax</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optax</span><span class="p">.</span><span class="nf">adam</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">opt_state</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
</code></pre></div></div> <p>We won’t go into detail about training loops here, but you can use JAX’s <code class="language-plaintext highlighter-rouge">grad</code> function to compute gradients and update the model weights. We use JAX’s <code class="language-plaintext highlighter-rouge">jit</code> function to compile the <code class="language-plaintext highlighter-rouge">train_step</code> function, speeding up our training loop. Just-In-Time (JIT) compilation improves the performance by compiling Python functions to optimized machine code at runtime.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">jit</span><span class="p">,</span> <span class="n">value_and_grad</span>
<span class="kn">from</span> <span class="n">jax.scipy.special</span> <span class="kn">import</span> <span class="n">logsumexp</span>

<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">images</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Computes the loss between the predicted labels and true labels.
    </span><span class="sh">"""</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="nc">CNN</span><span class="p">().</span><span class="nf">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
    <span class="n">logprobs</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="nf">logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">logprobs</span> <span class="o">*</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">opt_state</span><span class="p">:</span> <span class="n">optax</span><span class="p">.</span><span class="n">OptState</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">images</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Performs a single training step.
    </span><span class="sh">"""</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="nf">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">updates</span><span class="p">,</span> <span class="n">new_opt_state</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
    <span class="n">new_params</span> <span class="o">=</span> <span class="n">optax</span><span class="p">.</span><span class="nf">apply_updates</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_opt_state</span><span class="p">,</span> <span class="n">new_params</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <h3 id="pre-compiling-functions-for-faster-execution">Pre-Compiling Functions for Faster Execution</h3> <p>You might have noticed a somewhat unusual block of code right before our training loop:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pre-compile functions
# Use a small subset of data to trigger JIT compilation
</span><span class="n">sample_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">sample_labels</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">jit_loss_fn</span> <span class="o">=</span> <span class="nf">jit</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)</span>
<span class="n">jit_train_step</span> <span class="o">=</span> <span class="nf">jit</span><span class="p">(</span><span class="n">train_step</span><span class="p">)</span>

<span class="c1"># Trigger JIT compilation
</span><span class="n">_</span> <span class="o">=</span> <span class="nf">jit_loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">sample_images</span><span class="p">,</span> <span class="n">sample_labels</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="nf">jit_train_step</span><span class="p">(</span><span class="n">opt_state</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">sample_images</span><span class="p">,</span> <span class="n">sample_labels</span><span class="p">)</span>
</code></pre></div></div> <p>What’s going on with the code above? This block of code is a technique to “warm up” or pre-compile our JAX functions, so they run faster during our training loop.</p> <p>We also create a small subset of dummy data, sample_images and sample_labels, that matches the shape and type of our real data. Then, we use JAX’s jit function to indicate that loss_fn and train_step should be JIT compiled.</p> <p>Finally, we run these JIT-compiled functions once using our dummy data. This step is crucial as it triggers the JIT compilation process, converting our Python functions into highly optimized machine code.</p> <h3 id="why-do-we-need-this">Why Do We Need This?</h3> <p>JAX uses Just-In-Time (JIT) compilation to optimize our code. JIT compilation works by looking at the operations in our functions and creating an optimized version of these functions. However, JIT compilation itself takes time. By pre-compiling, we do this step before entering our training loop, ensuring that our code runs at maximum speed when it matters the most.</p> <p>This pre-compilation step is particularly helpful in scenarios where the training loop has to run multiple times, helping us save time in the long run.</p> <p>Next, let’s divide the training data into training and validation sets:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Split the training data into training and validation sets
</span><span class="n">train_images</span><span class="p">,</span> <span class="n">val_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">val_labels</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># One-hot encode labels
</span><span class="n">train_labels_onehot</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">train_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">val_labels_onehot</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">val_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></div></div> <p>Now we can write the training loop.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pickle</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="c1"># Initialize variables to keep track of best model and performance
</span><span class="n">best_val_loss</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">inf</span><span class="sh">'</span><span class="p">)</span>
<span class="n">best_params</span> <span class="o">=</span> <span class="bp">None</span>

<span class="c1"># Lists to keep track of loss values for plotting
</span><span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="c1"># Training loop
</span>    <span class="n">train_loss_epoch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_images</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">train_images</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">])</span>
        <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">train_labels_onehot</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">])</span>
        <span class="n">opt_state</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">opt_state</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">batch_images</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">)</span>
        <span class="n">train_loss_epoch</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    
    <span class="n">avg_train_loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">train_loss_epoch</span><span class="p">))</span>
    <span class="n">train_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">avg_train_loss</span><span class="p">)</span>

    <span class="c1"># Validation loop
</span>    <span class="n">val_loss_epoch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">val_images</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">val_images</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">])</span>
        <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">val_labels_onehot</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">])</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch_images</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">)</span>
        <span class="n">val_loss_epoch</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>

    <span class="n">avg_val_loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">val_loss_epoch</span><span class="p">))</span>
    <span class="n">val_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">avg_val_loss</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, Train Loss: </span><span class="si">{</span><span class="n">avg_train_loss</span><span class="si">}</span><span class="s">, Val Loss: </span><span class="si">{</span><span class="n">avg_val_loss</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Save best model
</span>    <span class="k">if</span> <span class="n">avg_val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
        <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">avg_val_loss</span>
        <span class="n">best_params</span> <span class="o">=</span> <span class="n">params</span>

<span class="c1"># Calculate the training time with JAX
</span><span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">jax_training_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training time with JAX: </span><span class="si">{</span><span class="n">jax_training_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Save the best model parameters to a file
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">best_model_params.pkl</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">wb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="n">best_params</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div></div> <p>Then, we can plot the training and validation loss like below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Train Loss</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Validation Loss</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Epochs</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Loss</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/jax-performance-mnist-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/jax-performance-mnist-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/jax-performance-mnist-1400.webp"/> <img src="/assets/img/jax-performance-mnist.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. The plot of training and validation loss using JAX framework on MNIST dataset </div> <p>And that’s it! You’ve built a simple CNN for MNIST digit classification using JAX and Flax. Now, to get the point on why using those frameworks can be really crucial, let’s compare its training time with the training time when using tensorflow. Note that we measured the time taken to train a Convolutional Neural Network (CNN) on the MNIST dataset using both JAX and TensorFlow.</p> <p>Also note that for fair comparison, both models have the same architecture and are trained for the same number of epochs and batch size.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">models</span>

<span class="c1"># Preparing data
</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">val_images</span> <span class="o">=</span> <span class="n">val_images</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>

<span class="n">train_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">to_categorical</span><span class="p">(</span><span class="n">train_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">val_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">to_categorical</span><span class="p">(</span><span class="n">val_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Creating the model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">AveragePooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">AveragePooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Compiling the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># Measuring time for training
</span><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="c1"># Fitting the model
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
    <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="n">non_jax_training_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training time without JAX: </span><span class="si">{</span><span class="n">non_jax_training_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>In machine learning, training time is a crucial factor. Faster training allows for more iterations and experiments, speeding up the development process. Below is a bar graph that shows the training time for each framework.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Labels and corresponding values
</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">JAX</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">TensorFlow</span><span class="sh">'</span><span class="p">]</span>
<span class="n">times</span> <span class="o">=</span> <span class="p">[</span><span class="n">jax_training_time</span><span class="p">,</span> <span class="n">non_jax_training_time</span><span class="p">]</span>

<span class="c1"># Create the bar chart
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">barh</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">times</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Training Time (seconds)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Training Time Comparison: JAX vs TensorFlow</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Annotate with the exact times
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">time</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">times</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="n">time</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> s</span><span class="sh">'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="sh">'</span><span class="s">center</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/jax-flax-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/jax-flax-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/jax-flax-1400.webp"/> <img src="/assets/img/jax-flax.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. JAX vs. Tensorflow Speed Performance on Simple MNIST Image Classification Dataset </div> <p>As you can see, using JAX in simple dataset like MNIST can increase the speed significantly. You can imagine how fast it is when implementing it in bigger datasets and much more complex tasks!!</p> <h3 id="conclusion">Conclusion</h3> <p>JAX and Flax are powerful tools for machine learning research and projects. JAX provides fast and flexible numerical operations, while Flax offers a simple and extendable way to build neural networks.</p> <p>I hope this post helps you understand the basics of JAX and Flax. Below I also attach runned jupyter notebook about this blogpost. Happy coding!</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/Jax_and_Flax_Intro.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="blogpost"/><category term="JAX,"/><category term="Flax,"/><category term="Deep"/><category term="Learning"/><category term="Frameworks"/><summary type="html"><![CDATA[Understanding JAX and Flax!]]></summary></entry><entry><title type="html">Prompt Learning with Optimal Transport Presentation Slides</title><link href="ravialdy/ravialdy.github.io/blog/2023/plot-paper-review/" rel="alternate" type="text/html" title="Prompt Learning with Optimal Transport Presentation Slides"/><published>2023-07-31T12:57:00+00:00</published><updated>2023-07-31T12:57:00+00:00</updated><id>ravialdy/ravialdy.github.io/blog/2023/plot-paper-review</id><content type="html" xml:base="ravialdy/ravialdy.github.io/blog/2023/plot-paper-review/"><![CDATA[<p>Here is the presentation slides when I did paper review about Prompt Learning with Optimal Transport paper.</p> <p><a href="/assets/pdf/(Ravialdy) Review about Prompt Learning with Optimal Transport.pdf">Download Slides</a></p>]]></content><author><name></name></author><category term="slides"/><category term="prompt"/><category term="learning,"/><category term="optimal"/><category term="transport"/><summary type="html"><![CDATA[Here is the presentation slides that I have created when explaining about Prompt Learning with Optimal Transport paper.]]></summary></entry><entry><title type="html">Visual Prompting For Adversarial Robustness Presentation Slides</title><link href="ravialdy/ravialdy.github.io/blog/2023/cavp-paper-review/" rel="alternate" type="text/html" title="Visual Prompting For Adversarial Robustness Presentation Slides"/><published>2023-07-29T12:57:00+00:00</published><updated>2023-07-29T12:57:00+00:00</updated><id>ravialdy/ravialdy.github.io/blog/2023/cavp-paper-review</id><content type="html" xml:base="ravialdy/ravialdy.github.io/blog/2023/cavp-paper-review/"><![CDATA[<p>Here is the presentation slides when I did paper review about Visual Prompting For Adversarial Robustness paper.</p> <p><a href="/assets/pdf/(Ravialdy) Paper Review of Visual Prompting For Adversarial Robustness.pdf">Download Slides</a></p>]]></content><author><name></name></author><category term="slides"/><category term="black-box"/><category term="model,"/><category term="visual"/><category term="prompting,"/><category term="adversarial"/><category term="attack"/><summary type="html"><![CDATA[Here is the presentation slides that I have created when explaining about Visual Prompting For Adversarial Robustness paper.]]></summary></entry><entry><title type="html">Reinforcement Learning from Human Feedback (RLHF) Presentation Slides</title><link href="ravialdy/ravialdy.github.io/blog/2023/rlhf-review/" rel="alternate" type="text/html" title="Reinforcement Learning from Human Feedback (RLHF) Presentation Slides"/><published>2023-07-28T12:57:00+00:00</published><updated>2023-07-28T12:57:00+00:00</updated><id>ravialdy/ravialdy.github.io/blog/2023/rlhf-review</id><content type="html" xml:base="ravialdy/ravialdy.github.io/blog/2023/rlhf-review/"><![CDATA[<p>Here is the presentation slides when I learn about Reinforcement Learning from Human Feedback (RLHF) used in GPT models.</p> <p><a href="/assets/pdf/(Ravialdy) Reinforcement Learning From Human Feedback.pdf">Download Slides</a></p>]]></content><author><name></name></author><category term="slides"/><category term="Reinforcement"/><category term="Learning"/><category term="(RL),"/><category term="RLHF,"/><category term="Natural"/><category term="Language"/><category term="Processing"/><category term="(NLP)"/><summary type="html"><![CDATA[Here is the presentation slides that I have created when explaining about Reinforcement Learning from Human Feedback (RLHF).]]></summary></entry><entry><title type="html">Black-Box Visual Prompting for Robust Transfer Learning Presentation Slides</title><link href="ravialdy/ravialdy.github.io/blog/2023/blackvip-paper-review/" rel="alternate" type="text/html" title="Black-Box Visual Prompting for Robust Transfer Learning Presentation Slides"/><published>2023-07-27T12:57:00+00:00</published><updated>2023-07-27T12:57:00+00:00</updated><id>ravialdy/ravialdy.github.io/blog/2023/blackvip-paper-review</id><content type="html" xml:base="ravialdy/ravialdy.github.io/blog/2023/blackvip-paper-review/"><![CDATA[<p>Here is the presentation slides when I did paper review about Black-Box Visual Prompting for Robust Transfer Learning (BlackVIP) paper.</p> <p><a href="/assets/pdf/(Ravialdy) Paper Review about BlackVIP_Black-Box Visual Prompting for Robust Transfer Learning.pdf">Download Slides</a></p>]]></content><author><name></name></author><category term="slides"/><category term="black-box"/><category term="model,"/><category term="visual"/><category term="prompting"/><summary type="html"><![CDATA[Here is the presentation slides that I have created when explaining about Black-Box Visual Prompting for Robust Transfer Learning paper.]]></summary></entry></feed>