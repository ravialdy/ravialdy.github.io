<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="ravialdy/ravialdy.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="ravialdy/ravialdy.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-10-06T06:49:28+00:00</updated><id>ravialdy/ravialdy.github.io/feed.xml</id><title type="html">Ravialdy’s Personal Website</title><subtitle>Experienced AI Researcher and Data Scientist with over 3 years of experience in computer vision, machine learning, and AI model deployment. Passionate about solving complex problems and advancing the state of technology. </subtitle><entry><title type="html">What are JAX and Flax? Why those Deep Learning Frameworks can be Very Important?</title><link href="ravialdy/ravialdy.github.io/blogpost/2023/10/05/jaxflax.html" rel="alternate" type="text/html" title="What are JAX and Flax? Why those Deep Learning Frameworks can be Very Important?"/><published>2023-10-05T13:56:00+00:00</published><updated>2023-10-05T13:56:00+00:00</updated><id>ravialdy/ravialdy.github.io/blogpost/2023/10/05/jaxflax</id><content type="html" xml:base="ravialdy/ravialdy.github.io/blogpost/2023/10/05/jaxflax.html"><![CDATA[<h1 id="understanding-jax-and-flax">Understanding JAX and Flax!</h1> <p>Hello, everyone! Today, we will learn about two powerful tools for machine learning: JAX and Flax. These frameworks can be much faster than the common ones, such as Pytorch and Tensorflow. JAX helps us with fast math calculations, and Flax makes it easier to build neural networks. We’ll use both to make a simple image classifier for handwritten digits.</p> <p><img src="/assets/img/jax-flax1.png" alt="JAX vs. Tensorflow Speed Performance on Simple MNIST Image Classification Dataset"/></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    Figure1. JAX vs. Tensorflow Speed Performance on Simple MNIST Image Classification Dataset
</code></pre></div></div> <h2 id="introduction">Introduction</h2> <p>Before diving into the technical details, let’s discuss why we even need frameworks like JAX and Flax when we already have powerful libraries like PyTorch and TensorFlow.</p> <h3 id="whats-the-issue-with-existing-frameworks">What’s the Issue with Existing Frameworks?</h3> <p>Don’t get me wrong—PyTorch and TensorFlow are great. They are versatile, powerful, and have huge communities. However, they can be a bit rigid for some research needs:</p> <ul> <li><strong>Not So Easy to Customize</strong>: If you need to modify the behavior of the training loop or gradient calculations, you might find it challenging.</li> <li><strong>Debugging</strong>: Debugging can be hard, especially when computation graphs become complex.</li> </ul> <h3 id="why-jax">Why JAX?</h3> <p>JAX is like NumPy which means that JAX’s features is its NumPy-compatible API allowing for easy transition from NumPy to JAX for numerical operations, but supercharged:</p> <ul> <li><strong>Flexibility</strong>: JAX is functional and allows for more fine-grained control, making it highly customizable.</li> <li><strong>Performance</strong>: With its just-in-time compilation, JAX can optimize your code for high-speed numerical computing.</li> </ul> <p>In many cases, it would make sense to use jax.numpy (often imported as jnp) instead of ordinary NumPy to take advantage of JAX’s features like automatic differentiation and GPU acceleration.</p> <h3 id="why-flax">Why Flax?</h3> <p>Flax is like the cherry on top of JAX:</p> <ul> <li><strong>Simplicity</strong>: Building neural networks becomes straightforward.</li> <li><strong>Extendable</strong>: Designed with research in mind, you can easily add unconventional elements to your network or training loop.</li> </ul> <h2 id="what-youll-learn">What You’ll Learn</h2> <ul> <li>What are JAX and Flax?</li> <li>How to install them</li> <li>Building a simple CNN model for MNIST image classification</li> </ul> <h3 id="what-is-jax">What is JAX?</h3> <p>JAX is a library that helps us do fast numerical operations. It can automatically make our code run faster and allows us to use the GPU easily by utilizing Just-In-Time (JIT) Compilation. It is widely used in research for its flexibility and speed.</p> <h3 id="so-what-is-just-in-time-compilation">So, what is Just-In-Time Compilation?</h3> <p>Imagine you’re a chef, and you have a recipe (your code). Traditional Python executes this recipe step-by-step, which is time-consuming. JIT compilation is like having an assistant chef who learns from watching you and then can perform the entire recipe in a much more optimized manner. This is particularly beneficial for repetitive tasks like the training loops in machine learning models.</p> <p>In my experience, after applying JIT compilation properly, JAX outperformed TensorFlow and Pytorch in training speed, making it highly efficient for machine learning tasks. While JAX is powerful, it also requires careful coding practices. For example, to make full use of JIT compilation, it is crucial to avoid changing the code inside the training loop to prevent re-compilation, which can slow down the training process. Once you grasp these nuances, harnessing JAX’s full power becomes straightforward.</p> <h3 id="what-is-flax">What is Flax?</h3> <p>Flax is built on top of JAX and provides a simple way to build and train neural networks. It is designed to be flexible, making it a good choice for research projects.</p> <h2 id="jax-and-flax-w-mnist-image-classification">JAX and Flax w/ MNIST Image Classification</h2> <p>Let’s go into simple practical implementation on MNIST dataset. Before you build a skyscraper, you need to know how to make a small house. The same goes for machine learning. Understanding how to build a simple image classifier can give you the foundation you need to tackle more complex problems later. MNIST Image Classification is a simple but fundamental task in machine learning. It gives us a perfect playground to explore JAX and Flax without getting lost in the complexity of the task itself.</p> <h3 id="installing-jax-and-flax">Installing JAX and Flax</h3> <p>First, let’s install JAX and Flax. Open your terminal and run:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">--upgrade</span> jax jaxlib
pip <span class="nb">install </span>flax
</code></pre></div></div> <h3 id="import-libraries">Import Libraries</h3> <p>Let’s import all the libraries we need.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">flax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">from</span> <span class="n">flax</span> <span class="kn">import</span> <span class="n">linen</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">tensorflow.keras</span> <span class="kn">import</span> <span class="n">datasets</span>
</code></pre></div></div> <h3 id="prepare-the-data">Prepare the Data</h3> <p>We’ll use the MNIST dataset, which is a set of 28x28 grayscale images of handwritten digits. We normalize the images by dividing by 255, as this scales the pixel values between 0 and 1, which generally helps the model to learn more efficiently.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>

<span class="c1"># Normalize and reshape the data using JAX's NumPy
</span><span class="n">train_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">train_images</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">test_images</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
</code></pre></div></div> <h3 id="create-the-model">Create the Model</h3> <p>Now let’s build a simple Convolutional Neural Network (CNN) using Flax.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define the CNN model using Flax
</span><span class="k">class</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    A simple CNN model for MNIST classification.
    </span><span class="sh">"""</span>
    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">window_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">window_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">256</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <h3 id="initialize-the-model">Initialize the Model</h3> <p>Before using our model, we need to initialize it. Initialization is crucial because it sets the initial random weights of the model, which will be updated during training.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">CNN</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <h3 id="training">Training</h3> <p>Now, let’s train the model. But first, let’s initialize the optimizer. We will use the Adam optimizer provided by Optax. Optax is a flexible and extensible optimization library that provides a wide range of optimization algorithms.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize the optimizer
</span><span class="kn">import</span> <span class="n">optax</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optax</span><span class="p">.</span><span class="nf">adam</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">opt_state</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
</code></pre></div></div> <p>We won’t go into detail about training loops here, but you can use JAX’s <code class="language-plaintext highlighter-rouge">grad</code> function to compute gradients and update the model weights. We use JAX’s <code class="language-plaintext highlighter-rouge">jit</code> function to compile the <code class="language-plaintext highlighter-rouge">train_step</code> function, speeding up our training loop. Just-In-Time (JIT) compilation improves the performance by compiling Python functions to optimized machine code at runtime.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">jit</span><span class="p">,</span> <span class="n">value_and_grad</span>
<span class="kn">from</span> <span class="n">jax.scipy.special</span> <span class="kn">import</span> <span class="n">logsumexp</span>

<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">images</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Computes the loss between the predicted labels and true labels.
    </span><span class="sh">"""</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="nc">CNN</span><span class="p">().</span><span class="nf">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
    <span class="n">logprobs</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="nf">logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">logprobs</span> <span class="o">*</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">opt_state</span><span class="p">:</span> <span class="n">optax</span><span class="p">.</span><span class="n">OptState</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">images</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Performs a single training step.
    </span><span class="sh">"""</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="nf">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">updates</span><span class="p">,</span> <span class="n">new_opt_state</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">)</span>
    <span class="n">new_params</span> <span class="o">=</span> <span class="n">optax</span><span class="p">.</span><span class="nf">apply_updates</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_opt_state</span><span class="p">,</span> <span class="n">new_params</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <h3 id="pre-compiling-functions-for-faster-execution">Pre-Compiling Functions for Faster Execution</h3> <p>You might have noticed a somewhat unusual block of code right before our training loop:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pre-compile functions
# Use a small subset of data to trigger JIT compilation
</span><span class="n">sample_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">sample_labels</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">jit_loss_fn</span> <span class="o">=</span> <span class="nf">jit</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)</span>
<span class="n">jit_train_step</span> <span class="o">=</span> <span class="nf">jit</span><span class="p">(</span><span class="n">train_step</span><span class="p">)</span>

<span class="c1"># Trigger JIT compilation
</span><span class="n">_</span> <span class="o">=</span> <span class="nf">jit_loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">sample_images</span><span class="p">,</span> <span class="n">sample_labels</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="nf">jit_train_step</span><span class="p">(</span><span class="n">opt_state</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">sample_images</span><span class="p">,</span> <span class="n">sample_labels</span><span class="p">)</span>
</code></pre></div></div> <p>What’s going on with the code above? This block of code is a technique to “warm up” or pre-compile our JAX functions, so they run faster during our training loop.</p> <p>Small Subset of Data: We create a small subset of dummy data, sample_images and sample_labels, that matches the shape and type of our real data. JIT Compilation: We then use JAX’s jit function to indicate that loss_fn and train_step should be JIT compiled. Trigger Compilation: Finally, we run these JIT-compiled functions once using our dummy data. This step is crucial as it triggers the JIT compilation process, converting our Python functions into highly optimized machine code.</p> <h3 id="why-do-we-need-this">Why Do We Need This?</h3> <p>JAX uses Just-In-Time (JIT) compilation to optimize our code. JIT compilation works by looking at the operations in our functions and creating an optimized version of these functions. However, JIT compilation itself takes time. By pre-compiling, we do this step before entering our training loop, ensuring that our code runs at maximum speed when it matters the most.</p> <p>This pre-compilation step is particularly helpful in scenarios where the training loop has to run multiple times, helping us save time in the long run.</p> <p>Next, let’s divide the training data into training and validation sets:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Split the training data into training and validation sets
</span><span class="n">train_images</span><span class="p">,</span> <span class="n">val_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">val_labels</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># One-hot encode labels
</span><span class="n">train_labels_onehot</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">train_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">val_labels_onehot</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">val_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></div></div> <p>Now we can write the training loop.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pickle</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="c1"># Initialize variables to keep track of best model and performance
</span><span class="n">best_val_loss</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">inf</span><span class="sh">'</span><span class="p">)</span>
<span class="n">best_params</span> <span class="o">=</span> <span class="bp">None</span>

<span class="c1"># Lists to keep track of loss values for plotting
</span><span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="c1"># Training loop
</span>    <span class="n">train_loss_epoch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_images</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">train_images</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">])</span>
        <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">train_labels_onehot</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">])</span>
        <span class="n">opt_state</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">opt_state</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">batch_images</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">)</span>
        <span class="n">train_loss_epoch</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    
    <span class="n">avg_train_loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">train_loss_epoch</span><span class="p">))</span>
    <span class="n">train_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">avg_train_loss</span><span class="p">)</span>

    <span class="c1"># Validation loop
</span>    <span class="n">val_loss_epoch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">val_images</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">batch_images</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">val_images</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">])</span>
        <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">val_labels_onehot</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">])</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch_images</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">)</span>
        <span class="n">val_loss_epoch</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>

    <span class="n">avg_val_loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">val_loss_epoch</span><span class="p">))</span>
    <span class="n">val_losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">avg_val_loss</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, Train Loss: </span><span class="si">{</span><span class="n">avg_train_loss</span><span class="si">}</span><span class="s">, Val Loss: </span><span class="si">{</span><span class="n">avg_val_loss</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Save best model
</span>    <span class="k">if</span> <span class="n">avg_val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
        <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">avg_val_loss</span>
        <span class="n">best_params</span> <span class="o">=</span> <span class="n">params</span>

<span class="c1"># Calculate the training time with JAX
</span><span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">jax_training_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training time with JAX: </span><span class="si">{</span><span class="n">jax_training_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Save the best model parameters to a file
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">best_model_params.pkl</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">wb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="n">best_params</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div></div> <p>Then, we can plot the training and validation loss like below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Train Loss</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Validation Loss</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Epochs</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Loss</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p><img src="/assets/img/jax-performance-mnist.png" alt="The plot of training and validation loss using JAX framework on MNIST dataset"/></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    Figure2. The plot of training and validation loss using JAX framework on MNIST dataset
</code></pre></div></div> <p>And that’s it! You’ve built a simple CNN for MNIST digit classification using JAX and Flax. Now, to get the point on why using those frameworks can be really crucial, let’s compare its training time with the training time when using tensorflow. Note that we measured the time taken to train a Convolutional Neural Network (CNN) on the MNIST dataset using both JAX and TensorFlow. For fair comparison, both models have the same architecture and are trained for the same number of epochs and batch size.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">models</span>

<span class="c1"># Preparing data
</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">val_images</span> <span class="o">=</span> <span class="n">val_images</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>

<span class="n">train_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">to_categorical</span><span class="p">(</span><span class="n">train_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">val_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">to_categorical</span><span class="p">(</span><span class="n">val_labels</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Creating the model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">AveragePooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">AveragePooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Compiling the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># Measuring time for training
</span><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="c1"># Fitting the model
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
    <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="n">non_jax_training_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Training time without JAX: </span><span class="si">{</span><span class="n">non_jax_training_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>In machine learning, training time is a crucial factor. Faster training allows for more iterations and experiments, speeding up the development process. Below is a bar graph that shows the training time for each framework.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Labels and corresponding values
</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">JAX</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">TensorFlow</span><span class="sh">'</span><span class="p">]</span>
<span class="n">times</span> <span class="o">=</span> <span class="p">[</span><span class="n">jax_training_time</span><span class="p">,</span> <span class="n">non_jax_training_time</span><span class="p">]</span>

<span class="c1"># Create the bar chart
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">barh</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">times</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Training Time (seconds)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Training Time Comparison: JAX vs TensorFlow</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Annotate with the exact times
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">time</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">times</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="n">time</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">time</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> s</span><span class="sh">'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="sh">'</span><span class="s">center</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p><img src="/assets/img/jax-flax.png" alt="JAX vs. Tensorflow Speed Performance on Simple MNIST Image Classification Dataset"/></p> <p>As you can see, using JAX in simple dataset like MNIST can increase the speed significantly. You can imagine how fast it is when implementing it in bigger datasets and much more complex tasks!!</p> <h3 id="conclusion">Conclusion</h3> <p>JAX and Flax are powerful tools for machine learning research and projects. JAX provides fast and flexible numerical operations, while Flax offers a simple and extendable way to build neural networks.</p> <p>I hope this post helps you understand the basics of JAX and Flax. Below I also attach runned jupyter notebook about this blogpost. Happy coding!</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/Jax_and_Flax_Intro.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="blogpost"/><category term="JAX,"/><category term="Flax,"/><category term="Deep"/><category term="Learning"/><category term="Frameworks"/><summary type="html"><![CDATA[Understanding JAX and Flax!]]></summary></entry><entry><title type="html">Visual ChatGPT Presentation Slides</title><link href="ravialdy/ravialdy.github.io/slides/2023/07/25/jupyter-notebook.html" rel="alternate" type="text/html" title="Visual ChatGPT Presentation Slides"/><published>2023-07-25T12:57:00+00:00</published><updated>2023-07-25T12:57:00+00:00</updated><id>ravialdy/ravialdy.github.io/slides/2023/07/25/jupyter-notebook</id><content type="html" xml:base="ravialdy/ravialdy.github.io/slides/2023/07/25/jupyter-notebook.html"><![CDATA[<p>Here is the presentation slides that I have created when explaining about Visual ChatGPT paper.</p> <p><a href="/assets/pdf/Paper Review _Visual ChatGPT.pdf">Download Slides</a></p>]]></content><author><name></name></author><category term="slides"/><category term="ChatGPT,"/><category term="GPT-4,"/><category term="Large"/><category term="Language"/><category term="Model"/><category term="(LLM)"/><summary type="html"><![CDATA[Here is the presentation slides that I have created when explaining about Visual ChatGPT paper.]]></summary></entry><entry><title type="html">GPT-4 Presentation Slides</title><link href="ravialdy/ravialdy.github.io/slides/2023/07/12/post-bibliography.html" rel="alternate" type="text/html" title="GPT-4 Presentation Slides"/><published>2023-07-12T13:56:00+00:00</published><updated>2023-07-12T13:56:00+00:00</updated><id>ravialdy/ravialdy.github.io/slides/2023/07/12/post-bibliography</id><content type="html" xml:base="ravialdy/ravialdy.github.io/slides/2023/07/12/post-bibliography.html"><![CDATA[<p>Here is the presentation slides that I have created when explaining about GPT-4. You can download the slides below if you are interested with that topic :)</p> <p><a href="/assets/pdf/Paper Review _GPT-4.pdf">Download Slides</a></p>]]></content><author><name></name></author><category term="slides"/><category term="GPT-4,"/><category term="Large"/><category term="Language"/><category term="Model"/><category term="(LLM),"/><category term="Reinforcement"/><category term="Learning"/><category term="from"/><category term="Human"/><category term="Feedback"/><category term="(RLHF)"/><summary type="html"><![CDATA[Here is the presentation slides that I have created when explaining about GPT-4. You can download the slides below if you are interested with that topic :)]]></summary></entry><entry><title type="html">Proximal Policy Optimization (PPO) Presentation Slides</title><link href="ravialdy/ravialdy.github.io/slides/2023/05/12/custom-blockquotes.html" rel="alternate" type="text/html" title="Proximal Policy Optimization (PPO) Presentation Slides"/><published>2023-05-12T19:53:00+00:00</published><updated>2023-05-12T19:53:00+00:00</updated><id>ravialdy/ravialdy.github.io/slides/2023/05/12/custom-blockquotes</id><content type="html" xml:base="ravialdy/ravialdy.github.io/slides/2023/05/12/custom-blockquotes.html"><![CDATA[<p>Below is the presentation slides that I have created when explaining about Proximal Policy Optimization (PPO) paper.</p> <p><a href="/assets/pdf/(Ravialdy) Paper Review-Proximal Policy Optimization 1.pdf">Download Slides</a></p>]]></content><author><name></name></author><category term="slides"/><category term="Proximal"/><category term="Policy"/><category term="Optimization"/><category term="(PPO),"/><category term="Reinforcement"/><category term="Learning,"/><category term="Policy"/><category term="Gradient"/><summary type="html"><![CDATA[Presentation slides that I have created when explaining about Proximal Policy Optimization (PPO) paper.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="ravialdy/ravialdy.github.io/2022/04/23/displaying-external-posts-on-your-al-folio-blog.html" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>ravialdy/ravialdy.github.io/2022/04/23/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="ravialdy/ravialdy.github.io/2022/04/23/displaying-external-posts-on-your-al-folio-blog.html"><![CDATA[]]></content><author><name></name></author></entry></feed>