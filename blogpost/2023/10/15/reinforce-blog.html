<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Understanding Phenomenal REINFORCE Policy Gradient Method | Ravialdy's Blog</title> <meta name="author" content="Ravialdy Hidayat"> <meta name="description" content="Experienced AI Researcher and Data Scientist with over 3 years of experience in computer vision, machine learning, and AI model deployment. Passionate about solving complex problems and advancing the state of technology. "> <meta name="keywords" content="AI Research, Data Science, Computer Vision, Model Deployment, Machine Learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="ravialdy/ravialdy.github.io/blogpost/2023/10/15/reinforce-blog.html"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Ravialdy's Blog</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding Phenomenal REINFORCE Policy Gradient Method</h1> <p class="post-meta">October 15, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/reinforcement"> <i class="fas fa-hashtag fa-sm"></i> Reinforcement</a>   <a href="/blog/tag/learning"> <i class="fas fa-hashtag fa-sm"></i> Learning</a>   <a href="/blog/tag/rl"> <i class="fas fa-hashtag fa-sm"></i> (RL),</a>   <a href="/blog/tag/reinforce"> <i class="fas fa-hashtag fa-sm"></i> REINFORCE,</a>   <a href="/blog/tag/policy"> <i class="fas fa-hashtag fa-sm"></i> Policy</a>   <a href="/blog/tag/gradient"> <i class="fas fa-hashtag fa-sm"></i> Gradient</a>     ·   <a href="/blog/category/blogpost"> <i class="fas fa-tag fa-sm"></i> blogpost</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#introduction">Introduction</a> <ul> <li class="toc-entry toc-h3"><a href="#brief-recap-about-reinforcement-learning-framework">Brief Recap about Reinforcement Learning Framework</a></li> <li class="toc-entry toc-h3"><a href="#what-are-we-trying-to-optimize">What Are We Trying to Optimize?</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#the-problem-with-traditional-methods">The Problem with Traditional Methods</a></li> <li class="toc-entry toc-h2"> <a href="#the-policy-gradient-theorem">The Policy Gradient Theorem</a> <ul> <li class="toc-entry toc-h3"><a href="#the-formal-objective">The Formal Objective</a></li> <li class="toc-entry toc-h3"><a href="#the-policy-gradient-theorem-in-detail">The Policy Gradient Theorem in Detail</a></li> <li class="toc-entry toc-h3"><a href="#the-log-derivative-trick">The Log-Derivative Trick</a></li> <li class="toc-entry toc-h3"><a href="#why-should-we-care-about-policy-gradients">Why Should We Care About Policy Gradients?</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#introducing-phenomenal-reinforce-algorithm">Introducing Phenomenal REINFORCE Algorithm</a> <ul> <li class="toc-entry toc-h3"><a href="#relation-between-reinforce-and-policy-gradient-theorem">Relation between REINFORCE and Policy Gradient Theorem</a></li> <li class="toc-entry toc-h3"><a href="#mathematical-details-of-reinforce-algorithm">Mathematical Details of REINFORCE Algorithm</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>Welcome to my blog post! Today we’re going to discuss about a very fascinating topic in the world of AI and Reinforcement Learning (RL) — the Policy Gradient REINFORCE Method. This method is quite famous for solving some complex problems in RL. Don’t worry if you’re new to this field; I’ll try to keep things simple and easy to understand. First of all, I will be focusing on the background of the REINFORCE method and why it was proposed in the first place.</p> <h3 id="brief-recap-about-reinforcement-learning-framework">Brief Recap about Reinforcement Learning Framework</h3> <p>Before diving into the core method, it’s important to get some basics right. In RL, an agent (for simplicity, you can imagine this like a robot that learns something) interacts with an environment (like a maze). At each time \(t\), the agent is in a state \(s_t\), takes an action \(a_t\), and receives a reward \(r_t\).</p> <p>The agent follows a “policy” \(\pi(s, a)\), which tells it what action \(a\) to take when in state \(s\). This policy is controlled by some parameters \(\theta\), which we adjust to make the policy better. Here are the more formal definitions of important terms in RL:</p> <ul> <li> <p><strong>Environment</strong>: The space or setting in which the agent operates.</p> </li> <li> <p><strong>State</strong>: The condition of the environment at a given time point, often denoted as \(s\) or \(s_t\) to indicate its time-dependence.</p> </li> <li> <p><strong>Agent</strong>: An entity that observes the state of the environment and takes actions to achieve a specific objective.</p> </li> <li> <p><strong>Action</strong>: A specific operation that an agent can execute, typically denoted by \(a\) or \(a_t\).</p> </li> <li> <p><strong>Policy</strong>: A policy, denoted by \(\pi(a \mid s)\) or \(\pi(s, a)\), is a mapping from states to actions, or to probabilities of selecting each action.</p> </li> <li> <p><strong>Reward</strong>: A scalar value, often denoted by \(r\) or \(r_t\), that the environment returns in response to the agent’s action.</p> </li> </ul> <h3 id="what-are-we-trying-to-optimize">What Are We Trying to Optimize?</h3> <p>The ultimate goal in the RL method is to maximize the long-term reward. We often denote this as \(\rho(\pi)\). This is the average reward the agent expects to get over time while following policy \(\pi\).</p> <h2 id="the-problem-with-traditional-methods">The Problem with Traditional Methods</h2> <p>In RL, we often want a computer to learn how to make decisions by itself. For instance, think of a game where a robot must find its way out of a maze. The robot learns by trying different paths and seeing which ones get it out of the maze faster. Sounds simple, right? But when the maze is large and complicated, the number of decisions the robot must make becomes huge. This is where function approximators like neural networks come in handy; they help the robot generalize from its experience to make better decisions.</p> <p>For a long time, people used something called a “value-function approach” to do this. In this approach, all the effort is put into calculating a value for each decision or “action” the robot can make. The robot then chooses the action with the highest value. However, this approach has some downsides:</p> <ul> <li> <p><strong>Deterministic Policies</strong>: The traditional method is good for making a fixed decision, but sometimes we want the robot to be a bit random. Why? Because the best decision can depend on chance or unknown factors.</p> </li> <li> <p><strong>Sensitive Choices</strong>: A tiny change in the calculated value can dramatically change the action taken by the robot. This is risky because we want the robot to learn stable behavior.</p> </li> <li> <p><strong>Convergence Issues</strong>: Looks like a fancy term, but it simply means that using the value-function approach does not always guarantee that the robot will find the best way to act in all situations.</p> </li> </ul> <h2 id="the-policy-gradient-theorem">The Policy Gradient Theorem</h2> <p>Before we delve into the details of REINFORCE algorithm, let’s clarify why policy gradients can be a game-changer in the world of RL. The reason for this is that REINFORCE itself belongs to this approach. Unlike traditional value-based methods which assess the “goodness” of states or state-action pairs, policy gradients aim to directly tweak the policy—a mapping from states to actions. This approach can avoid at least three potential problems:</p> <ul> <li> <p><strong>Curse of Dimensionality</strong>: Value-based methods often suffer from the “curse of dimensionality.” The state-action space can grow exponentially with the number of features describing the state and the range of actions available. This makes the computational cost expensive.</p> </li> <li> <p><strong>Non-Markovian Environments</strong>: In some cases, the environment is not following the Markov Property, where the future state depends only on the current state and action. In that case, using a value function to capture the “goodness” of a state can be misleading or incomplete.</p> </li> <li> <p><strong>Exploration vs. Exploitation</strong>: Value-based methods often cause the agent to stick to known high-value states and actions, missing out on potentially better options. While exploration strategies exist, they add another layer of complexity to the algorithm.</p> </li> </ul> <p>In simpler terms, by focusing directly on optimizing the policy, policy gradient methods can sidestep many of these issues. They are particularly well-suited for high-dimensional or continuous action spaces, can naturally accommodate stochastic policies, and are less sensitive to the challenges associated with value function approximation.</p> <h3 id="the-formal-objective">The Formal Objective</h3> <p>The objective is to maximize the expected return \(\rho(\pi)\), defined as the average sum of rewards an agent can expect to receive while following a specific policy \(\pi\).</p> \[\max_{\theta} \mathbb{E}_{\pi_{\theta}}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t}\right]\] <p>In this equation, \(\gamma\) is the discount factor, \(\theta\) are the parameters governing the policy \(\pi\), and \(T\) is the time horizon.</p> <h3 id="the-policy-gradient-theorem-in-detail">The Policy Gradient Theorem in Detail</h3> <p>To find the maximum of this objective function, we need its gradient concerning \(\theta\). The Policy Gradient Theorem provides this invaluable piece of information. Formally, it is expressed as:</p> \[\frac{\partial \rho(\pi)}{\partial \theta} = \sum_{s} d^{\pi}(s) \sum_{a} \frac{\partial \pi(s, a)}{\partial \theta} Q^{\pi}(s, a)\] <p>Here, \(d^{\pi}(s)\) represents the stationary distribution of states when following policy \(\pi\), and \(Q^{\pi}(s, a)\) is the expected return of taking action \(a\) in state \(s\) while following \(\pi\).</p> <p>This equation essentially tells us how a minute change in \(\theta\) will influence the expected return \(\rho(\pi)\).</p> <h3 id="the-log-derivative-trick">The Log-Derivative Trick</h3> <p>For effective computation of the gradient, the log-derivative trick is often employed. This trick allows us to rephrase the gradient as an expectation:</p> \[\frac{\partial \rho(\pi)}{\partial \theta} = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) Q^{\pi}(s_t, a_t) \right]\] <p>This is essentially a restatement of the gradient of a function with respect to its logarithm, which can be formally described as:</p> \[\nabla_{\theta} \pi(a \mid s) = \pi(a \mid s) \nabla_{\theta} \log \pi(a \mid s)\] <p>To prove this, we’ll take the derivative of \(\log \pi(a \mid s)\) with respect to \(\theta\):</p> \[\nabla_{\theta} \log \pi(a \mid s) = \frac{\nabla_{\theta} \pi(a \mid s)}{\pi(a \mid s)}\] <p>Rearranging the terms gives:</p> \[\nabla_{\theta} \pi(a \mid s) = \pi(a \mid s) \nabla_{\theta} \log \pi(a \mid s)\] <p>Now, let’s see how this trick fits into the policy gradient equation. The original policy gradient theorem can be expressed as:</p> \[\frac{\partial \rho(\pi)}{\partial \theta} = \sum_{s} d^{\pi}(s) \sum_{a} \nabla_{\theta} \pi(s, a) Q^{\pi}(s, a)\] <p>Here, \(d^{\pi}(s)\) represents the stationary distribution of states when following the policy \(\pi\).</p> <p>When you apply the Log-Derivative Trick to \(\nabla_{\theta} \pi(s, a)\), it becomes \(\pi(s, a) \nabla_{\theta} \log \pi(s, a)\). Substituting this into the policy gradient theorem, and then rewriting the sum as an expectation, we obtain:</p> \[\frac{\partial \rho(\pi)}{\partial \theta} = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) Q^{\pi}(s_t, a_t) \right]\] <p>In this expression, \(\tau\) symbolizes a trajectory, and \(\nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\) is the gradient of the log-probability of the action taken at time \(t\).</p> <p>This brings us to the policy gradient equation that I mentioned earlier. But why is this necessary? Computing gradients directly can be computationally expensive or even infeasible, especially when you are dealing with complex policies parameterized by neural networks.</p> <p>Let’s say you have a term like \(\pi(a \mid s)\) that depends on some parameters \(\theta\). Taking the derivative of this term directly with respect to \(\theta\) might be challenging. However, the Log-Derivative Trick provides a workaround. It transforms this term into:</p> \[\nabla_{\theta} \pi(a \mid s) = \pi(a \mid s) \nabla_{\theta} \log \pi(a \mid s)\] <p>Notice that \(\nabla_{\theta} \log \pi(a \mid s)\) is usually easier to compute. Also, this trick allows us to rephrase the Policy Gradient Theorem in a more computationally friendly manner.</p> <h3 id="why-should-we-care-about-policy-gradients">Why Should We Care About Policy Gradients?</h3> <ol> <li> <p><strong>Direct Optimization</strong>: Unlike value-based methods, policy gradients directly tweak what actually matters—the policy itself.</p> </li> <li> <p><strong>Stochasticity Handling</strong>: Policy gradients can optimize stochastic policies, crucial for situations where the optimal action can differ due to inherent randomness.</p> </li> <li> <p><strong>Sample Efficiency</strong>: Because the focus is on policy improvement, fewer samples are often required to learn a good policy, making the method generally more efficient.</p> </li> </ol> <p>By understanding the Policy Gradient Theorem and its underlying principles, you’ll find that it’s a fundamental building block for more advanced algorithms in the RL domain. Not only does it provide a method to directly optimize the policy, but it also offers the flexibility, stability, and efficiency required for real-world applications.</p> <h2 id="introducing-phenomenal-reinforce-algorithm">Introducing Phenomenal REINFORCE Algorithm</h2> <p>After understanding the power and flexibility of Policy Gradient methods, it’s time to delve into one of its most famous implementations: the REINFORCE algorithm which stands for REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility, this algorithm is not just a fancy acronym; it’s often considered as one of the fundamental building block in the world of Reinforcement Learning.</p> <table> <tbody> <tr> <td>Remember that the Policy Gradient methods aim to optimize the policy in a way that increases the expected return from any state \(s\). However, calculating the true gradient of this expected return is often computationally infeasible or requires a model of the environment, which we usually don’t have. REINFORCE is one of the Policy Gradient algorithms that makes us possible to directly optimizing the policy function $$ \pi(a</td> <td>s) $$ to maximize the cumulative reward. While there are many algorithms under the Policy Gradient category, REINFORCE stands out for its simplicity and directness in estimating the gradient.</td> </tr> </tbody> </table> <p>The core idea of REINFORCE that differentiate it with other methods is in its utilization of Monte Carlo methods to estimate the gradients needed for policy optimization. By taking sample paths through the state and action space, REINFORCE avoids the need for a model of the environment and sidesteps the computational bottleneck of calculating the true gradients. This is particularly useful when the state and/or action spaces are large or continuous, making other methods infeasible.</p> <h3 id="relation-between-reinforce-and-policy-gradient-theorem">Relation between REINFORCE and Policy Gradient Theorem</h3> <p>Recall that the Policy Gradient Theorem provides an expression for the gradient of the expected return with respect to the policy parameters. REINFORCE directly employs this theorem but takes it a step further by providing a practical way to estimate this gradient through sampling. The mathematical equation for obtaining expected return \(J(\theta)\) using this theorem can be written as:</p> \[\nabla J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(A_t|S_t) Q^{\pi}(S_t, A_t) \right]\] <p>REINFORCE simplifies this expression by utilizing the Monte Carlo estimate for \(Q^{\pi}(S_t, A_t)\), which is the sampled return \(G_t\):</p> \[\nabla J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(A_t|S_t) G_t \right]\] <p>In essence, REINFORCE is a concrete implementation of the Policy Gradient method that uses Monte Carlo sampling to estimate the otherwise intractable or unknown quantities in the Policy Gradient Theorem. By doing so, it provides a computationally efficient, model-free method to optimize policies in complex environments.</p> <h3 id="mathematical-details-of-reinforce-algorithm">Mathematical Details of REINFORCE Algorithm</h3> <p>The REINFORCE algorithm can be understood through a sequence of mathematical steps, which are as follows:</p> <ol> <li> <p><strong>Initialize Policy Parameters</strong>: Randomly initialize the policy parameters \(\theta\).</p> </li> <li> <p><strong>Generate Episode</strong>: Using the current policy \(\pi_\theta\), generate an episode \(S_1, A_1, R_2, \ldots, S_T\).</p> </li> <li> <strong>Compute Gradients</strong>: For each step \(t\) in the episode, <ul> <li>Compute the return \(G_t\).</li> <li> <table> <tbody> <tr> <td>Compute the policy gradient $$ \Delta \theta_t = \alpha \gamma^t G_t \nabla_\theta \log \pi_\theta(A_t</td> <td>S_t) $$.</td> </tr> </tbody> </table> </li> </ul> </li> <li> <strong>Update Policy</strong>: Update the policy parameters \(\theta\) using \(\Delta \theta\).</li> </ol> <p>The key equation that governs this update is:</p> \[\nabla J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(A_t|S_t) G_t \right]\] <p>Here, \(G_t\) is the return obtained using a Monte Carlo estimate, providing a sample-based approximation of \(Q^\pi(S_t, A_t)\).</p> </div> </article> <h2>References</h2> <div class="publications"> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="REINFORCE" class="col-sm-8"> <div class="title">Policy Gradient Methods for Reinforcement Learning with Function Approximation</div> <div class="author"> Satinder Singh Richard S. Sutton, and Yishay Mansour</div> <div class="periodical"> <em>Conference on Neural Information Processing Systems (NeurIPS) 1999</em>, 1999 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li></ol> </div> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Ravialdy Hidayat. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>