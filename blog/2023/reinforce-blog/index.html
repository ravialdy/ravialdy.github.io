<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Understanding Phenomenal REINFORCE Policy Gradient Method | Ravialdy's Blog</title> <meta name="author" content="Ravialdy Hidayat"> <meta name="description" content="Experienced AI Researcher and Data Scientist with over 3 years of experience in computer vision, machine learning, and AI model deployment. Passionate about solving complex problems and advancing the state of technology. "> <meta name="keywords" content="AI Research, Data Science, Computer Vision, Model Deployment, Machine Learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="ravialdy/ravialdy.github.io/blog/2023/reinforce-blog/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Ravialdy's Blog</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV_Ravialdy%20(October%202023)%20(3).pdf" target="_blank">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding Phenomenal REINFORCE Policy Gradient Method</h1> <p class="post-meta">October 15, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/reinforcement"> <i class="fas fa-hashtag fa-sm"></i> Reinforcement</a>   <a href="/blog/tag/learning"> <i class="fas fa-hashtag fa-sm"></i> Learning</a>   <a href="/blog/tag/rl"> <i class="fas fa-hashtag fa-sm"></i> (RL),</a>   <a href="/blog/tag/reinforce"> <i class="fas fa-hashtag fa-sm"></i> REINFORCE,</a>   <a href="/blog/tag/policy"> <i class="fas fa-hashtag fa-sm"></i> Policy</a>   <a href="/blog/tag/gradient"> <i class="fas fa-hashtag fa-sm"></i> Gradient</a>     ·   <a href="/blog/category/blogpost"> <i class="fas fa-tag fa-sm"></i> blogpost</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#introduction">Introduction</a> <ul> <li class="toc-entry toc-h3"><a href="#brief-recap-about-reinforcement-learning">Brief Recap about Reinforcement Learning</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#the-policy-gradient-theorem">The Policy Gradient Theorem</a> <ul> <li class="toc-entry toc-h3"><a href="#the-formal-objective">The Formal Objective</a></li> <li class="toc-entry toc-h3"><a href="#the-policy-gradient-theorem-in-detail">The Policy Gradient Theorem in Detail</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#introducing-reinforce-algorithm">Introducing REINFORCE Algorithm</a> <ul> <li class="toc-entry toc-h3"><a href="#main-idea-of-reinforce">Main Idea of REINFORCE</a></li> <li class="toc-entry toc-h3"><a href="#reinforce--policy-gradient-theorem">REINFORCE &amp; Policy Gradient Theorem</a></li> <li class="toc-entry toc-h3"><a href="#mathematical-details-of-reinforce">Mathematical Details of REINFORCE</a></li> <li class="toc-entry toc-h3"><a href="#conclusion-and-limitations">Conclusion and Limitations</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <style>h2{margin-top:1.25em;margin-bottom:.5em}h3{margin-top:1em;margin-bottom:.5em}</style> <h2 id="introduction">Introduction</h2> <p>Welcome to my blog post! Today we are going to discuss about a very fascinating and important topic in the world of Reinforcement Learning (RL) — the Policy Gradient REINFORCE Method. This method is quite famous for solving some complex problems in RL.</p> <p>Don’t worry if you’re new to this field, I will try to keep things simple and easy to understand. First of all, I will be focusing on the background of the Policy Gradient Theorem and why it was proposed in the first place.</p> <h3 id="brief-recap-about-reinforcement-learning">Brief Recap about Reinforcement Learning</h3> <p>Before diving into the core method, it’s important to get some basics right. Imagine we have a small robot placed at the entrance of a maze. The maze is simple: it has walls, open passages, and a cheese located at the exit. The robot’s ultimate goal is to find the most efficient path to reach the cheese.</p> <p>In RL, an agent (a robot in this case) interacts with an environment (like a maze). At each time \(t\), the agent is in a state \(s_t\), takes an action \(a_t\), and receives a reward \(r_t\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/reinforce/DALL%C2%B7E%202023-10-24%2015.38.59%20-%20Vector%20design%20of%20a%20playful%20scene%20where%20a%20cartoon%20robot%20is%20gearing%20up%20to%20enter%20a%20maze.%20The%20maze's%20pathways%20are%20clear,%20with%20walls%20separating%20the%20routes-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/reinforce/DALL%C2%B7E%202023-10-24%2015.38.59%20-%20Vector%20design%20of%20a%20playful%20scene%20where%20a%20cartoon%20robot%20is%20gearing%20up%20to%20enter%20a%20maze.%20The%20maze's%20pathways%20are%20clear,%20with%20walls%20separating%20the%20routes-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/reinforce/DALL%C2%B7E%202023-10-24%2015.38.59%20-%20Vector%20design%20of%20a%20playful%20scene%20where%20a%20cartoon%20robot%20is%20gearing%20up%20to%20enter%20a%20maze.%20The%20maze's%20pathways%20are%20clear,%20with%20walls%20separating%20the%20routes-1400.webp"></source> <img src="/assets/img/reinforce/DALL%C2%B7E%202023-10-24%2015.38.59%20-%20Vector%20design%20of%20a%20playful%20scene%20where%20a%20cartoon%20robot%20is%20gearing%20up%20to%20enter%20a%20maze.%20The%20maze's%20pathways%20are%20clear,%20with%20walls%20separating%20the%20routes.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Illustration of an agent (robot) tries to reach the cheese as soon as possible (Image source : DALLE-3). </div> <p>The agent follows a “policy” \(\pi(a \mid s)\), which tells it what action \(a\) to take when in state \(s\). The policy itself is controlled by some parameters \(\theta\), which we adjust to make it better. Here are the more formal definitions of important terms in RL:</p> <ul> <li> <p><strong>Environment</strong>: The space or setting in which the agent operates.</p> </li> <li> <p><strong>State</strong>: The condition of the environment at a given time point, often denoted as \(s\) or \(s_t\) to indicate its time-dependence.</p> </li> <li> <p><strong>Agent</strong>: An entity that observes the state of the environment and takes actions to achieve a specific objective.</p> </li> <li> <p><strong>Action</strong>: A specific operation that an agent can execute, typically denoted by \(a\) or \(a_t\).</p> </li> <li> <p><strong>Policy</strong>: A policy, denoted by \(\pi(a \mid s)\) or \(\pi(s, a)\), is a mapping from states to actions, or to probabilities of selecting each action.</p> </li> <li> <p><strong>Reward</strong>: A scalar value, often denoted by \(r\) or \(r_t\), that the environment returns in response to the agent’s action.</p> </li> </ul> <h2 id="the-policy-gradient-theorem">The Policy Gradient Theorem</h2> <p>Before we delve into the details of REINFORCE algorithm, we need to understand what policy gradient really is and why it can be a game-changer in the world of RL. The reason for this is that REINFORCE itself belongs to this approach.</p> <p>Unlike traditional value-based methods which assess the “goodness” of states or state-action pairs, policy gradients aim to directly optimize the policy. Let’s take a look at what kind of potential problems value-based approach has and how policy gradient methods can avoid those issues:</p> <ul> <li> <p><strong>Curse of Dimensionality</strong>: Value-based methods require an estimated value for every possible state or state-action pair. As the number of states and actions increases, the size of the value function grows exponentially. By focusing directly on optimizing the policy, policy gradient methods can avoid this issue since it works with a much smaller set of parameters.</p> </li> <li> <p><strong>Non-Markovian Environments</strong>: In some cases, the environment is not following the Markov Property, where the future state depends only on the current state and action. Policy gradient methods do not rely on the Markov property because they do not predict future values; they only need to evaluate the outcomes of current actions.</p> </li> <li> <p><strong>Exploration vs. Exploitation</strong>: Value-based methods often cause the agent to stick to known high-value states and actions, missing out on potentially better options. By adjusting the policy parameters directly, policy gradient methods can encourage the agent to explore different actions with probabilities, rather than committing to the action with the highest estimated value.</p> </li> </ul> <p>In other words, the key difference is that the size of the parameter set in policy gradient methods is determined by the complexity of the policy representation (e.g., the architecture of the neural network), not by the size of the state or action space.</p> <p>For example, suppose you have a neural network with 1000 parameters. It can still process thousands or even millions of different states and output actions for each of them because the same parameters are used to evaluate every state through the network’s forward pass.</p> <p>This means that even for complex environments, the number of parameters doesn’t necessarily increase with the complexity of the state space, which is often the case with value-based methods.</p> <p>Thus, policy gradient methods are particularly well-suited for high-dimensional or continuous action spaces, can accommodate stochastic policies, and are less sensitive to the challenges associated with value function approximation.</p> <h3 id="the-formal-objective">The Formal Objective</h3> <p>The objective is to maximize the expected return \(J(\theta)\), defined as the average sum of rewards an agent can expect to receive while following a specific policy \(\pi\).</p> \[\max_{\theta} \mathbb{E}_{\tau \sim \pi_{\theta}}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t}\right]\] <p>In this equation, \(\gamma\) is the discount factor, \(\theta\) are the parameters governing the policy \(\pi\), and \(T\) is the time horizon.</p> <h3 id="the-policy-gradient-theorem-in-detail">The Policy Gradient Theorem in Detail</h3> <p>To find the maximum of this objective function, we need its gradient w.r.t \(\theta\). But first we need to understand that the probability of observing a trajectory \(\tau = (s_0, a_0, ..., s_{T+1})\) under policy \(\pi_{\theta}\) can be expressed as :</p> \[P(\tau \mid \theta) = \rho_0 (s_0) \prod_{t=0}^{T} P(s_{t+1}\mid s_t, a_t) \pi_{\theta}(a_t \mid s_t)\] <p>where \(\rho_0(s_0)\) represents the probability of starting in the initial state, \(\pi_{\theta}(a_t \mid s_t)\) is the probability of taking action \(a_t\) in state \(s_t\), as dictated by the policy, and \(P(s_{t+1} \mid s_t, a_t)\) represents the probability of transitioning to state \(s_{t+1}\) after taking action \(a_t\) in state \(s_t\).</p> <p>The reason for getting the expression \(P(\tau \mid \theta)\) is due to Markov Decision Process (MDP). Since each state-action pair and subsequent state transition is considered an independent event, the probability of the entire trajectory \(\tau\) occurring is the product of the probabilities of each of these events.</p> <p>The process includes the probability of the initial state, the probability of each action taken according to the policy, and the probability of each state transition as dictated by the environment’s dynamics.</p> <p>If we apply the log function to both sides of equation, we will get :</p> \[\log P(\tau \mid \theta) = \log \rho_0 (s_0) + \sum_{t=0}^{T} \left( \log P(s_{t+1}\mid s_t, a_t) + \log \pi_{\theta}(a_t \mid s_t)\right)\] <p>Since log operation can change multiplications into summations. Next, we will use the Log-Derivative trick like below,</p> \[\nabla_{\theta} P(\tau \mid \theta) = P(\tau \mid \theta) \nabla_{\theta} \log P(\tau \mid \theta)\] <p>Notice that if we take the derivative w.r.t \(\theta\), then the gradient of environment-specific functions like \(\rho_0(s_0)\), \(P(s_{t+1} \mid s_t, a_t)\), which do not depend on \(\theta\), are zero.</p> <p>Thus, applying that and implementing the Log-Derivative trick into the equation above will generate the equation below :</p> \[\nabla_{\theta} \log P(\tau \mid \theta) = \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\] <p>So now we are ready to calculate the gradient of the expected return w.r.t model parameters \(\theta\) which is the essence of the policy gradient theorem that we have discussed so far.</p> <p>Recall that by definition, \(J(\theta)\) is the expectation of the return \(R(\tau)\) over all possible trajectories \(\tau\) under the policy \(\pi_{\theta}\). Mathematically, this expectation can be represented as an integral over all possible trajectories, weighted by the probability of each trajectory under our policy,</p> \[\mathbb{E}_{\tau \sim \pi_{\theta}}[R(\tau)] = \int_{\tau} P(\tau \mid \theta) R(\tau)\] <p>After that, we want to bring the gradient inside the integral. This is possible due to the linearity property of gradients, allowing us to interchange the order of differentiation and integration. Thus, we can rewrite the above expression as,</p> \[\int_{\tau} \nabla_{\theta} P(\tau \mid \theta) R(\tau)\] <p>Next, we apply the log-derivative trick in order to change the focus from the gradient of the probability to the gradient of its logarithm,</p> \[\int_{\tau} P(\tau \mid \theta) \nabla_{\theta} \log P(\tau \mid \theta) R(\tau)\] <p>Notice that the equation above essentially can be seen as the expected value of the gradient of the log-probability of the trajectory times the return, under our policy. Thus, we can say :</p> \[\mathbb{E}_{\tau \sim \pi_{\theta}}[\nabla_{\theta} \log P(\tau \mid \theta) R(\tau)]\] <p>Then, we can apply the expression that we have derived before \(\nabla_{\theta} \log P(\tau \mid \theta) = \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\) into the above equation,</p> \[\therefore \nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}\left[\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) R(\tau)\right]\] <p>As you can see above, that expression is very special since it provides a gradient of the expected return with respect to the policy parameters. This allows the policy gradient theorem to directly optimize the policy parameters \(\theta\) ignoring the parts that are independent of the policy.</p> <h2 id="introducing-reinforce-algorithm">Introducing REINFORCE Algorithm</h2> <p>After understanding the power and flexibility of Policy Gradient methods, it’s time to delve into one of its most famous implementations: the REINFORCE algorithm which stands for REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility.</p> <p>This algorithm is often considered as one of the most important and fundamental building block in the world of Reinforcement Learning.</p> <h3 id="main-idea-of-reinforce">Main Idea of REINFORCE</h3> <p>The core idea of REINFORCE that differentiate it with other methods is in its utilization of Monte Carlo methods to estimate the gradients needed for policy optimization.</p> <p>By taking sample paths through the state and action space, REINFORCE avoids the need for a model of the environment and sidesteps the computational bottleneck of calculating the true gradients. This is particularly useful when the state and/or action spaces are large or continuous, making other methods infeasible.</p> <p>For those who are not familiar with Monte Carlo approach, it is basically the process of sampling and averaging for estimating expected values in situations with large or infinite state spaces. By doing this, we can get the estimates that are unbiased without incorporating all available data.</p> <p>To understand more about the role of Monte Carlo in the REINFORCE, see the explanation below.</p> <h3 id="reinforce--policy-gradient-theorem">REINFORCE &amp; Policy Gradient Theorem</h3> <p>REINFORCE directly employs Policy Gradient theorem but takes it a step further by providing a practical way to estimate this gradient through sampling. Recall that mathematical equation for obtaining expected return \(J(\theta)\) using this theorem can be written as:</p> \[\nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) Q^{\pi}(s_t, a_t) \right]\] <p>REINFORCE simplifies this expression by utilizing the Monte Carlo estimate for \(Q^{\pi}(s_t, a_t)\), which is the sampled return \(G_t\):</p> \[\nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t) G_t \right]\] <p>Here, \(G_t\) is the return obtained using a Monte Carlo estimate. Note that while \(R(\tau)\) is used in the theoretical derivation to denote the total return of a trajectory, \(G_t\) in REINFORCE serves as a practical, sampled estimate of the return from time \(t\) onwards.</p> <p>In essence, REINFORCE is a concrete implementation of the Policy Gradient method that uses Monte Carlo sampling to estimate the otherwise intractable or unknown quantities in the Policy Gradient Theorem. By doing so, it provides a computationally efficient, model-free method to optimize policies in complex environments.</p> <h3 id="mathematical-details-of-reinforce">Mathematical Details of REINFORCE</h3> <p>The REINFORCE algorithm can be understood through a sequence of mathematical steps, which are as follows:</p> <ol> <li> <p><strong>Initialize Policy Parameters</strong>: Randomly initialize the policy parameters \(\theta\).</p> </li> <li> <p><strong>Generate Episode</strong>: Using the current policy \(\pi_\theta\), generate an episode \(S_1, A_1, R_2, \ldots, S_T\).</p> </li> <li> <strong>Compute Gradients</strong>: For each step \(t\) in the episode, <ul> <li>Compute the return \(G_t\).</li> <li>Compute the policy gradient \(\Delta \theta_t = \alpha \gamma^t G_t \nabla_\theta \log \pi_{\theta}(a_t \mid s_t)\).</li> </ul> </li> <li> <strong>Update Policy</strong>: Update the policy parameters \(\theta\) using \(\Delta \theta\).</li> </ol> <p>The key equation that governs this update is:</p> \[\nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_{\theta}(a_t \mid s_t) G_t \right]\] <h3 id="conclusion-and-limitations">Conclusion and Limitations</h3> <p>While REINFORCE is oftenly used for its simplicity and directness, it’s also essential to recognize its limitations. The method tends to have high variance in its gradient estimates, which could lead to unstable training. However, various techniques, like using a baseline or employing advanced variance reduction methods, can alleviate these issues to some extent.</p> <p>REINFORCE is often the easy choice when you need a simple yet effective method for policy optimization, especially in high-dimensional or continuous action spaces.</p> </div> </article> <h2>References</h2> <div class="publications"> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="REINFORCE" class="col-sm-8"> <div class="title">Policy Gradient Methods for Reinforcement Learning with Function Approximation</div> <div class="author"> Satinder Singh Richard S. Sutton, and Yishay Mansour</div> <div class="periodical"> <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 1999 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li></ol> </div> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Ravialdy Hidayat. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>