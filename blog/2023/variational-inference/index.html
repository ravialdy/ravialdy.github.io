<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>The Magic of Variational Inference | Ravialdy's Blog</title> <meta name="author" content="Ravialdy Hidayat"> <meta name="description" content="Experienced AI Researcher and Data Scientist with over 3 years of experience in computer vision, machine learning, and AI model deployment. Passionate about solving complex problems and advancing the state of technology. "> <meta name="keywords" content="AI Research, Data Science, Computer Vision, Model Deployment, Machine Learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="ravialdy/ravialdy.github.io/blog/2023/variational-inference/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Ravialdy's Blog</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV_Ravialdy%20(October%202023)%20(3).pdf" target="_blank">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Magic of Variational Inference</h1> <p class="post-meta">November 20, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/variational"> <i class="fas fa-hashtag fa-sm"></i> Variational,</a>   <a href="/blog/tag/inference"> <i class="fas fa-hashtag fa-sm"></i> Inference,</a>   <a href="/blog/tag/bayesian"> <i class="fas fa-hashtag fa-sm"></i> Bayesian</a>     ·   <a href="/blog/category/blogpost"> <i class="fas fa-tag fa-sm"></i> blogpost</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h2"><a href="#latent-variable-models">Latent Variable Models</a></li> <li class="toc-entry toc-h2"><a href="#posterior-distribution">Posterior Distribution</a></li> <li class="toc-entry toc-h2"><a href="#approximate-posterior-distribution">Approximate Posterior Distribution</a></li> <li class="toc-entry toc-h2"><a href="#evidence-lower-bound-objective-elbo">Evidence Lower Bound Objective (ELBO)</a></li> </ul> </div> <hr> <div id="markdown-content"> <style>h2{margin-top:1.25em;margin-bottom:.5em}h3{margin-top:1em;margin-bottom:.5em}</style> <h2 id="introduction">Introduction</h2> <p>Hi guys, welcome to my blogpost again :) Today, I want to discuss about the magical and how wonderful Variational Inference (VI) is. This approach is widely used in many applications, such as text-to-image generation, motion planning, Reinforcement Learning (RL), etc.</p> <p>The reason for this is that in many cases the distribution of generated output that we want is very complex, e.g., images, text, video, etc. This is where VI can help us through latent variables. I believe that once we can master this concept well, then we can understand many recent AI techniques more easily and intuitively.</p> <p>I often found the explanation on the internet about this topic is not clear enough in explaining the reasons why this concept needs to exist somehow, why we need to calculate many fancy math terms, etc. Therefore, in this post I also want to focus more on the reasoning part so that all of us can understand the essence of this method beyond the derivations and usefulness of VI.</p> <p>This post is based on my understanding of the topic, so if you find any mistake please let me know :)</p> <div class="row mt-4 justify-content-center"> <div class="col-12 col-md-8 mx-auto mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/VI/DALL%C2%B7E%202023-11-23%2017.07.41-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/VI/DALL%C2%B7E%202023-11-23%2017.07.41-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/VI/DALL%C2%B7E%202023-11-23%2017.07.41-1400.webp"></source> <img src="/assets/img/VI/DALL%C2%B7E%202023-11-23%2017.07.41.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 1. Illustration of the output from text-to-image model (Image source : DALLE-3). </div> <h2 id="latent-variable-models">Latent Variable Models</h2> <p>You may ask what Variational Inference (VI) really is? How it can be oftenly used in many recent AI methods? To answer those questions, let me start with latent variable models.</p> <p>Let’s say we want to build a regression model that can fit a simple data distribution like this,</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/VI/regression-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/VI/regression-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/VI/regression-1400.webp"></source> <img src="/assets/img/VI/regression.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Regression model that tries to fit simple data distribution (Image source : <a href="https://www.analyticsvidhya.com/blog/2022/01/different-types-of-regression-models/" rel="external nofollow noopener" target="_blank">Analytics Vidhya</a>). </div> <p>What we basically try to do from the image above is to model \(p(\mathbf{y} \mid \mathbf{x})\) where \(y\) is our data given \(x\). It seems very simple right? But now let’s imagine we have quite complex data dsitribution like below,</p> <div class="row mt-4 justify-content-center"> <div class="col-12 col-md-8 mx-auto mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/VI/mixture_gaussian-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/VI/mixture_gaussian-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/VI/mixture_gaussian-1400.webp"></source> <img src="/assets/img/VI/mixture_gaussian.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 3. The scenario where data distribution is complex enough (Image source : <a href="https://www.youtube.com/watch?v=iL1c1KmYPM0&amp;t=2900s" rel="external nofollow noopener" target="_blank">Stanford Online</a>). </div> <p>You might be confused initially on how we can build a model that fits that distribution. But don’t worry, I was also used to be like that too :) In reality, the distribution that we face might be much more complex than that.</p> <p>Fortunately, we can approximate that distribution through multiplication of two simple distributions. How we can do that? This is where the concept of latent variable models comes into play.</p> <p>The data distribution itself \(p(\mathbf{x})\) can be expressed mathematically as,</p> \[p(\mathbf{x})=\sum_z p(\mathbf{x} \mid \mathbf{z}) p(\mathbf{z})\] <p>where \(\mathbf{z}\) is the latent variable. Maybe you ask, what is that thing? Basically it is the hidden value that is not the variable \(y\) nor \(x\), but needs to be considered where we want to calculate the probability of the observed data \(\mathbf{x}\) in a more complex distribution. These latent variables represent underlying factors or characteristics that might not be directly observable but significantly influence the observed data.</p> <p>For example, the latent variables for figure 3 is the categorical value that maps each data point into cluster blue, green, or yellow.</p> <p>You may still wonder, how latent variable models is used in this case? First, we need to know that the prior or latent variable distribution \(p(\mathbf{z})\) is assumed to be a simple distribution, typically chosen as a standard gaussian distribution \(\mathcal{N}\left(0, \boldsymbol\Sigma^{2}\right)\), with variance \(\boldsymbol\Sigma^{2}\).</p> <p>So how about the distribution \(p(\mathbf{x} \mid \mathbf{z})\)? This is also assumed to be a normal distribution, but the parameters mean \(\boldsymbol\mu_{nn}\) and the variance \(\boldsymbol\Sigma_{nn}^{2}\) are generated by our neural networks. This means that even though the process of defining that distribution can be quite complex, but it is still considered to be a simple distribution since we can parameterize it.</p> <p>Thus, by doing like that we basically can approximate our data distribution \(p(\mathbf{x})\) as the multiplication of two simple distributions \(p(\mathbf{x})= \sum_z p(\mathbf{x} \mid \mathbf{z}) p(\mathbf{z})\). This is why we use latent variable models.</p> <p>But, there is a problem when we use this approach directly. Remember that earlier we assumed that the prior distribution \(p(\mathbf{z})\) is just a standard gaussian distribution which also means that it is a unimodal distribution.</p> <p>Can you imagine if we leverage that simple distribution directly without any training or learning process to approximate a very complex distribution which is oftenly has many modes? What will happen is that the approximation result will be not good since we do not incorporate any knowledge about our data into the pre-defined prior distribution. This is the where VI plays an important role :)</p> <h2 id="posterior-distribution">Posterior Distribution</h2> <p>From the previous explanation, you may be curious on how to update our prior belief represented by \(p(\mathbf{z})\). Actually, bayes theorem provides a way to do that. Specifically, the answer lies on what we call as posterior distribution \(p(\mathbf{z} \mid \mathbf{x})\). But in many cases, we cannot compute that expression or even if we can, then we need a lot of resources.</p> <p>For understanding why is that, let me briefly recap about the use of bayes theorem in this case.</p> <p>Our posterior can mathematically be expressed as,</p> \[p(\mathbf{z} \mid \mathbf{x}) = \frac{p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z})}{p(\mathbf{x})}\] <p>where \(p(\mathbf{x})\) is the marginal likelihood or evidence of our data distribution.</p> <p>There is also a joint distribution \(p(\mathbf{z}, \mathbf{x})\) that represents the probability of both the latent variables \(\mathbf{z}\) and the observed data \(\mathbf{x}\) occurring together. It can be factored into the product of the likelihood and the prior:</p> \[p(\mathbf{z}, \mathbf{x}) = p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z})\] <p>To calculate the marginal likelihood \(p(\mathbf{x})\), we can view it from bayesian perspective as the probability of observing the data \(\mathbf{x}\) marginalized over all possible values of the latent variables \(\mathbf{z}\). It is obtained by integrating (or summing, in the case of discrete variables) the joint distribution over \(\mathbf{z}\):</p> \[p(\mathbf{x}) = \int p(\mathbf{z}, \mathbf{x}) \, \mathrm{d}\mathbf{z} = \int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z}\] <p>This integral accounts for all possible configurations of the latent variables that could have generated the observed data.</p> <p>So why \(p(\mathbf{z} \mid \mathbf{x})\) is very difficult to calculate? The reason is that if we want to calculate it, then it means that we also need to calculate \(p(\mathbf{x})\) since it is located in the denominator of the posterior math equation. This also means that the complexity will arise rapidly as the dimensionality of \(\mathbf{z}\) grows.</p> <p>This is because the integral used for calculating \(p(\mathbf{x})\) sums over all possible values of \(\mathbf{z}\), and each evaluation of the joint distribution within the integral can also be computationally expensive, leading to an intractable integral.</p> <p>For example, let’s imagine we have a mixture model with \(K = 2\) clusters as our latent variable and \(n = 3\) data points which represents our observed data \(\mathbf{x}\). Thus, we will have 8 combinations as follows: (1,1,1), (1,1,2), (1,2,1), (1,2,2), (2,1,1), (2,1,2), (2,2,1), (2,2,2). Here, each tuple represents the cluster assignments for the three data points.</p> <p>For each of the 8 combinations of cluster assignments, we have to evaluate the likelihood of the entire data set given these assignments and the cluster means. The integral for every combination will be a two-dimensional integral over the two means (\(\mu_1\) and \(\mu_2\)). Then, the integral for marginal distribution can be written as,</p> \[p(\mathbf{x}) = \int \int \prod_{i=1}^{3} p(x_i \mid \mu_{c_i}) \, p(\mu_1) \, p(\mu_2) \, d\mu_1 \, d\mu_2\] <p>Here, \(p(x_i \mid \mu_{c_i})\) represents the likelihood of data point \(x_i\) given the mean of its assigned cluster \(\mu_{c_i}\), where \(c_i\) is the cluster assignment for data point \(i\).</p> <p>Notice that the number of integral follows the total dimensions that our latent variable \(\mathbf{z}\) has. In reality, we can have up to thousands of latent dimensions which means that we need to calculate thousands-dimensional integral!!</p> <p>Even for our very simple example, the cost can be computationally intensive, particularly when the likelihood and prior distributions do not have closed-form solutions!!</p> <p>Note* : closed-form solutions refers to the exact results from using standard math operations.</p> <h2 id="approximate-posterior-distribution">Approximate Posterior Distribution</h2> <p>Now you understand why calculating the exact posterior distribution is often very difficult. Many researchers try to solve this problem by approximating that distribution in various ways. In this post, I just want to focus on the estimation method related to the VI concept. Let’s go into more detail yeeyy :)</p> <p>Remember that the root cause is not the posterior itself, but its requirement to calculate marginal distribution \(p(\mathbf{x})\) to derive \(p(\mathbf{z} \mid \mathbf{x})\) which involves integrations. Thus, the key idea here is to approximate the posterior by replacing the annoying integral operations with the optimization process of expected value \(E_{z \sim q_i(\mathbf{z})}\) with respect to approximate posterior \(q_i(\mathbf{z})\).</p> <p>Specifically, the optimization is used to find the best approximation \(q(\mathbf{z} ; \boldsymbol{v*})\) where \(\boldsymbol{v}\) are the variational parameters from a chosen family of distributions that minimizes the difference (specifically, the Kullback-Leibler divergence) from the true posterior \(p(\mathbf{z} \mid \mathbf{x})\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/VI/VI_optimization-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/VI/VI_optimization-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/VI/VI_optimization-1400.webp"></source> <img src="/assets/img/VI/VI_optimization.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Optimization process happening in Variational Inference (VI) (Image source : <a href="https://www.cs.columbia.edu/~blei/talks/2016_NIPS_VI_tutorial.pdf" rel="external nofollow noopener" target="_blank">NIPS 2016 Tutorial</a>). </div> <p>So how we can do that approximation? First recall that the marginal distribution can be expressed as,</p> \[p(\mathbf{x}) = \int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z}\] <p>That above equation can also be written as,</p> \[p(\mathbf{x}) = \int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \mathrm{d}\mathbf{z} = \int p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z}) \, \frac{q_i(\mathbf{z})}{q_i(\mathbf{z})} \, \mathrm{d}\mathbf{z}\] <p>The introduction of \(\frac{q_i(\mathbf{z})}{q_i(\mathbf{z})}\) is a mathematical trick that allows us to rewrite the marginal likelihood in terms of the variational distribution \(q_i(\mathbf{z})\). By doing this, we can utilize the expected value with respect to \(q_i(\mathbf{z})\) to approximate the integral.</p> <p>The modified expression for the marginal likelihood becomes,</p> \[p(\mathbf{x}) = \int q_i(\mathbf{z}) \, \frac{p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z})}{q_i(\mathbf{z})} \, \mathrm{d}\mathbf{z}\] <p>The above expression can be interpreted as the expected value of the ratio \(\frac{p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z})}{q_i(\mathbf{z})}\) under the variational distribution \(q_i(\mathbf{z})\). Therefore, we can write like this,</p> \[p(\mathbf{x}) = E_{\mathbf{z} \sim q_i(\mathbf{z})}\left[\frac{p(\mathbf{x} \mid \mathbf{z}) \, p(\mathbf{z})}{q_i(\mathbf{z})}\right]\] <p>By using this formulation, we can avoid the direct computation of the integral for the marginal likelihood, which is typically intractable. Instead, we can focus on finding the optimal \(q_i(\mathbf{z})\) that minimizes the difference from the true posterior, as measured by the Kullback-Leibler (KL) divergence.</p> <p>This approach is the essence of variational inference, transforming a challenging integration problem into a more manageable optimization problem via expected value calculation.</p> <h2 id="evidence-lower-bound-objective-elbo">Evidence Lower Bound Objective (ELBO)</h2> <p>It turns out that if we approximate the true posterior by doing what we discussed before, then we can construct a lower bound for the marginal distribution \(p(x_i)\) of each \(x_i\) point. This can be a very powerful idea because by having that lower bound we can also maximize the loglikelihood of \(p(x_i)\).</p> <p>Note that generally increasing the lower bound does not necessarily mean also increase the loglikelihood of \(p(x_i)\), but if some conditions are satisfied, then it does (we will discuss more about this later).</p> <p>Recall that by looking at the previous equation, we can also derive the mathematical equation for each data point \(x_i\) of the distribution \(p(x_i)\) as,</p> \[p(x_i) = E_{\mathbf{z} \sim q_i(z)}\left[\frac{p(x_i \mid z) \, p(z)}{q_i(z)}\right]\] <p>If we apply log operation for both sides of the equation, we can get the expression like below,</p> \[\log p\left(x_i\right) = \log E_{z \sim q_i(z)}\left[\frac{p\left(x_i \mid z\right) p(z)}{q_i(z)}\right]\] <p>Then, we can implement jensen’s inequality \(\log E[y] \geq E[\log y]\) into our case, then we can get,</p> \[\log p\left(x_i\right) \geq E_{z \sim q_i(z)}\left[\log \frac{p\left(x_i \mid z\right) p(z)}{q_i(z)}\right]\] <p>By leveraging log property, the above equation can also be expressed as,</p> \[\log p\left(x_i\right) \geq E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right)+\log p(z)\right] - E_{z \sim q_i(z)}\left[\log q_i(z) \right]\] <p>where \(- E_{z \sim q_i(z)}\left[\log q_i(z) \right]\) is the entropy \(\mathcal{H}\left(q_i\right)\). The above inequality is also called as variational lower bound \(\mathcal{L}_i\left(p, q_i\right)\).</p> <p>So how we can make that lower bound to be tighter? The answer lies on how we can find a good approximation for \(q_i(z)\). So how we can do that? Yes you are right, the answer is by using KL divergence!</p> <p>The mathematical equation for implementing KL divergence between approximate and the true posterior can be written like this,</p> \[D_{\mathrm{KL}}\left(q_i\left(x_i\right) \| p\left(z \mid x_i\right)\right) = E_{z \sim q_i(z)}\left[\log \frac{q_i(z)}{p\left(z \mid x_i\right)}\right]\] <p>For those who are not familiar with KL divergence before, so basically the equation above measures how one probability distribution diverges from a second, expected probability distribution.</p> <p>Recall that the bayes theorem tells us,</p> \[p(z \mid x_i) = \frac{p(x_i \mid z) p(z)}{p(x_i)}\] <p>Therefore, we can use that to rewrite the term inside our KL divergence as:</p> \[\frac{q_i(z)}{p\left(z \mid x_i\right)} = \frac{q_i(z)}{\frac{p(x_i \mid z) p(z)}{p(x_i)}} = \frac{q_i(z) p(x_i)}{p(x_i, z)}\] <p>where \(p(x_i \mid z) p(z) = p(x_i, z)\) is derived from the definition of joint probability. By inserting above expression into inside our KL divergence, we can get,</p> \[D_{\mathrm{KL}}\left(q_i\left(x_i\right) \| p\left(z \mid x_i\right)\right) = E_{z \sim q_i(z)}\left[\log \frac{q_i(z) p\left(x_i\right)}{p\left(x_i, z\right)}\right]\] <p>After that, by using the log property we can also write above equation as,</p> \[\begin{aligned} D_{\mathrm{KL}}\left(q_i\left(x_i\right) \| p\left(z \mid x_i\right)\right) = &amp; -E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right) + \log p(z)\right] \\ &amp; + E_{z \sim q_i(z)}\left[\log q_i(z)\right] + E_{z \sim q_i(z)}\left[\log p\left(x_i\right)\right] \end{aligned}\] <p>Since \(- E_{z \sim q_i(z)}\left[\log q_i(z) \right]\) is the entropy \(\mathcal{H}\left(q_i\right)\), we can also write,</p> \[\begin{aligned} D_{\mathrm{KL}}\left(q_i\left(x_i\right) \| p\left(z \mid x_i\right)\right) = &amp; -E_{z \sim q_i(z)}\left[\log p\left(x_i \mid z\right) + \log p(z)\right] \\ &amp; - \mathcal{H}\left(q_i\right) + \log p\left(x_i\right) \end{aligned}\] <p>Then, we can also express above equation as,</p> \[D_{\mathrm{KL}}\left(q_i\left(x_i\right) \| p\left(z \mid x_i\right)\right) = -\mathcal{L}_i\left(p, q_i\right)+\log p\left(x_i\right)\] <p>Rearranging above equation give us,</p> \[\log p\left(x_i\right) = D_{\mathrm{KL}}\left(q_i(z) \| p\left(z \mid x_i\right)\right)+\mathcal{L}_i\left(p, q_i\right)\] <p>As you can see, from the equation above we can say that if we successfully minimize the KL divergence part into 0 (which means our approximate posterior is exactly same with the true one), then the loglikelihood of our marginal or data distribution is also exactly same with the variational lower bound that we have defined before.</p> <p>Thus, we already have a way to make that lower bound more tight by minimizing the KL divergence part.</p> <p>For the next part of this post, stay tune :) I will complete this as soon as I can :)</p> </div> </article> <h2>References</h2> <div class="publications"> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="VarInfer" class="col-sm-8"> <div class="title">Variational Inference: A Review for Statisticians</div> <div class="author"> Jon D. McAuliffe David M. Blei</div> <div class="periodical"> <em>arXiv</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/1601.00670.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li></ol> </div> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Ravialdy Hidayat. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>