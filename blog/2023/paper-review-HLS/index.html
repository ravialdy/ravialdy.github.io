<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Paper Review "Hierarchical Latent Structure for Multi-Modal Vehicle Trajectory Forecasting" | Ravialdy's Blog</title> <meta name="author" content="Ravialdy Hidayat"> <meta name="description" content="Experienced AI Researcher and Data Scientist with over 3 years of experience in computer vision, machine learning, and AI model deployment. Passionate about solving complex problems and advancing the state of technology. "> <meta name="keywords" content="AI Research, Data Science, Computer Vision, Model Deployment, Machine Learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="ravialdy/ravialdy.github.io/blog/2023/paper-review-HLS/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Ravialdy's Blog</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV_Ravialdy%20(October%202023)%20(3).pdf" target="_blank">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Paper Review "Hierarchical Latent Structure for Multi-Modal Vehicle Trajectory Forecasting"</h1> <p class="post-meta">October 31, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/autonomous"> <i class="fas fa-hashtag fa-sm"></i> Autonomous</a>   <a href="/blog/tag/driving"> <i class="fas fa-hashtag fa-sm"></i> Driving,</a>   <a href="/blog/tag/trajectory"> <i class="fas fa-hashtag fa-sm"></i> Trajectory</a>   <a href="/blog/tag/planning"> <i class="fas fa-hashtag fa-sm"></i> Planning</a>     ·   <a href="/blog/category/blogpost"> <i class="fas fa-tag fa-sm"></i> blogpost</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li> <li class="toc-entry toc-h2"><a href="#notations-and-definitions">Notations and Definitions</a></li> <li class="toc-entry toc-h2"><a href="#the-main-problem--mode-blur">The Main Problem : “Mode Blur”</a></li> <li class="toc-entry toc-h2"><a href="#key-contributions">Key Contributions</a></li> <li class="toc-entry toc-h2"> <a href="#hierarchical-latent-structure-hls">Hierarchical Latent Structure (HLS)</a> <ul> <li class="toc-entry toc-h3"><a href="#introduction-to-hls">Introduction to HLS</a></li> <li class="toc-entry toc-h3"><a href="#hls-to-avoid-mode-blur">HLS to Avoid “Mode Blur”</a></li> <li class="toc-entry toc-h3"><a href="#hls-overall-architecture">HLS Overall Architecture</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li> </ul> </div> <hr> <div id="markdown-content"> <style>h2{margin-top:1.25em;margin-bottom:.5em}h3{margin-top:1em;margin-bottom:.5em}</style> <h2 id="introduction">Introduction</h2> <p><strong>Disclaimer</strong> : This review is based on my understanding of the reference paper [1]. While I have made much effort to ensure the accuracy of this article, there may things that I have not fully captured. If you notice any misinterpretation or error, please feel free to point them out in the comments section.</p> <p>I’m very excited to present a review of the paper titled “Hierarchical Latent Structure for Multi-Modal Vehicle Trajectory Forecasting” [1] authored by Dooseop Choi and KyoungWook Min. This paper is a very good work proved by its acceptance at the European Conference on Computer Vision (ECCV) 2022.</p> <p>For you who are not familiar with academia world in the AI field yet, ECCV is one of the most prestigious conferences in the domain of computer vision and having a paper accepted there indicates the importance of this work. I truly believe this research paper is crucial for the autonomous driving topic, particularly in trajectory forecasting.</p> <div class="row mt-4"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/HLS.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/HLS.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/HLS.gif-1400.webp"></source> <img src="/assets/img/HLS_Paper/HLS.gif" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Illustration of how the proposed Hierarchical Latent Structure (HLS) is used in the trajectory forecasting (Image source : D. Choi &amp; K. Min [1]). </div> <h2 id="notations-and-definitions">Notations and Definitions</h2> <table> <thead> <tr> <th>Notation</th> <th>Definition</th> </tr> </thead> <tbody> <tr> <td>\(N\)</td> <td>Number of vehicles in the traffic scene</td> </tr> <tr> <td>\(T\)</td> <td>Total number of timesteps for which trajectories are forecasted</td> </tr> <tr> <td>\(H\)</td> <td>Number of previous timesteps considered for positional history</td> </tr> <tr> <td>\(V_{i}\)</td> <td>The \(i^{th}\) vehicle in the traffic scene</td> </tr> <tr> <td>\(\mathbf{Y}_{i}\)</td> <td>Future positions of \(V_{i}\) for the next \(T\) timesteps</td> </tr> <tr> <td>\(\mathbf{X}_{i}\)</td> <td>Positional history of \(V_{i}\) for the previous \(H\) timesteps at time \(t\)</td> </tr> <tr> <td>\(\mathcal{C}_{i}\)</td> <td>Additional scene information available to \(V_{i}\)</td> </tr> <tr> <td>\(\mathbf{L}^{(1: M)}\)</td> <td>Lane candidates available for \(V_{i}\) at time \(t\)</td> </tr> <tr> <td>\(\mathbf{z}_{l}\)</td> <td>Low-level latent variable used to model the modes</td> </tr> <tr> <td>\(\mathbf{z}_{h}\)</td> <td>High-level latent variable used to model the weights for the modes</td> </tr> <tr> <td>\(p_{\theta}\)</td> <td>Decoder network</td> </tr> <tr> <td>\(p_{\gamma}\)</td> <td>Prior network</td> </tr> <tr> <td>\(\mathcal{L}_{E L B O}\)</td> <td>Modified ELBO objective</td> </tr> <tr> <td>\(q_{\phi}\)</td> <td>Approximated posterior network</td> </tr> <tr> <td>\(f_{\varphi}\)</td> <td>Proposed mode selection network</td> </tr> <tr> <td>VLI</td> <td>Vehicle-Lane Interaction</td> </tr> <tr> <td>V2I</td> <td>Vehicle-to-Vehicle Interaction</td> </tr> </tbody> </table> <h2 id="the-main-problem--mode-blur">The Main Problem : “Mode Blur”</h2> <p>The paper aims to overcome a specific limitation in vehicle trajectory forecasting models that leverage Variational Autoencoders (VAEs) concept called as the “mode blur” problem. For clearer illustration, please take a look at the figure below (this corresponds to the figure 1 in the reference paper [1]) :</p> <div class="row mt-4 justify-content-center"> <div class="col-12 col-md-8 mx-auto mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/figure1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/figure1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/figure1-1400.webp"></source> <img src="/assets/img/HLS_Paper/figure1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 2. Illustration of the "mode blur" problem in VAE-based generated trajectory forecasts (Image source : D. Choi &amp; K. Min [1]). </div> <p>As you can see from the figure above, the red vehicle is attempting to forecast its future trajectory represented by the branching gray paths. The challenge faced here lies in the generated forecast trajectories’ that are sometimes between defined lane paths.</p> <p>This phenomenon is what the author mean by the “mode blur” problem. Specifically, the VAE-based model is not committing to a specific path, but rather giving a “blurred” average of possible outcomes.</p> <div class="row mt-4"> <div class="col-12 col-lg mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/modeblur-previousSOTA-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/modeblur-previousSOTA-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/modeblur-previousSOTA-1400.webp"></source> <img src="/assets/img/HLS_Paper/modeblur-previousSOTA.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 3. Example of "mode blur" problem that exists in the previous SOTA model (Image source: Cui et al, 2021 [2]). </div> <p>If you still wonder why the “mode blur” problem can be very important, consider the above figure example taken from the previous SOTA model as observed by D. Choi &amp; K. Min [1]. Before analyzing that figure in more detail, assume that the green bounding box represents the Autonomous Vehicle (AV), the light blue bounding boxes represent surrounding vehicles, and the trajectories (path predictions) of the surrounding vehicles are shown using the solid lines with light blue dots.</p> <div class="row mt-4 justify-content-center"> <div class="col-12 col-md-8 mx-auto mt-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/scenario2_ModeBlur-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/scenario2_ModeBlur-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/scenario2_ModeBlur-1400.webp"></source> <img src="/assets/img/HLS_Paper/scenario2_ModeBlur.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 4. Scenario 2 of the "mode blur" problem that exist in the previous SOTA model (Image source : Cui et al, 2021 [2]). </div> <p>In scenario 2, a clear observation here is the overlapping and intersecting trajectories, especially around the intersection. These trajectories seem to be “blurred” between the lanes rather than being clearly defined in one lane or another. While in the scenario 3, despite the clearer trajectory forecasts than the previous one, we can still observe “mode blur” problems. Some predicted trajectories seem to be dispersed across the lane without a distinct path.</p> <p>This issue can lead to the Autonomous Vehicle (AV) having to make frequent adjustments to its path. This is indeed problematic as the AV might need to execute sudden brakes and make abrupt steering changes. This not only results in an uncomfortable ride for the passengers but also raises safety concerns.</p> <div class="row mt-4 justify-content-center"> <div class="col-12 col-md-8 mx-auto mt-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/scenario3_ModeBlur-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/scenario3_ModeBlur-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/scenario3_ModeBlur-1400.webp"></source> <img src="/assets/img/HLS_Paper/scenario3_ModeBlur.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 5. Scenario 3 of the "mode blur" problem that exist in the previous SOTA model (Image source : Cui et al, 2021 [2]). </div> <p>The reason for this problem is the use of Variational Autoencoders (VAEs) in the trajectory forecasting models since they have a well-known limitation: the outputs that they generate can often be “blurry”. The authors of paper [1] observed that similar problem also found in the trajectory planning case, not only in the tasks involving image reconstruction and synthesis.</p> <p>VAEs aim to learn a probabilistic latent space representation of the data. When dealing with complex distributions such as future vehicle trajectories, the latent space needs to capture the multi-modal nature of the data, representing different possible future states (modes). Recall that the main objective of the VAEs is to optimize the Evidence Lower Bound Objective (ELBO) on the marginal likelihood of data \(p_\theta(\mathbf{x})\). This lower bound is formulated as:</p> \[\text{ELBO} = \mathbb{E}_{q_\phi(\mathbf{z} \mid \mathbf{x})}[\log p_\theta(\mathbf{x} \mid \mathbf{z})] - D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) \| p_\theta(\mathbf{z}))\] <p>Two components in the ELBO:</p> <ul> <li>The first term \(\mathbb{E}_{q_\phi(\mathbf{z} \mid \mathbf{x})}[\log p_\theta(\mathbf{x} \mid \mathbf{z})]\) is the reconstruction loss which measures how well the VAE reconstructs the original data when sampled from the approximate posterior \(q_\phi\).</li> <li>The second term \(D_{KL}(q_\phi(\mathbf{z} \mid \mathbf{x}) \| p_\theta(\mathbf{z}))\) is the Kullback-Leibler divergence between the approximate posterior \(q_\phi\) and the prior \(p_\theta\). This term acts as a regularizer, pushing the approximate posterior towards the prior.</li> </ul> <div class="row mt-4"> <div class="col-12 col-lg mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/VAE_Image-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/VAE_Image-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/VAE_Image-1400.webp"></source> <img src="/assets/img/HLS_Paper/VAE_Image.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 6. Variational Autoencoder (VAE) which uses variational bayesian principle (Image source : <a href="https://sebastianraschka.com/teaching/stat453-ss2021/" rel="external nofollow noopener" target="_blank">Sebastian Raschka slide</a>). </div> <div class="row mt-4"> <div class="col-12 col-lg mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/AEvsVAE_Latent-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/AEvsVAE_Latent-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/AEvsVAE_Latent-1400.webp"></source> <img src="/assets/img/HLS_Paper/AEvsVAE_Latent.png" class="img-fluid" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 7. Generated latent variable from common Autoencoder (fixed value) vs. VAE (probability distribution) (Image source : <a href="https://www.jeremyjordan.me/variational-autoencoders/" rel="external nofollow noopener" target="_blank">Jeremy Jordan</a>). </div> <p>For more detailed understanding, you can take a look at this very good blogpost <a href="https://lilianweng.github.io/posts/2018-08-12-vae/" rel="external nofollow noopener" target="_blank">Lil’Log</a> or excellent explanation by <a href="https://www.youtube.com/watch?v=YHldNC1SZVk" rel="external nofollow noopener" target="_blank">Ahlad Kumar</a>.</p> <p>As you can see from the objective function above, the VAE wants to minimize reconstruction loss, while the KL divergence term encourages the VAE not to create very distinct and separate clusters for each mode in the latent space but to keep them close to the prior.</p> <p>As far as i know, many previous works assume the prior distribution for the latent variables, \(Z\), to be a standard Gaussian distribution, \(\mathcal{N}(0, I)\), which is fixed and does not depend on the input context. The reason for using this assumption is to simplify the learning process. The latent variables here are considered to capture the latent scene dynamics that influence the trajectories of multiple actors in the scene.</p> <p>This can be problematic because a standard Gaussian prior assumes that the latent space is unimodal and therefore does not capture the multi-modal nature of the future trajectories where multiple distinct future paths (modes) are possible.</p> <p>When the VAE learns to represent data in the latent space, it must balance the reconstruction and KL divergence terms. It wants to spread out the representations to minimize the reconstruction loss (since the trajectory distribution is multi-modal) but it is also constrained by the KL divergence to keep these representations from getting too dispersed (since the prior is unimodal).</p> <p>As a consequence, during the generation phase, when the model samples from these latent representations, it also may end up sampling from “in-between” spaces if the distinct modes are not well-separated. This results in outputs that are a blend of several possible outcomes rather than committing to a single, distinct outcome.</p> <p>So in the context of trajectory planning, the “mode blur” problem is most likely happened due to the balancing act between reconstruction loss and the KL divergence done by the ELBO objective function. When generating data, the VAE may generate a predicted trajectory that doesn’t clearly commit to any of the possible paths (like staying in the lane, changing lanes, turning, etc). Instead, it generates a trajectory that lies somewhere in between.</p> <h2 id="key-contributions">Key Contributions</h2> <p>Based on my understanding so far, there are 4 major contributions of this paper [1]:</p> <ol> <li> <p><strong>Mitigating Mode Blur</strong>: Propose a hierarchical latent structure within a VAE-based forecasting model to avoid “mode blur” problem, enabling clearer and more precise trajectory predictions.</p> </li> <li> <p><strong>Context Vectors</strong>: Two lane-level context vectors <code class="language-plaintext highlighter-rouge">VLI</code> and <code class="language-plaintext highlighter-rouge">V2I</code> are conditioned on the low-level latent variables for more accurate trajectory predictions.</p> </li> <li> <p><strong>Additional Methods</strong>: Introduce positional data preprocessing and GAN-based regularization to further enhance the performance.</p> </li> <li> <p><strong>Benchmark Performance</strong>: The state-of-the-art performance on two large-scale real-world datasets.</p> </li> </ol> <div class="row mt-4"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/VLI-visualization-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/VLI-visualization-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/VLI-visualization-1400.webp"></source> <img src="/assets/img/HLS_Paper/VLI-visualization.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 8. Visualization of VLI (Image source : D. Choi &amp; K. Min [1]). </div> <h2 id="hierarchical-latent-structure-hls">Hierarchical Latent Structure (HLS)</h2> <div class="row mt-4"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/HLS-Avoid-ModeBlur_Example-fotor-20231104133653-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/HLS-Avoid-ModeBlur_Example-fotor-20231104133653-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/HLS-Avoid-ModeBlur_Example-fotor-20231104133653-1400.webp"></source> <img src="/assets/img/HLS_Paper/HLS-Avoid-ModeBlur_Example-fotor-20231104133653.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 9. Example of how HLS avoids "mode blur" problem (Image source : D. Choi &amp; K. Min [1]). </div> <h3 id="introduction-to-hls">Introduction to HLS</h3> <p>In the complex traffic scenes with <code class="language-plaintext highlighter-rouge">N</code> vehicles, predicting the future trajectory of each vehicle can be challenging. The Hierarchical Latent Structure (HLS) proposed by D. Choi &amp; K. Min [1] aims to generate plausible trajectory distributions, taking into consideration both individual vehicle history and the overall scene.</p> <p>You may wonder how that kind of approach can avoid the “mode blur” problem that happens in the previous work. The goal of the proposed method is to generate a trajectory distribution \(p\left(\mathbf{Y}_{i} \mid \mathbf{X}_{i}, \mathcal{C}_{i}\right)\) for vehicles. This distribution is supposed to predict the future positions \(\mathbf{Y}_{i}\) based on the past positional history \(\mathbf{X}_{i}\) and the scene context \(\mathcal{C}_{i}\).</p> <p>The generated trajectory distribution is represented as a sum of modes, weighted by their probability or importance. Mathematically, it can be defined like below :</p> \[p\left(\mathbf{Y}_{i} \mid \mathbf{X}_{i}, \mathcal{C}_{i}\right)=\sum_{m=1}^{M} \underbrace{p\left(\mathbf{Y}_{i} \mid E_{m}, \mathbf{X}_{i}, \mathcal{C}_{i}\right)}_{\text {mode }} \underbrace{p\left(E_{m} \mid \mathbf{X}_{i}, \mathcal{C}_{i}\right)}_{\text {weight }}\] <p>The equation above indicates that the trajectory distribution \(p(\mathbf{Y}_{i} \mid \mathbf{X}_{i}, \mathcal{C}_{i})\) can be expressed as a weighted sum of distributions called modes. The term “mode” represents a plausible path, and the term “weight” represents the probability of each mode occurring.</p> <p>Remember that in a standard VAE, the generation process can sometimes collapse to the mean, resulting in less diverse samples. However, in a hierarchical Conditional VAE (C-VAE) which we will discuss in more detail later, the lower levels of the hierarchy are responsible for generating multiple potential trajectories and the higher levels can assign probabilities to the generated trajectories given a certain condition (like the current state of the car and its environment). This doesn’t mean picking the “average” path but selecting from a distribution of paths where each path is weighted according to its fit to the current context.</p> <h3 id="hls-to-avoid-mode-blur">HLS to Avoid “Mode Blur”</h3> <p>The key intuition here is to consider each possible trajectory (mode) separately. It does this through a model that uses latent variables to represent different modes. By modeling each mode with a latent variable, the model can sample trajectories from these modes based on their weights or importance. This allows for diverse trajectory predictions rather than a blurred average.</p> <div class="row mt-4 justify-content-center"> <div class="col-12 col-md-8 mx-auto mt-4 img-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/figure1b_mode-separately-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/figure1b_mode-separately-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/figure1b_mode-separately-1400.webp"></source> <img src="/assets/img/HLS_Paper/figure1b_mode-separately.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center mb-4"> Figure 10. Illustration of the trajectory forecasting distribution generated by HLS model (Image source : D. Choi &amp; K. Min [1]). </div> <p>The HLS approach consists of two latent variables within a conditional VAE framework:</p> <ol> <li> <p><strong>Low-level latent variable (\(\mathbf{z}_{l}\))</strong>: Used to model individual modes of the trajectory distributions. This is done through a decoder network that generates the vehicle’s future positions and a prior network that defines the distribution of the latent variable given the past positions and scene context.</p> </li> <li> <p><strong>High-level latent variable (\(\mathbf{z}_{h}\))</strong>: Represents the weights for different modes. This is the output of a mode selection network that determines the probabilities associated with different lanes.</p> </li> </ol> <p>The low-level latent variable helps the forecasting model define the mode distribution. This captures the variation within a mode, essentially the variety of possible trajectories given that a vehicle has chosen a particular lane. The high-level latent variable models the weights of the modes, deciding which trajectories are more likely than others based on the current situation.</p> <p>This is done by training the model to reconstruct the future trajectories \(\mathbf{Y}_i\) from the latent variables, while also ensuring that the latent variables are regularized to follow a prior distribution. Here is the mathematical equation of that new objective function :</p> \[\begin{aligned} \mathcal{L}_{ELBO} = &amp; -\mathbb{E}_{\mathbf{z}_{l} \sim q_{\phi}}\left[\log p_{\theta}\left(\mathbf{Y}_{i} \mid \mathbf{z}_{l}, \mathbf{X}_{i}, \mathcal{C}_{i}^{m}\right)\right] \\ &amp; + \beta KL\Big(q_{\phi}\left(\mathbf{z}_{l} \mid \mathbf{Y}_{i}, \mathbf{X}_{i}, \mathcal{C}_{i}^{m}\right)\| p_{\gamma}\left(\mathbf{z}_{l} \mid \mathbf{X}_{i}, \mathcal{C}_{i}^{m}\right)\Big), \end{aligned}\] <p>As you can see from the equation above, the expectation \(\mathbb{E}_{\mathbf{z}_l \sim q_{\phi}}[ \cdot ]\) term forces the model to reconstruct \(\mathbf{Y}_i\) from the low-level latent variables \(\mathbf{z}_l\), input \(\mathbf{X}_i\), and some conditioning context \(\mathcal{C}_i^{m}\). This implies that the model will learn a different prior for different subsets of the data, which is guided by the input and the context. When the conditional prior is a complex distribution such as a mixture of Gaussians, it has multiple modes corresponding to different data variations.</p> <p>In addition to that, the KL divergence term in the modified ELBO is weighted by a factor \(\beta\). By adjusting this parameter, we can find a trade-off between reconstruction and KL divergence aspects, potentially avoiding the mode collapse problem, where the model generates outputs that are too similar to each other, or the blurring issue, where the details in the reconstructed outputs become indistinct. Adjusting \(\beta\) allows for a more flexible and controlled latent space representation, which is crucial for generating diverse and sharp outputs.</p> <p>By having a hierarchical structure, the C-VAE can separate the task of generating trajectories (done by low-level latent variables) from the task of weighting them according to their likelihood (achieved with high-level latent variables). Thus, when it generates a sample, it’s not just picking the average or most common path but rather sampling from a distribution of paths that are likely under current conditions.</p> <h3 id="hls-overall-architecture">HLS Overall Architecture</h3> <div class="row mt-4"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/HLS_Architecture-fotor-20231104133313-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/HLS_Architecture-fotor-20231104133313-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/HLS_Architecture-fotor-20231104133313-1400.webp"></source> <img src="/assets/img/HLS_Paper/HLS_Architecture-fotor-20231104133313.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 11. Diagram of HLS architecture (Image source : D. Choi &amp; K. Min [1]). </div> <p>The proposed method from the paper focuses on predicting the future trajectory of vehicles by considering the interaction with the surrounding environment, particularly lanes and other vehicles. The approach is organized into the following modules:</p> <ol> <li> <strong>Feature Extraction Module</strong>: <ul> <li>This module uses three LSTM networks to encode positional data for vehicles and lanes.</li> <li>Data preprocessing involves calculating speed, heading for vehicles, and tangent vectors for lanes. This is to capture the motion history and lane’s orientation, making predictions more accurate.</li> </ul> </li> <li> <strong>Scene Context Extraction Module</strong>: <ul> <li>It considers the interactions of a vehicle with its reference lane <code class="language-plaintext highlighter-rouge">VLI</code> and other surrounding vehicles <code class="language-plaintext highlighter-rouge">V2I</code>.</li> <li>For the lane interaction, it uses attention mechanisms to weigh the importance of surrounding lanes relative to the reference lane.</li> <li>For vehicle-to-vehicle interactions, a Graph Neural Network (GNN) is employed. Only vehicles within a certain distance from the reference lane are considered. The interactions are captured through multiple rounds of message passing, and the final context vector represents the interaction history.</li> <li>There’s an emphasis on the distance threshold, which is empirically set to 5 meters, representing the typical distance between two nearby lane centerlines in straight roads.</li> </ul> </li> <li> <strong>Mode Selection Network</strong>: <ul> <li>Determines the weights for different modes of trajectory distribution. Each mode corresponds to a lane, capturing the assumption that the lanes heavily influence the vehicle’s motion.</li> <li>It uses lane-level scene context vectors, which contain information about both lane and vehicle interactions.</li> <li>A softmax operation is applied to get the final weights, representing the probability distribution over different modes.</li> </ul> </li> <li> <strong>Encoder, Prior, and Decoder</strong>: <ul> <li> <p><strong>Encoder</strong>: This is often referred to as the recognition network. It is responsible for approximating the posterior distribution and is implemented as Multi-Layer Perceptrons (MLPs) with the encoding of the future trajectory \(\tilde{\mathbf{Y}}_{i}\) and the lane-level scene context vector \(\mathbf{c}_{i}^{m}\) as inputs. The encoder outputs two vectors, mean \(\mu_{e}\) and standard deviation \(\sigma_{e}\). Notably, the encoder is used only during the training phase because the future trajectory \(\mathbf{Y}_{i}\) is not available during inference.</p> </li> <li> <p><strong>Prior</strong>: This represents the prior distribution over the latent variable and is also implemented as MLPs. It takes the lane-level scene context vector \(\mathbf{c}_{i}^{m}\) as its input and outputs mean \(\mu_{p}\) and standard deviation \(\sigma_{p}\) vectors.</p> </li> <li> <p><strong>Decoder</strong>: This network is responsible for generating predictions for the future trajectory \(\hat{\mathbf{Y}}_{i}\). It does so via an LSTM network. The input to the LSTM consists of an embedding of the predicted position \(\mathbf{e}_{i}^{t}\) along with the lane-level scene context vector \(\mathbf{c}_{i}^{m}\) and the latent variable \(\mathbf{z}_{l}\). The LSTM updates its hidden state \(\mathbf{h}_{i}^{t+1}\) based on these inputs, and the new predicted position \(\hat{\mathbf{p}}_{i}^{t+1}\) is generated from this hidden state.</p> </li> </ul> </li> </ol> <p>The design of this method aims to provide a holistic understanding of the vehicle’s motion by considering both lane and vehicle interactions. Lanes guide the general direction of movement, while nearby vehicles influence more immediate decisions like lane changes or speed adjustments.</p> <h2 id="conclusion">Conclusion</h2> <div class="row mt-4"> <div class="col-sm mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/HLS_Paper/Example_HLS_nuScene-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/HLS_Paper/Example_HLS_nuScene-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/HLS_Paper/Example_HLS_nuScene-1400.webp"></source> <img src="/assets/img/HLS_Paper/Example_HLS_nuScene.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 12. Example of trajectory forecasting generated by HLS on nuScenes dataset (Image source : D. Choi &amp; K. Min [1]). </div> <p>This paper proposes a novel and unique way to tackle the problem of “mode blur” predictions in trajectory forecasting. Instead of just mixing all possible paths, it uses a system of weights to represent different possible futures. This is achieved by introducing a hierarchy in latent variables which can make the model to be more accurate in representing different possible outcomes. The use of lane-level context vectors can add more precision, especially in understanding vehicle-lane and vehicle-vehicle interactions. With the additional techniques like positional data processing and GAN-based regularization, this work not only sharpens the predictions but also can outperform the previous SOTA models in terms of accuracy.</p> </div> </article> <h2>References</h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="HLS" class="col-sm-8"> <div class="title">Hierarchical Latent Structure for Multi-Modal Vehicle Trajectory Forecasting</div> <div class="author"> Dooseop Choi, and KyoungWook Min</div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2207.04624.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="LookOut" class="col-sm-8"> <div class="title">LookOut: Diverse Multi-Future Prediction and Planning for Self-Driving</div> <div class="author"> </div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2101.06547.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> </ol> </div> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Ravialdy Hidayat. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>